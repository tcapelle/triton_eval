{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 864/864 [00:00<00:00, 38782.95 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'uuid': '6d32b21d-530f-4d1d-a2b2-bb08a0c056cc',\n",
       " 'file_name': 'triton_matric_matmul.py',\n",
       " 'repo_name': 'elphinkuo/fast_matrix_multiplication',\n",
       " 'file_path': 'dot_product/on_gpu/Triton/triton_matric_matmul.py',\n",
       " 'commit_hash': '4e875a17e95b7ccf9af102d2c0f8cc31ed9a29f3',\n",
       " 'starcount': 0,\n",
       " 'input': \"@triton.jit\\ndef _matmul_kernel(A, B, C, M, N, K, **meta):\\n    TILE_M = meta['BLOCK_M']\\n    TILE_N = meta['BLOCK_N']\\n    TILE_K = 128\\n    m = tl.program_id(0) * TILE_M + tl.arange(0, TILE_M)\\n    n = tl.program_id(1) * TILE_N + tl.arange(0, TILE_N)\\n    acc = tl.zeros((TILE_M, TILE_N), dtype=tl.float32)\\n    for k in range(0, K, TILE_K):\\n        a = tl.load(A + m[:, None] * K + k, mask=[m[:, None] < M, None],\\n            other=0.0)\\n        b = tl.load(B + k * N + n, mask=[None, n < N], other=0.0)\\n        acc += tl.dot(a, b)\\n    tl.store(C + m[:, None] * N + n, acc, mask=[m[:, None] < M, n < N])\\n\",\n",
       " 'category': {'Data Type': ['fp32'],\n",
       "  'Functionality': ['Matrix Multiplication', 'Elementwise Operations'],\n",
       "  'Memory Access Pattern': ['Tiled', 'Coalesced'],\n",
       "  'Parallelization Strategy': ['Thread-Block Mappings'],\n",
       "  'Performance Objective': ['Compute Bound', 'High Throughput']},\n",
       " 'licenses': ['MIT'],\n",
       " 'github_url': 'https://github.com/elphinkuo/fast_matrix_multiplication/blob/4e875a17e95b7ccf9af102d2c0f8cc31ed9a29f3/dot_product/on_gpu/Triton/triton_matric_matmul.py',\n",
       " 'description': \"### Context Overview\\nThis Triton code is for a matrix multiplication kernel implemented in Triton, a language designed to enable highly efficient yet easy-to-write custom kernels for GPUs. This function efficiently computes the matrix product of two matrices, A and B, storing the result in matrix C.\\n\\n### Detailed Explanation\\n- **Function Signature**:\\n  - Decorated with `@triton.jit`, indicating just-in-time compilation.\\n  - Takes matrices A, B, and C as inputs, along with dimensions M, N, K, and additional metadata.\\n\\n- **Tile Dimensions**:\\n  - `TILE_M`, `TILE_N`, `TILE_K` define the dimensions of tiles used in partitioning the workload; `TILE_K` is set to a fixed value of 128.\\n  - `TILE_M` and `TILE_N` are fetched from metadata, allowing flexible configurations for tuning.\\n\\n- **Index Calculation**:\\n  - `m` and `n` are arrays representing the indices for matrices A and B, computed using the thread identifiers `tl.program_id(0)` and `tl.program_id(1)`, respectively. \\n\\n- **Accumulation Initialization**:\\n  - Initializes accumulator `acc` as a zero matrix of shape `(TILE_M, TILE_N)` using `tl.zeros()` where each thread block accumulates its results.\\n\\n- **Iterative Dot Product**:\\n  - Loop over segments of the K dimension in steps `TILE_K`.\\n  - Load corresponding slices from A and B with boundary checks using `tl.load()` and apply a zero-fill mechanism for invalid indices, avoiding out-of-bound reads.\\n- **Matrix Multiplication**:\\n  - Perform block-wise dot product with `tl.dot()` accumulating in `acc`.\\n\\n- **Storing the Result**:\\n  - After accumulating results in blocked segments, store them back into C using `tl.store()` with boundary validation similar to loading.\\n\\n### Rationale\\n- **Efficiency**:\\n  - By using tiling, the kernel optimally uses shared memory bandwidth and computational units, maximizing GPU resource utilization.\\n  - Unrolling and parallelizing the loop over blocks permit efficient use of parallel cores, reducing latency.\\n- **Flexibility**:\\n  - Meta parameters `BLOCK_M` and `BLOCK_N` allow tuning of tile size to optimize performance on different hardware architectures.\\n- **Robustness**:\\n  - Boundary masks and zero-filling ensure correctness, even when matrix dimensions aren't perfectly divisible by block sizes, making the code robust.\\n\\nIn summary, this Triton-based matrix multiplication efficiently computes results by leveraging parallelism and structured memory access patterns while remaining flexible to hardware constraints. The design ensures optimized GPU performance by harnessing the power of task tiling and just-in-time compilation.\",\n",
       " 'pytorch_code_with_tests': 'import torch\\n\\ndef matmul_pytorch(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Perform matrix multiplication using pure PyTorch.\\n    This function computes the product of matrix A and matrix B, equivalent to A @ B.\\n    Parameters:\\n      A (torch.Tensor): A tensor of shape (M, K).\\n      B (torch.Tensor): A tensor of shape (K, N).\\n    Returns:\\n      torch.Tensor: The product matrix of shape (M, N).\\n    \"\"\"\\n    # Verify dimensions match for matrix multiplication\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\\'Both A and B must be 2D tensors.\\')\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\\'Inner dimensions must match, got A: {A.shape}, B: {B.shape}\\')\\n    \\n    # Using torch.matmul which is equivalent to A @ B\\n    return torch.matmul(A, B)\\n\\n\\ndef test_matmul():\\n    results = {}\\n\\n    # Test Case 1: Small square matrices\\n    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\\n    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\\n    C1 = matmul_pytorch(A1, B1)\\n    results[\\'test_case_1\\'] = C1\\n\\n    # Test Case 2: Rectangular matrices (more rows than columns in A, more columns than rows in B)\\n    A2 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  # shape (2, 3)\\n    B2 = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]])  # shape (3, 2)\\n    C2 = matmul_pytorch(A2, B2)\\n    results[\\'test_case_2\\'] = C2\\n\\n    # Test Case 3: Larger matrices\\n    torch.manual_seed(42)\\n    A3 = torch.randn(64, 128)\\n    B3 = torch.randn(128, 32)\\n    C3 = matmul_pytorch(A3, B3)\\n    # Verify against torch.mm\\n    expected_C3 = torch.mm(A3, B3)\\n    results[\\'test_case_3\\'] = {\\n        \\'result\\': C3,\\n        \\'expected\\': expected_C3,\\n        \\'close\\': torch.allclose(C3, expected_C3)\\n    }\\n\\n    # Test Case 4: Non-divisible dimensions\\n    # Even if the sizes do not match tiling boundaries used in Triton, the algorithm must work correctly\\n    A4 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]])  # shape (4,3)\\n    B4 = torch.tensor([[2.0, 0.0], [1.0, 3.0], [0.0, 4.0]])  # shape (3,2)\\n    C4 = matmul_pytorch(A4, B4)\\n    results[\\'test_case_4\\'] = C4\\n\\n    # Test Case 5: Using random tensors on GPU (if available)\\n    if torch.cuda.is_available():\\n        device = torch.device(\\'cuda\\')\\n        A5 = torch.randn(128, 256, device=device)\\n        B5 = torch.randn(256, 64, device=device)\\n        C5 = matmul_pytorch(A5, B5)\\n        expected_C5 = torch.matmul(A5, B5)\\n        results[\\'test_case_5\\'] = {\\n            \\'result\\': C5,\\n            \\'expected\\': expected_C5,\\n            \\'close\\': torch.allclose(C5, expected_C5)\\n        }\\n    else:\\n        results[\\'test_case_5\\'] = \\'CUDA not available, skipped GPU test.\\'\\n\\n    return results\\n\\n\\nif __name__ == \\'__main__\\':\\n    test_results = test_matmul()\\n    for key, value in test_results.items():\\n        print(f\"{key}: {value}\")\\n',\n",
       " 'format_pt_code': 'import torch\\n\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\ndef matmul_pytorch(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Performs matrix multiplication of two 2D tensors.\\n\\n    Args:\\n        A (torch.Tensor): A 2D tensor of shape (M, K).\\n        B (torch.Tensor): A 2D tensor of shape (K, N).\\n\\n    Returns:\\n        torch.Tensor: The product tensor of shape (M, N).\\n\\n    Raises:\\n        ValueError: If A or B is not 2D or their inner dimensions do not match.\\n    \"\"\"\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\"Both A and B must be 2D tensors.\")\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\"Inner dimensions must match, got A: {A.shape}, B: {B.shape}\")\\n    return torch.matmul(A, B)\\n\\n########################################\\n\\nimport torch\\ntorch.manual_seed(42)\\n\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\ndef test_matmul_pytorch() -> dict:\\n    results = {}\\n\\n    # Test Case 1: Small square matrices\\n    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\\n    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=DEVICE)\\n    out1 = matmul_pytorch(A1, B1)\\n    exp1 = torch.matmul(A1, B1)\\n    results[\"test_case_1\"] = bool(torch.equal(out1, exp1))\\n\\n    # Test Case 2: Rectangular matrices (2x3 @ 3x2)\\n    A2 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=DEVICE)\\n    B2 = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], device=DEVICE)\\n    out2 = matmul_pytorch(A2, B2)\\n    exp2 = torch.matmul(A2, B2)\\n    results[\"test_case_2\"] = bool(torch.equal(out2, exp2))\\n\\n    # Test Case 3: Inner-dimension mismatch should raise ValueError\\n    A3 = torch.randn(2, 3, device=DEVICE)\\n    B3 = torch.randn(2, 2, device=DEVICE)\\n    try:\\n        _ = matmul_pytorch(A3, B3)\\n        results[\"test_case_3\"] = False\\n    except ValueError:\\n        results[\"test_case_3\"] = True\\n\\n    # Test Case 4: Non-2D inputs should raise ValueError\\n    A4 = torch.tensor([1.0, 2.0, 3.0], device=DEVICE)\\n    B4 = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device=DEVICE)\\n    try:\\n        _ = matmul_pytorch(A4, B4)\\n        results[\"test_case_4\"] = False\\n    except ValueError:\\n        results[\"test_case_4\"] = True\\n\\n    print(results)\\n\\ntest_matmul_pytorch()\\n',\n",
       " 'entrypoint': 'matmul_pytorch',\n",
       " 'pt_code_runs': True,\n",
       " 'stop_reason': 'done',\n",
       " 'pt_code_without_tests': 'import torch\\ntorch.manual_seed(42)\\n\\nDEVICE = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\n\\n\\nimport torch\\n\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\ndef matmul_pytorch(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Performs matrix multiplication of two 2D tensors.\\n\\n    Args:\\n        A (torch.Tensor): A 2D tensor of shape (M, K).\\n        B (torch.Tensor): A 2D tensor of shape (K, N).\\n\\n    Returns:\\n        torch.Tensor: The product tensor of shape (M, N).\\n\\n    Raises:\\n        ValueError: If A or B is not 2D or their inner dimensions do not match.\\n    \"\"\"\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\"Both A and B must be 2D tensors.\")\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\"Inner dimensions must match, got A: {A.shape}, B: {B.shape}\")\\n    return torch.matmul(A, B)\\n\\n',\n",
       " 'tests': 'import torch\\ntorch.manual_seed(42)\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\ndef test_matmul_pytorch() -> dict:\\n    results = {}\\n    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\\n    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=DEVICE)\\n    out1 = matmul_pytorch(A1, B1)\\n    exp1 = torch.matmul(A1, B1)\\n    results[\"test_case_1\"] = bool(torch.equal(out1, exp1))\\n    A2 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=DEVICE)\\n    B2 = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], device=DEVICE)\\n    out2 = matmul_pytorch(A2, B2)\\n    exp2 = torch.matmul(A2, B2)\\n    results[\"test_case_2\"] = bool(torch.equal(out2, exp2))\\n    A3 = torch.randn(2, 3, device=DEVICE)\\n    B3 = torch.randn(2, 2, device=DEVICE)\\n    try:\\n        _ = matmul_pytorch(A3, B3)\\n        results[\"test_case_3\"] = False\\n    except ValueError:\\n        results[\"test_case_3\"] = True\\n    A4 = torch.tensor([1.0, 2.0, 3.0], device=DEVICE)\\n    B4 = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device=DEVICE)\\n    try:\\n        _ = matmul_pytorch(A4, B4)\\n        results[\"test_case_4\"] = False\\n    except ValueError:\\n        results[\"test_case_4\"] = True\\n    print(results)\\n\\ntest_matmul_pytorch()',\n",
       " 'stdout': \"{'test_case_1': True, 'test_case_2': True, 'test_case_3': True, 'test_case_4': True}\\n\",\n",
       " 'stderr': '',\n",
       " 'runtime': 0.01568238396430388}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"tcapelle/train_ds_triton_v2f2\"\n",
    "\n",
    "ds = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.remove_columns(\"pytorch_code_with_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.rename_column(\"format_pt_code\", \"pt_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uuid': '6d32b21d-530f-4d1d-a2b2-bb08a0c056cc',\n",
       " 'file_name': 'triton_matric_matmul.py',\n",
       " 'repo_name': 'elphinkuo/fast_matrix_multiplication',\n",
       " 'file_path': 'dot_product/on_gpu/Triton/triton_matric_matmul.py',\n",
       " 'commit_hash': '4e875a17e95b7ccf9af102d2c0f8cc31ed9a29f3',\n",
       " 'starcount': 0,\n",
       " 'input': \"@triton.jit\\ndef _matmul_kernel(A, B, C, M, N, K, **meta):\\n    TILE_M = meta['BLOCK_M']\\n    TILE_N = meta['BLOCK_N']\\n    TILE_K = 128\\n    m = tl.program_id(0) * TILE_M + tl.arange(0, TILE_M)\\n    n = tl.program_id(1) * TILE_N + tl.arange(0, TILE_N)\\n    acc = tl.zeros((TILE_M, TILE_N), dtype=tl.float32)\\n    for k in range(0, K, TILE_K):\\n        a = tl.load(A + m[:, None] * K + k, mask=[m[:, None] < M, None],\\n            other=0.0)\\n        b = tl.load(B + k * N + n, mask=[None, n < N], other=0.0)\\n        acc += tl.dot(a, b)\\n    tl.store(C + m[:, None] * N + n, acc, mask=[m[:, None] < M, n < N])\\n\",\n",
       " 'category': {'Data Type': ['fp32'],\n",
       "  'Functionality': ['Matrix Multiplication', 'Elementwise Operations'],\n",
       "  'Memory Access Pattern': ['Tiled', 'Coalesced'],\n",
       "  'Parallelization Strategy': ['Thread-Block Mappings'],\n",
       "  'Performance Objective': ['Compute Bound', 'High Throughput']},\n",
       " 'licenses': ['MIT'],\n",
       " 'github_url': 'https://github.com/elphinkuo/fast_matrix_multiplication/blob/4e875a17e95b7ccf9af102d2c0f8cc31ed9a29f3/dot_product/on_gpu/Triton/triton_matric_matmul.py',\n",
       " 'description': \"### Context Overview\\nThis Triton code is for a matrix multiplication kernel implemented in Triton, a language designed to enable highly efficient yet easy-to-write custom kernels for GPUs. This function efficiently computes the matrix product of two matrices, A and B, storing the result in matrix C.\\n\\n### Detailed Explanation\\n- **Function Signature**:\\n  - Decorated with `@triton.jit`, indicating just-in-time compilation.\\n  - Takes matrices A, B, and C as inputs, along with dimensions M, N, K, and additional metadata.\\n\\n- **Tile Dimensions**:\\n  - `TILE_M`, `TILE_N`, `TILE_K` define the dimensions of tiles used in partitioning the workload; `TILE_K` is set to a fixed value of 128.\\n  - `TILE_M` and `TILE_N` are fetched from metadata, allowing flexible configurations for tuning.\\n\\n- **Index Calculation**:\\n  - `m` and `n` are arrays representing the indices for matrices A and B, computed using the thread identifiers `tl.program_id(0)` and `tl.program_id(1)`, respectively. \\n\\n- **Accumulation Initialization**:\\n  - Initializes accumulator `acc` as a zero matrix of shape `(TILE_M, TILE_N)` using `tl.zeros()` where each thread block accumulates its results.\\n\\n- **Iterative Dot Product**:\\n  - Loop over segments of the K dimension in steps `TILE_K`.\\n  - Load corresponding slices from A and B with boundary checks using `tl.load()` and apply a zero-fill mechanism for invalid indices, avoiding out-of-bound reads.\\n- **Matrix Multiplication**:\\n  - Perform block-wise dot product with `tl.dot()` accumulating in `acc`.\\n\\n- **Storing the Result**:\\n  - After accumulating results in blocked segments, store them back into C using `tl.store()` with boundary validation similar to loading.\\n\\n### Rationale\\n- **Efficiency**:\\n  - By using tiling, the kernel optimally uses shared memory bandwidth and computational units, maximizing GPU resource utilization.\\n  - Unrolling and parallelizing the loop over blocks permit efficient use of parallel cores, reducing latency.\\n- **Flexibility**:\\n  - Meta parameters `BLOCK_M` and `BLOCK_N` allow tuning of tile size to optimize performance on different hardware architectures.\\n- **Robustness**:\\n  - Boundary masks and zero-filling ensure correctness, even when matrix dimensions aren't perfectly divisible by block sizes, making the code robust.\\n\\nIn summary, this Triton-based matrix multiplication efficiently computes results by leveraging parallelism and structured memory access patterns while remaining flexible to hardware constraints. The design ensures optimized GPU performance by harnessing the power of task tiling and just-in-time compilation.\",\n",
       " 'pt_code': 'import torch\\n\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\ndef matmul_pytorch(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Performs matrix multiplication of two 2D tensors.\\n\\n    Args:\\n        A (torch.Tensor): A 2D tensor of shape (M, K).\\n        B (torch.Tensor): A 2D tensor of shape (K, N).\\n\\n    Returns:\\n        torch.Tensor: The product tensor of shape (M, N).\\n\\n    Raises:\\n        ValueError: If A or B is not 2D or their inner dimensions do not match.\\n    \"\"\"\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\"Both A and B must be 2D tensors.\")\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\"Inner dimensions must match, got A: {A.shape}, B: {B.shape}\")\\n    return torch.matmul(A, B)\\n\\n########################################\\n\\nimport torch\\ntorch.manual_seed(42)\\n\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\ndef test_matmul_pytorch() -> dict:\\n    results = {}\\n\\n    # Test Case 1: Small square matrices\\n    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\\n    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=DEVICE)\\n    out1 = matmul_pytorch(A1, B1)\\n    exp1 = torch.matmul(A1, B1)\\n    results[\"test_case_1\"] = bool(torch.equal(out1, exp1))\\n\\n    # Test Case 2: Rectangular matrices (2x3 @ 3x2)\\n    A2 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=DEVICE)\\n    B2 = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], device=DEVICE)\\n    out2 = matmul_pytorch(A2, B2)\\n    exp2 = torch.matmul(A2, B2)\\n    results[\"test_case_2\"] = bool(torch.equal(out2, exp2))\\n\\n    # Test Case 3: Inner-dimension mismatch should raise ValueError\\n    A3 = torch.randn(2, 3, device=DEVICE)\\n    B3 = torch.randn(2, 2, device=DEVICE)\\n    try:\\n        _ = matmul_pytorch(A3, B3)\\n        results[\"test_case_3\"] = False\\n    except ValueError:\\n        results[\"test_case_3\"] = True\\n\\n    # Test Case 4: Non-2D inputs should raise ValueError\\n    A4 = torch.tensor([1.0, 2.0, 3.0], device=DEVICE)\\n    B4 = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device=DEVICE)\\n    try:\\n        _ = matmul_pytorch(A4, B4)\\n        results[\"test_case_4\"] = False\\n    except ValueError:\\n        results[\"test_case_4\"] = True\\n\\n    print(results)\\n\\ntest_matmul_pytorch()\\n',\n",
       " 'entrypoint': 'matmul_pytorch',\n",
       " 'pt_code_runs': True,\n",
       " 'stop_reason': 'done',\n",
       " 'pt_code_without_tests': 'import torch\\ntorch.manual_seed(42)\\n\\nDEVICE = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\n\\n\\nimport torch\\n\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\ndef matmul_pytorch(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Performs matrix multiplication of two 2D tensors.\\n\\n    Args:\\n        A (torch.Tensor): A 2D tensor of shape (M, K).\\n        B (torch.Tensor): A 2D tensor of shape (K, N).\\n\\n    Returns:\\n        torch.Tensor: The product tensor of shape (M, N).\\n\\n    Raises:\\n        ValueError: If A or B is not 2D or their inner dimensions do not match.\\n    \"\"\"\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\"Both A and B must be 2D tensors.\")\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\"Inner dimensions must match, got A: {A.shape}, B: {B.shape}\")\\n    return torch.matmul(A, B)\\n\\n',\n",
       " 'tests': 'import torch\\ntorch.manual_seed(42)\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\ndef test_matmul_pytorch() -> dict:\\n    results = {}\\n    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\\n    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=DEVICE)\\n    out1 = matmul_pytorch(A1, B1)\\n    exp1 = torch.matmul(A1, B1)\\n    results[\"test_case_1\"] = bool(torch.equal(out1, exp1))\\n    A2 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=DEVICE)\\n    B2 = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], device=DEVICE)\\n    out2 = matmul_pytorch(A2, B2)\\n    exp2 = torch.matmul(A2, B2)\\n    results[\"test_case_2\"] = bool(torch.equal(out2, exp2))\\n    A3 = torch.randn(2, 3, device=DEVICE)\\n    B3 = torch.randn(2, 2, device=DEVICE)\\n    try:\\n        _ = matmul_pytorch(A3, B3)\\n        results[\"test_case_3\"] = False\\n    except ValueError:\\n        results[\"test_case_3\"] = True\\n    A4 = torch.tensor([1.0, 2.0, 3.0], device=DEVICE)\\n    B4 = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device=DEVICE)\\n    try:\\n        _ = matmul_pytorch(A4, B4)\\n        results[\"test_case_4\"] = False\\n    except ValueError:\\n        results[\"test_case_4\"] = True\\n    print(results)\\n\\ntest_matmul_pytorch()',\n",
       " 'stdout': \"{'test_case_1': True, 'test_case_2': True, 'test_case_3': True, 'test_case_4': True}\\n\",\n",
       " 'stderr': '',\n",
       " 'runtime': 0.01568238396430388}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['uuid', 'file_name', 'repo_name', 'file_path', 'commit_hash', 'starcount', 'input', 'category', 'licenses', 'github_url', 'description', 'pt_code', 'entrypoint', 'pt_code_runs', 'stop_reason', 'pt_code_without_tests', 'tests', 'stdout', 'stderr', 'runtime'],\n",
       "    num_rows: 864\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_test_signature(test_code):\n",
    "    # use a re to find the test_* items\n",
    "    return re.findall(r\"test_[^ ]+\", test_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tests_signature(row):\n",
    "    \"We should have a test that match the entrypoint -> test_<entrypoint>\"\n",
    "    tests = row[\"tests\"]\n",
    "    if tests == \"\":\n",
    "        print(\"Error: tests is empty\")\n",
    "        return False\n",
    "    entrypoint = row[\"entrypoint\"]\n",
    "    if f\"test_{entrypoint}\" not in tests:\n",
    "        actual_tests = get_test_signature(tests)\n",
    "        print(f\"Error: test_{entrypoint} not in tests\")\n",
    "        print(f\"Actual tests: {actual_tests}\")\n",
    "        return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 864/864 [00:00<00:00, 27910.56 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: test_fwd_prepare_wy_repr not in tests\n",
      "Actual tests: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fds = ds.filter(lambda x: check_tests_signature(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 10.72ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tcapelle/train_ds_triton_v2f2/commit/f96471b74ac0da3a4d527e9b51a2f1b0121d3721', commit_message='fix_test_signature', commit_description='', oid='f96471b74ac0da3a4d527e9b51a2f1b0121d3721', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tcapelle/train_ds_triton_v2f2', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tcapelle/train_ds_triton_v2f2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fds.push_to_hub(dataset_name, commit_message=\"fix_test_signature\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
