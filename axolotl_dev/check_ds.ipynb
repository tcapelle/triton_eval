{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 828/828 [00:00<00:00, 67708.10 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'entrypoint': 'matmul',\n",
       " 'triton_code': 'import torch\\nimport triton\\nimport triton.language as tl\\n\\n# Global device standard\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n\\n@triton.jit\\ndef _matmul_kernel(A, B, C, M, N, K, **meta):\\n    \"\"\"Triton kernel for matrix multiplication using tiling.\"\"\"\\n    # Tiling sizes\\n    TILE_M = meta[\\'BLOCK_M\\']\\n    TILE_N = meta[\\'BLOCK_N\\']\\n    TILE_K = 128\\n    \\n    # Indices for output tile computed by the current program instance\\n    m = tl.program_id(0) * TILE_M + tl.arange(0, TILE_M)\\n    n = tl.program_id(1) * TILE_N + tl.arange(0, TILE_N)\\n    \\n    # Initialize the accumulator for the resultant tile\\n    acc = tl.zeros((TILE_M, TILE_N), dtype=tl.float32)\\n    \\n    # Loop over the K dimension with tiles\\n    for k in range(0, K, TILE_K):\\n        a = tl.load(A + m[:, None] * K + k, mask=[m[:, None] < M, None], other=0.0)\\n        b = tl.load(B + k * N + n, mask=[None, n < N], other=0.0)\\n        acc += tl.dot(a, b)\\n    \\n    # Store the computed tile into C\\n    tl.store(C + m[:, None] * N + n, acc, mask=[m[:, None] < M, n < N])\\n\\n\\n\\ndef matmul(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Perform matrix multiplication using a Triton kernel that computes A @ B.\\n\\n    Parameters:\\n      A (torch.Tensor): Tensor of shape (M, K) on DEVICE.\\n      B (torch.Tensor): Tensor of shape (K, N) on DEVICE.\\n\\n    Returns:\\n      torch.Tensor: The product matrix of shape (M, N).\\n    \"\"\"\\n    # Verify dimensions\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\\'Both A and B must be 2D tensors.\\')\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\\'Inner dimensions must match, got A: {A.shape}, B: {B.shape}\\')\\n\\n    # Allocate the output tensor on the global DEVICE\\n    C = torch.empty((M, N), device=DEVICE, dtype=torch.float32)\\n\\n    # Define block sizes (tuning parameters)\\n    BLOCK_M = 64\\n    BLOCK_N = 64\\n    \\n    # Calculate the grid dimensions\\n    grid = ((M + BLOCK_M - 1) // BLOCK_M, (N + BLOCK_N - 1) // BLOCK_N)\\n\\n    # Launch the Triton kernel\\n    _matmul_kernel[grid](A, B, C, M, N, K, meta={\\'BLOCK_M\\': BLOCK_M, \\'BLOCK_N\\': BLOCK_N})\\n    return C\\n\\n########################\\n\\ndef test_matmul():\\n    \"\"\"\\n    Test function for Triton-based matrix multiplication on DEVICE.\\n\\n    Returns:\\n      dict: Dictionary storing test results for each test case.\\n    \"\"\"\\n    results = {}\\n\\n    # Test Case 1: Small square matrices\\n    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\\n    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=DEVICE)\\n    C1 = matmul(A1, B1)\\n    results[\\'test_case_1\\'] = C1\\n\\n    # Test Case 2: Rectangular matrices\\n    A2 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=DEVICE)  # (2, 3)\\n    B2 = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], device=DEVICE)  # (3, 2)\\n    C2 = matmul(A2, B2)\\n    results[\\'test_case_2\\'] = C2\\n\\n    # Test Case 3: Larger matrices\\n    torch.manual_seed(42)\\n    A3 = torch.randn(64, 128, device=DEVICE)\\n    B3 = torch.randn(128, 32, device=DEVICE)\\n    C3 = matmul(A3, B3)\\n    expected_C3 = torch.mm(A3, B3)\\n    results[\\'test_case_3\\'] = {\\n        \\'result\\': C3,\\n        \\'expected\\': expected_C3,\\n        \\'close\\': torch.allclose(C3, expected_C3)\\n    }\\n\\n    # Test Case 4: Non-divisible dimensions\\n    A4 = torch.tensor([\\n        [1.0, 2.0, 3.0], \\n        [4.0, 5.0, 6.0], \\n        [7.0, 8.0, 9.0], \\n        [10.0, 11.0, 12.0]\\n    ], device=DEVICE)  # (4, 3)\\n    B4 = torch.tensor([\\n        [2.0, 0.0], \\n        [1.0, 3.0], \\n        [0.0, 4.0]\\n    ], device=DEVICE)  # (3, 2)\\n    C4 = matmul(A4, B4)\\n    results[\\'test_case_4\\'] = C4\\n\\n    # Test Case 5: Random tensors on GPU\\n    if torch.cuda.is_available():\\n        A5 = torch.randn(128, 256, device=DEVICE)\\n        B5 = torch.randn(256, 64, device=DEVICE)\\n        C5 = matmul(A5, B5)\\n        expected_C5 = torch.matmul(A5, B5)\\n        results[\\'test_case_5\\'] = {\\n            \\'result\\': C5,\\n            \\'expected\\': expected_C5,\\n            \\'close\\': torch.allclose(C5, expected_C5)\\n        }\\n    else:\\n        results[\\'test_case_5\\'] = \\'CUDA not available, skipped GPU test.\\'\\n\\n    return results\\n\\n########################\\n\\n# Run tests and print only the final test_results dictionary.\\ntest_results = test_matmul()\\nprint(test_results)\\n',\n",
       " 'pt_code': 'import torch\\n\\n# Global device standard\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n\\ndef matmul(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Perform matrix multiplication using pure PyTorch with torch.matmul.\\n\\n    Parameters:\\n      A (torch.Tensor): Tensor of shape (M, K) on DEVICE.\\n      B (torch.Tensor): Tensor of shape (K, N) on DEVICE.\\n\\n    Returns:\\n      torch.Tensor: The product matrix of shape (M, N).\\n    \"\"\"\\n    # Verify dimensions\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\\'Both A and B must be 2D tensors.\\')\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\\'Inner dimensions must match, got A: {A.shape}, B: {B.shape}\\')\\n\\n    # Perform matrix multiplication using torch.matmul\\n    return torch.matmul(A, B)\\n\\n########################\\n\\n',\n",
       " 'tests': 'def test_matmul():\\n    \"\"\"\\n    Test function for pure PyTorch matrix multiplication on DEVICE.\\n\\n    Returns:\\n      dict: Dictionary storing test results for each test case.\\n    \"\"\"\\n    results = {}\\n\\n    # Test Case 1: Small square matrices\\n    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\\n    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=DEVICE)\\n    C1 = matmul(A1, B1)\\n    results[\\'test_case_1\\'] = C1\\n\\n    # Test Case 2: Rectangular matrices\\n    A2 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=DEVICE)\\n    B2 = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], device=DEVICE)\\n    C2 = matmul(A2, B2)\\n    results[\\'test_case_2\\'] = C2\\n\\n    # Test Case 3: Larger matrices\\n    torch.manual_seed(42)\\n    A3 = torch.randn(64, 128, device=DEVICE)\\n    B3 = torch.randn(128, 32, device=DEVICE)\\n    C3 = matmul(A3, B3)\\n    expected_C3 = torch.mm(A3, B3)\\n    results[\\'test_case_3\\'] = {\\n        \\'result\\': C3,\\n        \\'expected\\': expected_C3,\\n        \\'close\\': torch.allclose(C3, expected_C3)\\n    }\\n\\n    # Test Case 4: Non-divisible dimensions\\n    A4 = torch.tensor([\\n        [1.0, 2.0, 3.0],\\n        [4.0, 5.0, 6.0],\\n        [7.0, 8.0, 9.0],\\n        [10.0, 11.0, 12.0]\\n    ], device=DEVICE)\\n    B4 = torch.tensor([\\n        [2.0, 0.0],\\n        [1.0, 3.0],\\n        [0.0, 4.0]\\n    ], device=DEVICE)\\n    C4 = matmul(A4, B4)\\n    results[\\'test_case_4\\'] = C4\\n\\n    # Test Case 5: Random tensors on GPU\\n    if torch.cuda.is_available():\\n        A5 = torch.randn(128, 256, device=DEVICE)\\n        B5 = torch.randn(256, 64, device=DEVICE)\\n        C5 = matmul(A5, B5)\\n        expected_C5 = torch.matmul(A5, B5)\\n        results[\\'test_case_5\\'] = {\\n            \\'result\\': C5,\\n            \\'expected\\': expected_C5,\\n            \\'close\\': torch.allclose(C5, expected_C5)\\n        }\\n    else:\\n        results[\\'test_case_5\\'] = \\'CUDA not available, skipped GPU test.\\'\\n\\n    return results\\n\\n########################\\n\\n# Run tests and print only the final test_results dictionary.\\ntest_results = test_matmul()\\nprint(test_results)\\n',\n",
       " 'pt_runs': True,\n",
       " 'pt_stdout': \"{'test_case_1': tensor([[19., 22.],\\n        [43., 50.]], device='cuda:0'), 'test_case_2': tensor([[ 58.,  64.],\\n        [139., 154.]], device='cuda:0'), 'test_case_3': {'result': tensor([[  9.3524,  20.1801,   1.3200,  ..., -21.0338,   3.0357,  -8.3879],\\n        [ -5.5521,   5.0191, -26.5503,  ...,  -5.4739,  -7.3350,  -0.0405],\\n        [  2.6591,  -5.7370,   2.5628,  ...,  22.7629,   1.0609,  -6.0721],\\n        ...,\\n        [  0.7112,  11.1433,   7.8263,  ...,  -8.2718,  -5.5668,  -6.1661],\\n        [ 17.1974,  -6.1684,   1.1457,  ...,  -6.9263, -12.8880,   5.2832],\\n        [-10.5624,   2.1081, -10.1488,  ...,   7.4583,  -1.6897,  -1.7082]],\\n       device='cuda:0'), 'expected': tensor([[  9.3524,  20.1801,   1.3200,  ..., -21.0338,   3.0357,  -8.3879],\\n        [ -5.5521,   5.0191, -26.5503,  ...,  -5.4739,  -7.3350,  -0.0405],\\n        [  2.6591,  -5.7370,   2.5628,  ...,  22.7629,   1.0609,  -6.0721],\\n        ...,\\n        [  0.7112,  11.1433,   7.8263,  ...,  -8.2718,  -5.5668,  -6.1661],\\n        [ 17.1974,  -6.1684,   1.1457,  ...,  -6.9263, -12.8880,   5.2832],\\n        [-10.5624,   2.1081, -10.1488,  ...,   7.4583,  -1.6897,  -1.7082]],\\n       device='cuda:0'), 'close': True}, 'test_case_4': tensor([[ 4., 18.],\\n        [13., 39.],\\n        [22., 60.],\\n        [31., 81.]], device='cuda:0'), 'test_case_5': {'result': tensor([[-20.8893,  18.6847, -25.4520,  ...,  11.5636,   0.8549,  -2.4848],\\n        [  4.9682, -24.7063,   5.0134,  ...,  17.0403, -24.0015,  22.7481],\\n        [ -4.9066,   7.7711,   6.0983,  ..., -12.2099,  13.7487,  -2.7126],\\n        ...,\\n        [-17.5489, -11.0569,   7.5630,  ...,  -6.2937, -21.6412,  19.4340],\\n        [-22.7031,  -7.3231,   6.3431,  ..., -25.8886, -23.7392, -13.3596],\\n        [-23.1386,  -4.5383, -27.9520,  ...,  -9.5806,   2.6319,   8.3291]],\\n       device='cuda:0'), 'expected': tensor([[-20.8893,  18.6847, -25.4520,  ...,  11.5636,   0.8549,  -2.4848],\\n        [  4.9682, -24.7063,   5.0134,  ...,  17.0403, -24.0015,  22.7481],\\n        [ -4.9066,   7.7711,   6.0983,  ..., -12.2099,  13.7487,  -2.7126],\\n        ...,\\n        [-17.5489, -11.0569,   7.5630,  ...,  -6.2937, -21.6412,  19.4340],\\n        [-22.7031,  -7.3231,   6.3431,  ..., -25.8886, -23.7392, -13.3596],\\n        [-23.1386,  -4.5383, -27.9520,  ...,  -9.5806,   2.6319,   8.3291]],\\n       device='cuda:0'), 'close': True}}\\n\",\n",
       " 'pt_stderr': '',\n",
       " 'pt_exec_time': 3.164108832948841,\n",
       " 'prompt': [{'content': '\\nYou are an expert in Triton programming, capable of writing corresponding Triton kernels and wrapper functions based on functional descriptions and function parameters. \\n\\n# Instructions\\n- Ensure that the wrapper function fully corresponds to the provided function information.\\n- Generate a detailed plan on how to convert and optimize the Pytorch code to a Triton kernel before writing the code.\\n- The reasoning process MUST BE enclosed within <think> and </think> tags.\"\\n- Reply with the thinking process and a single blob of code surrounded with ```python and ```.\\n',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Convert the following PyTorch code to a Triton kernel.\\nPytorch code:\\n```python\\nimport torch\\n\\n# Global device standard\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n\\ndef matmul(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Perform matrix multiplication using pure PyTorch with torch.matmul.\\n\\n    Parameters:\\n      A (torch.Tensor): Tensor of shape (M, K) on DEVICE.\\n      B (torch.Tensor): Tensor of shape (K, N) on DEVICE.\\n\\n    Returns:\\n      torch.Tensor: The product matrix of shape (M, N).\\n    \"\"\"\\n    # Verify dimensions\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\\'Both A and B must be 2D tensors.\\')\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\\'Inner dimensions must match, got A: {A.shape}, B: {B.shape}\\')\\n\\n    # Perform matrix multiplication using torch.matmul\\n    return torch.matmul(A, B)\\n\\n########################\\n\\n```\\n\\nThe function should have the same name as the PyTorch function: matmul\\n\\nDon\\'t forget to format your answer as:\\n<think>\\nthinking process\\n</think>\\n```python\\ncode\\n```',\n",
       "   'role': 'user'},\n",
       "  {'content': '<think>', 'role': 'assistant'}]}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"tcapelle/train_ds_triton\", split=\"train\")\n",
    "\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert in Triton programming, capable of writing corresponding Triton kernels and wrapper functions based on functional descriptions and function parameters. \n",
      "\n",
      "# Instructions\n",
      "- Ensure that the wrapper function fully corresponds to the provided function information.\n",
      "- Generate a detailed plan on how to convert and optimize the Pytorch code to a Triton kernel before writing the code.\n",
      "- The reasoning process MUST BE enclosed within <think> and </think> tags.\"\n",
      "- Reply with the thinking process and a single blob of code surrounded with ```python and ```.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert the following PyTorch code to a Triton kernel.\n",
      "Pytorch code:\n",
      "```python\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def matmul(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Perform matrix multiplication using pure PyTorch with torch.matmul.\n",
      "\n",
      "    Parameters:\n",
      "      A (torch.Tensor): Tensor of shape (M, K) on DEVICE.\n",
      "      B (torch.Tensor): Tensor of shape (K, N) on DEVICE.\n",
      "\n",
      "    Returns:\n",
      "      torch.Tensor: The product matrix of shape (M, N).\n",
      "    \"\"\"\n",
      "    # Verify dimensions\n",
      "    if A.dim() != 2 or B.dim() != 2:\n",
      "        raise ValueError('Both A and B must be 2D tensors.')\n",
      "    M, K = A.shape\n",
      "    K2, N = B.shape\n",
      "    if K != K2:\n",
      "        raise ValueError(f'Inner dimensions must match, got A: {A.shape}, B: {B.shape}')\n",
      "\n",
      "    # Perform matrix multiplication using torch.matmul\n",
      "    return torch.matmul(A, B)\n",
      "\n",
      "########################\n",
      "\n",
      "```\n",
      "\n",
      "The function should have the same name as the PyTorch function: matmul\n",
      "\n",
      "Don't forget to format your answer as:\n",
      "<think>\n",
      "thinking process\n",
      "</think>\n",
      "```python\n",
      "code\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(ds[0][\"prompt\"][0][\"content\"])\n",
    "print(\"-\"*100)\n",
    "print(ds[0][\"prompt\"][1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def test_matmul():\n",
      "    \"\"\"\n",
      "    Test function for pure PyTorch matrix multiplication on DEVICE.\n",
      "\n",
      "    Returns:\n",
      "      dict: Dictionary storing test results for each test case.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "\n",
      "    # Test Case 1: Small square matrices\n",
      "    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\n",
      "    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=DEVICE)\n",
      "    C1 = matmul(A1, B1)\n",
      "    results['test_case_1'] = C1\n",
      "\n",
      "    # Test Case 2: Rectangular matrices\n",
      "    A2 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=DEVICE)\n",
      "    B2 = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], device=DEVICE)\n",
      "    C2 = matmul(A2, B2)\n",
      "    results['test_case_2'] = C2\n",
      "\n",
      "    # Test Case 3: Larger matrices\n",
      "    torch.manual_seed(42)\n",
      "    A3 = torch.randn(64, 128, device=DEVICE)\n",
      "    B3 = torch.randn(128, 32, device=DEVICE)\n",
      "    C3 = matmul(A3, B3)\n",
      "    expected_C3 = torch.mm(A3, B3)\n",
      "    results['test_case_3'] = {\n",
      "        'result': C3,\n",
      "        'expected': expected_C3,\n",
      "        'close': torch.allclose(C3, expected_C3)\n",
      "    }\n",
      "\n",
      "    # Test Case 4: Non-divisible dimensions\n",
      "    A4 = torch.tensor([\n",
      "        [1.0, 2.0, 3.0],\n",
      "        [4.0, 5.0, 6.0],\n",
      "        [7.0, 8.0, 9.0],\n",
      "        [10.0, 11.0, 12.0]\n",
      "    ], device=DEVICE)\n",
      "    B4 = torch.tensor([\n",
      "        [2.0, 0.0],\n",
      "        [1.0, 3.0],\n",
      "        [0.0, 4.0]\n",
      "    ], device=DEVICE)\n",
      "    C4 = matmul(A4, B4)\n",
      "    results['test_case_4'] = C4\n",
      "\n",
      "    # Test Case 5: Random tensors on GPU\n",
      "    if torch.cuda.is_available():\n",
      "        A5 = torch.randn(128, 256, device=DEVICE)\n",
      "        B5 = torch.randn(256, 64, device=DEVICE)\n",
      "        C5 = matmul(A5, B5)\n",
      "        expected_C5 = torch.matmul(A5, B5)\n",
      "        results['test_case_5'] = {\n",
      "            'result': C5,\n",
      "            'expected': expected_C5,\n",
      "            'close': torch.allclose(C5, expected_C5)\n",
      "        }\n",
      "    else:\n",
      "        results['test_case_5'] = 'CUDA not available, skipped GPU test.'\n",
      "\n",
      "    return results\n",
      "\n",
      "########################\n",
      "\n",
      "# Run tests and print only the final test_results dictionary.\n",
      "test_results = test_matmul()\n",
      "print(test_results)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds[0][\"tests\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"entrypoint\", \"triton_code\", \"pt_code_without_tests\", \"tests\"]\n",
    "ds = ds.select_columns(columns_to_keep)\n",
    "# ds.push_to_hub(\"tcapelle/train_ds_triton\", commit_message=\"Filter columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.rename_column(\"pt_code_without_tests\", \"pt_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['entrypoint', 'triton_code', 'pt_code', 'tests'],\n",
       "    num_rows: 847\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run via server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import httpx\n",
    "\n",
    "SERVER_URL = \"http://127.0.0.1:9347\"\n",
    "RUN_CODE_ENDPOINT = f\"{SERVER_URL}/run_code\"\n",
    "\n",
    "client = httpx.Client()\n",
    "\n",
    "def send_run_request(client: httpx.Client, code: str, tests: str):\n",
    "    \"\"\"Sends a request to the /run_code endpoint.\"\"\"\n",
    "    payload = {\n",
    "        \"code\": code,\n",
    "        \"tests\": tests\n",
    "    }\n",
    "    try:\n",
    "        # Use console.print for richer output\n",
    "        # console.print(f\"Sending request for code snippet (first 50 chars): {code[:50]}...\") # Make logging less verbose\n",
    "        start_time = time.monotonic()\n",
    "        response = client.post(RUN_CODE_ENDPOINT, json=payload, timeout=180.0) # Adjusted timeout if needed\n",
    "        end_time = time.monotonic()\n",
    "        duration = end_time - start_time\n",
    "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        # console.print(f\"Request successful (Status: [green]{response.status_code}[/green], Duration: {duration:.2f}s)\") # Make logging less verbose\n",
    "        return response.json(), duration # Return duration along with result\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sample(row):\n",
    "    code = row[\"pt_code\"]\n",
    "    tests = row[\"tests\"]\n",
    "    response, duration = send_run_request(client, code, tests)\n",
    "    return {\"pt_runs\": response[\"status_code\"] == 0, \"pt_stdout\": response[\"stdout\"], \"pt_stderr\": response[\"stderr\"], \"pt_exec_time\": duration}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pt_runs': True,\n",
       " 'pt_stdout': \"{'test_case_1': tensor([[19., 22.],\\n        [43., 50.]], device='cuda:0'), 'test_case_2': tensor([[ 58.,  64.],\\n        [139., 154.]], device='cuda:0'), 'test_case_3': {'result': tensor([[  9.3524,  20.1801,   1.3200,  ..., -21.0338,   3.0357,  -8.3879],\\n        [ -5.5521,   5.0191, -26.5503,  ...,  -5.4739,  -7.3350,  -0.0405],\\n        [  2.6591,  -5.7370,   2.5628,  ...,  22.7629,   1.0609,  -6.0721],\\n        ...,\\n        [  0.7112,  11.1433,   7.8263,  ...,  -8.2718,  -5.5668,  -6.1661],\\n        [ 17.1974,  -6.1684,   1.1457,  ...,  -6.9263, -12.8880,   5.2832],\\n        [-10.5624,   2.1081, -10.1488,  ...,   7.4583,  -1.6897,  -1.7082]],\\n       device='cuda:0'), 'expected': tensor([[  9.3524,  20.1801,   1.3200,  ..., -21.0338,   3.0357,  -8.3879],\\n        [ -5.5521,   5.0191, -26.5503,  ...,  -5.4739,  -7.3350,  -0.0405],\\n        [  2.6591,  -5.7370,   2.5628,  ...,  22.7629,   1.0609,  -6.0721],\\n        ...,\\n        [  0.7112,  11.1433,   7.8263,  ...,  -8.2718,  -5.5668,  -6.1661],\\n        [ 17.1974,  -6.1684,   1.1457,  ...,  -6.9263, -12.8880,   5.2832],\\n        [-10.5624,   2.1081, -10.1488,  ...,   7.4583,  -1.6897,  -1.7082]],\\n       device='cuda:0'), 'close': True}, 'test_case_4': tensor([[ 4., 18.],\\n        [13., 39.],\\n        [22., 60.],\\n        [31., 81.]], device='cuda:0'), 'test_case_5': {'result': tensor([[-20.8893,  18.6847, -25.4520,  ...,  11.5636,   0.8549,  -2.4848],\\n        [  4.9682, -24.7063,   5.0134,  ...,  17.0403, -24.0015,  22.7481],\\n        [ -4.9066,   7.7711,   6.0983,  ..., -12.2099,  13.7487,  -2.7126],\\n        ...,\\n        [-17.5489, -11.0569,   7.5630,  ...,  -6.2937, -21.6412,  19.4340],\\n        [-22.7031,  -7.3231,   6.3431,  ..., -25.8886, -23.7392, -13.3596],\\n        [-23.1386,  -4.5383, -27.9520,  ...,  -9.5806,   2.6319,   8.3291]],\\n       device='cuda:0'), 'expected': tensor([[-20.8893,  18.6847, -25.4520,  ...,  11.5636,   0.8549,  -2.4848],\\n        [  4.9682, -24.7063,   5.0134,  ...,  17.0403, -24.0015,  22.7481],\\n        [ -4.9066,   7.7711,   6.0983,  ..., -12.2099,  13.7487,  -2.7126],\\n        ...,\\n        [-17.5489, -11.0569,   7.5630,  ...,  -6.2937, -21.6412,  19.4340],\\n        [-22.7031,  -7.3231,   6.3431,  ..., -25.8886, -23.7392, -13.3596],\\n        [-23.1386,  -4.5383, -27.9520,  ...,  -9.5806,   2.6319,   8.3291]],\\n       device='cuda:0'), 'close': True}}\\n\",\n",
       " 'pt_stderr': '',\n",
       " 'pt_exec_time': 1.7499170419760048}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=20): 100%|██████████| 847/847 [00:32<00:00, 26.02 examples/s] \n"
     ]
    }
   ],
   "source": [
    "test_ds_cuda = ds.map(test_sample, num_proc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entrypoint': 'matmul',\n",
       " 'triton_code': 'import torch\\nimport triton\\nimport triton.language as tl\\n\\n# Global device standard\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n\\n@triton.jit\\ndef _matmul_kernel(A, B, C, M, N, K, **meta):\\n    \"\"\"Triton kernel for matrix multiplication using tiling.\"\"\"\\n    # Tiling sizes\\n    TILE_M = meta[\\'BLOCK_M\\']\\n    TILE_N = meta[\\'BLOCK_N\\']\\n    TILE_K = 128\\n    \\n    # Indices for output tile computed by the current program instance\\n    m = tl.program_id(0) * TILE_M + tl.arange(0, TILE_M)\\n    n = tl.program_id(1) * TILE_N + tl.arange(0, TILE_N)\\n    \\n    # Initialize the accumulator for the resultant tile\\n    acc = tl.zeros((TILE_M, TILE_N), dtype=tl.float32)\\n    \\n    # Loop over the K dimension with tiles\\n    for k in range(0, K, TILE_K):\\n        a = tl.load(A + m[:, None] * K + k, mask=[m[:, None] < M, None], other=0.0)\\n        b = tl.load(B + k * N + n, mask=[None, n < N], other=0.0)\\n        acc += tl.dot(a, b)\\n    \\n    # Store the computed tile into C\\n    tl.store(C + m[:, None] * N + n, acc, mask=[m[:, None] < M, n < N])\\n\\n\\n\\ndef matmul(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Perform matrix multiplication using a Triton kernel that computes A @ B.\\n\\n    Parameters:\\n      A (torch.Tensor): Tensor of shape (M, K) on DEVICE.\\n      B (torch.Tensor): Tensor of shape (K, N) on DEVICE.\\n\\n    Returns:\\n      torch.Tensor: The product matrix of shape (M, N).\\n    \"\"\"\\n    # Verify dimensions\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\\'Both A and B must be 2D tensors.\\')\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\\'Inner dimensions must match, got A: {A.shape}, B: {B.shape}\\')\\n\\n    # Allocate the output tensor on the global DEVICE\\n    C = torch.empty((M, N), device=DEVICE, dtype=torch.float32)\\n\\n    # Define block sizes (tuning parameters)\\n    BLOCK_M = 64\\n    BLOCK_N = 64\\n    \\n    # Calculate the grid dimensions\\n    grid = ((M + BLOCK_M - 1) // BLOCK_M, (N + BLOCK_N - 1) // BLOCK_N)\\n\\n    # Launch the Triton kernel\\n    _matmul_kernel[grid](A, B, C, M, N, K, meta={\\'BLOCK_M\\': BLOCK_M, \\'BLOCK_N\\': BLOCK_N})\\n    return C\\n\\n########################\\n\\ndef test_matmul():\\n    \"\"\"\\n    Test function for Triton-based matrix multiplication on DEVICE.\\n\\n    Returns:\\n      dict: Dictionary storing test results for each test case.\\n    \"\"\"\\n    results = {}\\n\\n    # Test Case 1: Small square matrices\\n    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\\n    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=DEVICE)\\n    C1 = matmul(A1, B1)\\n    results[\\'test_case_1\\'] = C1\\n\\n    # Test Case 2: Rectangular matrices\\n    A2 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=DEVICE)  # (2, 3)\\n    B2 = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], device=DEVICE)  # (3, 2)\\n    C2 = matmul(A2, B2)\\n    results[\\'test_case_2\\'] = C2\\n\\n    # Test Case 3: Larger matrices\\n    torch.manual_seed(42)\\n    A3 = torch.randn(64, 128, device=DEVICE)\\n    B3 = torch.randn(128, 32, device=DEVICE)\\n    C3 = matmul(A3, B3)\\n    expected_C3 = torch.mm(A3, B3)\\n    results[\\'test_case_3\\'] = {\\n        \\'result\\': C3,\\n        \\'expected\\': expected_C3,\\n        \\'close\\': torch.allclose(C3, expected_C3)\\n    }\\n\\n    # Test Case 4: Non-divisible dimensions\\n    A4 = torch.tensor([\\n        [1.0, 2.0, 3.0], \\n        [4.0, 5.0, 6.0], \\n        [7.0, 8.0, 9.0], \\n        [10.0, 11.0, 12.0]\\n    ], device=DEVICE)  # (4, 3)\\n    B4 = torch.tensor([\\n        [2.0, 0.0], \\n        [1.0, 3.0], \\n        [0.0, 4.0]\\n    ], device=DEVICE)  # (3, 2)\\n    C4 = matmul(A4, B4)\\n    results[\\'test_case_4\\'] = C4\\n\\n    # Test Case 5: Random tensors on GPU\\n    if torch.cuda.is_available():\\n        A5 = torch.randn(128, 256, device=DEVICE)\\n        B5 = torch.randn(256, 64, device=DEVICE)\\n        C5 = matmul(A5, B5)\\n        expected_C5 = torch.matmul(A5, B5)\\n        results[\\'test_case_5\\'] = {\\n            \\'result\\': C5,\\n            \\'expected\\': expected_C5,\\n            \\'close\\': torch.allclose(C5, expected_C5)\\n        }\\n    else:\\n        results[\\'test_case_5\\'] = \\'CUDA not available, skipped GPU test.\\'\\n\\n    return results\\n\\n########################\\n\\n# Run tests and print only the final test_results dictionary.\\ntest_results = test_matmul()\\nprint(test_results)\\n',\n",
       " 'pt_code': 'import torch\\n\\n# Global device standard\\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n\\ndef matmul(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Perform matrix multiplication using pure PyTorch with torch.matmul.\\n\\n    Parameters:\\n      A (torch.Tensor): Tensor of shape (M, K) on DEVICE.\\n      B (torch.Tensor): Tensor of shape (K, N) on DEVICE.\\n\\n    Returns:\\n      torch.Tensor: The product matrix of shape (M, N).\\n    \"\"\"\\n    # Verify dimensions\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\\'Both A and B must be 2D tensors.\\')\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\\'Inner dimensions must match, got A: {A.shape}, B: {B.shape}\\')\\n\\n    # Perform matrix multiplication using torch.matmul\\n    return torch.matmul(A, B)\\n\\n########################\\n\\n',\n",
       " 'tests': 'def test_matmul():\\n    \"\"\"\\n    Test function for pure PyTorch matrix multiplication on DEVICE.\\n\\n    Returns:\\n      dict: Dictionary storing test results for each test case.\\n    \"\"\"\\n    results = {}\\n\\n    # Test Case 1: Small square matrices\\n    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\\n    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=DEVICE)\\n    C1 = matmul(A1, B1)\\n    results[\\'test_case_1\\'] = C1\\n\\n    # Test Case 2: Rectangular matrices\\n    A2 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=DEVICE)\\n    B2 = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], device=DEVICE)\\n    C2 = matmul(A2, B2)\\n    results[\\'test_case_2\\'] = C2\\n\\n    # Test Case 3: Larger matrices\\n    torch.manual_seed(42)\\n    A3 = torch.randn(64, 128, device=DEVICE)\\n    B3 = torch.randn(128, 32, device=DEVICE)\\n    C3 = matmul(A3, B3)\\n    expected_C3 = torch.mm(A3, B3)\\n    results[\\'test_case_3\\'] = {\\n        \\'result\\': C3,\\n        \\'expected\\': expected_C3,\\n        \\'close\\': torch.allclose(C3, expected_C3)\\n    }\\n\\n    # Test Case 4: Non-divisible dimensions\\n    A4 = torch.tensor([\\n        [1.0, 2.0, 3.0],\\n        [4.0, 5.0, 6.0],\\n        [7.0, 8.0, 9.0],\\n        [10.0, 11.0, 12.0]\\n    ], device=DEVICE)\\n    B4 = torch.tensor([\\n        [2.0, 0.0],\\n        [1.0, 3.0],\\n        [0.0, 4.0]\\n    ], device=DEVICE)\\n    C4 = matmul(A4, B4)\\n    results[\\'test_case_4\\'] = C4\\n\\n    # Test Case 5: Random tensors on GPU\\n    if torch.cuda.is_available():\\n        A5 = torch.randn(128, 256, device=DEVICE)\\n        B5 = torch.randn(256, 64, device=DEVICE)\\n        C5 = matmul(A5, B5)\\n        expected_C5 = torch.matmul(A5, B5)\\n        results[\\'test_case_5\\'] = {\\n            \\'result\\': C5,\\n            \\'expected\\': expected_C5,\\n            \\'close\\': torch.allclose(C5, expected_C5)\\n        }\\n    else:\\n        results[\\'test_case_5\\'] = \\'CUDA not available, skipped GPU test.\\'\\n\\n    return results\\n\\n########################\\n\\n# Run tests and print only the final test_results dictionary.\\ntest_results = test_matmul()\\nprint(test_results)\\n',\n",
       " 'pt_runs': True,\n",
       " 'pt_stdout': \"{'test_case_1': tensor([[19., 22.],\\n        [43., 50.]], device='cuda:0'), 'test_case_2': tensor([[ 58.,  64.],\\n        [139., 154.]], device='cuda:0'), 'test_case_3': {'result': tensor([[  9.3524,  20.1801,   1.3200,  ..., -21.0338,   3.0357,  -8.3879],\\n        [ -5.5521,   5.0191, -26.5503,  ...,  -5.4739,  -7.3350,  -0.0405],\\n        [  2.6591,  -5.7370,   2.5628,  ...,  22.7629,   1.0609,  -6.0721],\\n        ...,\\n        [  0.7112,  11.1433,   7.8263,  ...,  -8.2718,  -5.5668,  -6.1661],\\n        [ 17.1974,  -6.1684,   1.1457,  ...,  -6.9263, -12.8880,   5.2832],\\n        [-10.5624,   2.1081, -10.1488,  ...,   7.4583,  -1.6897,  -1.7082]],\\n       device='cuda:0'), 'expected': tensor([[  9.3524,  20.1801,   1.3200,  ..., -21.0338,   3.0357,  -8.3879],\\n        [ -5.5521,   5.0191, -26.5503,  ...,  -5.4739,  -7.3350,  -0.0405],\\n        [  2.6591,  -5.7370,   2.5628,  ...,  22.7629,   1.0609,  -6.0721],\\n        ...,\\n        [  0.7112,  11.1433,   7.8263,  ...,  -8.2718,  -5.5668,  -6.1661],\\n        [ 17.1974,  -6.1684,   1.1457,  ...,  -6.9263, -12.8880,   5.2832],\\n        [-10.5624,   2.1081, -10.1488,  ...,   7.4583,  -1.6897,  -1.7082]],\\n       device='cuda:0'), 'close': True}, 'test_case_4': tensor([[ 4., 18.],\\n        [13., 39.],\\n        [22., 60.],\\n        [31., 81.]], device='cuda:0'), 'test_case_5': {'result': tensor([[-20.8893,  18.6847, -25.4520,  ...,  11.5636,   0.8549,  -2.4848],\\n        [  4.9682, -24.7063,   5.0134,  ...,  17.0403, -24.0015,  22.7481],\\n        [ -4.9066,   7.7711,   6.0983,  ..., -12.2099,  13.7487,  -2.7126],\\n        ...,\\n        [-17.5489, -11.0569,   7.5630,  ...,  -6.2937, -21.6412,  19.4340],\\n        [-22.7031,  -7.3231,   6.3431,  ..., -25.8886, -23.7392, -13.3596],\\n        [-23.1386,  -4.5383, -27.9520,  ...,  -9.5806,   2.6319,   8.3291]],\\n       device='cuda:0'), 'expected': tensor([[-20.8893,  18.6847, -25.4520,  ...,  11.5636,   0.8549,  -2.4848],\\n        [  4.9682, -24.7063,   5.0134,  ...,  17.0403, -24.0015,  22.7481],\\n        [ -4.9066,   7.7711,   6.0983,  ..., -12.2099,  13.7487,  -2.7126],\\n        ...,\\n        [-17.5489, -11.0569,   7.5630,  ...,  -6.2937, -21.6412,  19.4340],\\n        [-22.7031,  -7.3231,   6.3431,  ..., -25.8886, -23.7392, -13.3596],\\n        [-23.1386,  -4.5383, -27.9520,  ...,  -9.5806,   2.6319,   8.3291]],\\n       device='cuda:0'), 'close': True}}\\n\",\n",
       " 'pt_stderr': '',\n",
       " 'pt_exec_time': 3.164108832948841}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds_cuda[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 847/847 [00:00<00:00, 73975.00 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['entrypoint', 'triton_code', 'pt_code', 'tests', 'pt_runs', 'pt_stdout', 'pt_stderr', 'pt_exec_time'],\n",
       "    num_rows: 828\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_cuda_pt = test_ds_cuda.filter(lambda x: x[\"pt_runs\"])\n",
    "runs_cuda_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 20.60ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tcapelle/train_ds_triton/commit/1e4e797ae4bab5c2125ced4ff69783951401f7ff', commit_message='Run CUDA PT', commit_description='', oid='1e4e797ae4bab5c2125ced4ff69783951401f7ff', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tcapelle/train_ds_triton', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tcapelle/train_ds_triton'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_cuda_pt.push_to_hub(\"tcapelle/train_ds_triton\", commit_message=\"Run CUDA PT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status_code': 0,\n",
       " 'stdout': \"{'test_case_1': tensor([[19., 22.],\\n        [43., 50.]]), 'test_case_2': tensor([[ 58.,  64.],\\n        [139., 154.]]), 'test_case_3': {'result': tensor([[ 2.4955e+00,  6.4769e+00,  1.6394e+00,  ...,  1.2249e+01,\\n          2.1542e+01,  8.3416e+00],\\n        [ 7.4504e+00,  1.9378e-02,  1.2411e+01,  ...,  8.3219e+00,\\n          2.8858e+00,  1.4705e+00],\\n        [ 2.8023e+00,  7.2151e+00,  3.0986e+00,  ...,  2.8608e+01,\\n         -1.5909e+01, -2.4647e+01],\\n        ...,\\n        [-7.1271e+00, -1.0447e+01,  9.8994e+00,  ...,  8.3518e+00,\\n         -7.8036e-01, -2.5926e+01],\\n        [ 2.3954e+00,  1.7080e+01, -4.1753e+00,  ..., -5.8380e-01,\\n          1.8727e+00,  2.1891e+00],\\n        [-2.0062e+00, -4.0143e+00, -9.1468e+00,  ..., -1.9226e+01,\\n         -1.0324e+01,  2.3399e+01]]), 'expected': tensor([[ 2.4955e+00,  6.4769e+00,  1.6394e+00,  ...,  1.2249e+01,\\n          2.1542e+01,  8.3416e+00],\\n        [ 7.4504e+00,  1.9378e-02,  1.2411e+01,  ...,  8.3219e+00,\\n          2.8858e+00,  1.4705e+00],\\n        [ 2.8023e+00,  7.2151e+00,  3.0986e+00,  ...,  2.8608e+01,\\n         -1.5909e+01, -2.4647e+01],\\n        ...,\\n        [-7.1271e+00, -1.0447e+01,  9.8994e+00,  ...,  8.3518e+00,\\n         -7.8036e-01, -2.5926e+01],\\n        [ 2.3954e+00,  1.7080e+01, -4.1753e+00,  ..., -5.8380e-01,\\n          1.8727e+00,  2.1891e+00],\\n        [-2.0062e+00, -4.0143e+00, -9.1468e+00,  ..., -1.9226e+01,\\n         -1.0324e+01,  2.3399e+01]]), 'close': True}, 'test_case_4': tensor([[ 4., 18.],\\n        [13., 39.],\\n        [22., 60.],\\n        [31., 81.]]), 'test_case_5': 'CUDA not available, skipped GPU test.'}\\n\",\n",
       " 'stderr': ''}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools import run_python_code\n",
    "\n",
    "def run_with_tests(code: str, tests: str) -> dict:\n",
    "    return run_python_code(code + \"\\n\" + tests)\n",
    "\n",
    "\n",
    "def test_sample_locally(row):\n",
    "    code = row[\"pt_code\"]\n",
    "    tests = row[\"tests\"]\n",
    "    return run_with_tests(code, tests)\n",
    "\n",
    "test_sample_locally(ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function test_sample_locally at 0x14501b560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map (num_proc=4): 100%|██████████| 847/847 [02:18<00:00,  6.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds = ds.map(test_sample_locally, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 847/847 [00:00<00:00, 94182.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "issues_ds = test_ds.filter(lambda x: x[\"status_code\"] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/work/triton_eval/axolotl_dev/temp_files/0fc4e62c-3c92-44f0-a2f7-2cbf7718ce1a.py\", line 183, in <module>\n",
      "    test_results = test_attn_fwd_inner_torch()\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/triton_eval/axolotl_dev/temp_files/0fc4e62c-3c92-44f0-a2f7-2cbf7718ce1a.py\", line 166, in test_attn_fwd_inner_torch\n",
      "    acc3, l_i3, m_i3 = attn_fwd_inner(acc0.clone(), l_i0.clone(), m_i0.clone(),\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/triton_eval/axolotl_dev/temp_files/0fc4e62c-3c92-44f0-a2f7-2cbf7718ce1a.py\", line 67, in attn_fwd_inner\n",
      "    k1 = safe_slice(K1_sub, 1, rel_index, BLOCK_N)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/triton_eval/axolotl_dev/temp_files/0fc4e62c-3c92-44f0-a2f7-2cbf7718ce1a.py\", line 18, in safe_slice\n",
      "    return tensor.narrow(dim, start, block_size)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: start (24) + length (8) exceeds dimension size (28).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(issues_ds[9][\"stderr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def matmul(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Perform matrix multiplication using pure PyTorch with torch.matmul.\n",
      "\n",
      "    Parameters:\n",
      "      A (torch.Tensor): Tensor of shape (M, K) on DEVICE.\n",
      "      B (torch.Tensor): Tensor of shape (K, N) on DEVICE.\n",
      "\n",
      "    Returns:\n",
      "      torch.Tensor: The product matrix of shape (M, N).\n",
      "    \"\"\"\n",
      "    # Verify dimensions\n",
      "    if A.dim() != 2 or B.dim() != 2:\n",
      "        raise ValueError('Both A and B must be 2D tensors.')\n",
      "    M, K = A.shape\n",
      "    K2, N = B.shape\n",
      "    if K != K2:\n",
      "        raise ValueError(f'Inner dimensions must match, got A: {A.shape}, B: {B.shape}')\n",
      "\n",
      "    # Perform matrix multiplication using torch.matmul\n",
      "    return torch.matmul(A, B)\n",
      "\n",
      "########################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds[0][\"entrypoint\"])\n",
    "print(\"=\"*100)\n",
    "print(ds[0][\"pt_code_without_tests\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_test_name(code: str) -> str:\n",
    "    pattern = r\"def test_(.*)\\(\"\n",
    "    match = re.search(pattern, code)\n",
    "    if match:\n",
    "        return \"test_\" + match.group(1)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_matmul'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = ds[0][\"pytorch_code_fixed\"]\n",
    "find_test_name(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_at_tests(code: str, test_name: str) -> tuple[str, str]:\n",
    "    pattern = f\"def {test_name}\"\n",
    "    match_index = code.find(pattern)\n",
    "    if match_index == -1:\n",
    "        return code, \"\"\n",
    "    code_without_tests = code[:match_index]\n",
    "    tests = code[match_index:]\n",
    "    return code_without_tests, tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def matmul(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\\n    \"\"\"\\n    Perform matrix multiplication using pure PyTorch with torch.matmul.\\n\\n    Parameters:\\n      A (torch.Tensor): Tensor of shape (M, K) on DEVICE.\\n      B (torch.Tensor): Tensor of shape (K, N) on DEVICE.\\n\\n    Returns:\\n      torch.Tensor: The product matrix of shape (M, N).\\n    \"\"\"\\n    # Verify dimensions\\n    if A.dim() != 2 or B.dim() != 2:\\n        raise ValueError(\\'Both A and B must be 2D tensors.\\')\\n    M, K = A.shape\\n    K2, N = B.shape\\n    if K != K2:\\n        raise ValueError(f\\'Inner dimensions must match, got A: {A.shape}, B: {B.shape}\\')\\n\\n    # Perform matrix multiplication using torch.matmul\\n    return torch.matmul(A, B)\\n\\n########################\\n\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_name = find_test_name(code)\n",
    "pt_code, test_code = split_at_tests(code, test_name)\n",
    "test_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_test_entrypoint(test_code: str, entrypoint: str) -> str:\n",
    "    test_name = find_test_name(test_code)\n",
    "    print(f\"Replacing `{test_name}` with `test_{entrypoint}`\")  \n",
    "    return test_code.replace(test_name, f\"test_{entrypoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrypoint = ds[0][\"entrypoint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing `` with `test_matmul`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test_matmuldtest_matmuletest_matmulftest_matmul test_matmulmtest_matmulatest_matmulttest_matmulmtest_matmulutest_matmulltest_matmul(test_matmulAtest_matmul:test_matmul test_matmulttest_matmulotest_matmulrtest_matmulctest_matmulhtest_matmul.test_matmulTtest_matmuletest_matmulntest_matmulstest_matmulotest_matmulrtest_matmul,test_matmul test_matmulBtest_matmul:test_matmul test_matmulttest_matmulotest_matmulrtest_matmulctest_matmulhtest_matmul.test_matmulTtest_matmuletest_matmulntest_matmulstest_matmulotest_matmulrtest_matmul)test_matmul test_matmul-test_matmul>test_matmul test_matmulttest_matmulotest_matmulrtest_matmulctest_matmulhtest_matmul.test_matmulTtest_matmuletest_matmulntest_matmulstest_matmulotest_matmulrtest_matmul:test_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmul\"test_matmul\"test_matmul\"test_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmulPtest_matmuletest_matmulrtest_matmulftest_matmulotest_matmulrtest_matmulmtest_matmul test_matmulmtest_matmulatest_matmulttest_matmulrtest_matmulitest_matmulxtest_matmul test_matmulmtest_matmulutest_matmulltest_matmulttest_matmulitest_matmulptest_matmulltest_matmulitest_matmulctest_matmulatest_matmulttest_matmulitest_matmulotest_matmulntest_matmul test_matmulutest_matmulstest_matmulitest_matmulntest_matmulgtest_matmul test_matmulptest_matmulutest_matmulrtest_matmuletest_matmul test_matmulPtest_matmulytest_matmulTtest_matmulotest_matmulrtest_matmulctest_matmulhtest_matmul test_matmulwtest_matmulitest_matmulttest_matmulhtest_matmul test_matmulttest_matmulotest_matmulrtest_matmulctest_matmulhtest_matmul.test_matmulmtest_matmulatest_matmulttest_matmulmtest_matmulutest_matmulltest_matmul.test_matmul\\ntest_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmulPtest_matmulatest_matmulrtest_matmulatest_matmulmtest_matmuletest_matmulttest_matmuletest_matmulrtest_matmulstest_matmul:test_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmul test_matmul test_matmulAtest_matmul test_matmul(test_matmulttest_matmulotest_matmulrtest_matmulctest_matmulhtest_matmul.test_matmulTtest_matmuletest_matmulntest_matmulstest_matmulotest_matmulrtest_matmul)test_matmul:test_matmul test_matmulTtest_matmuletest_matmulntest_matmulstest_matmulotest_matmulrtest_matmul test_matmulotest_matmulftest_matmul test_matmulstest_matmulhtest_matmulatest_matmulptest_matmuletest_matmul test_matmul(test_matmulMtest_matmul,test_matmul test_matmulKtest_matmul)test_matmul test_matmulotest_matmulntest_matmul test_matmulDtest_matmulEtest_matmulVtest_matmulItest_matmulCtest_matmulEtest_matmul.test_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmul test_matmul test_matmulBtest_matmul test_matmul(test_matmulttest_matmulotest_matmulrtest_matmulctest_matmulhtest_matmul.test_matmulTtest_matmuletest_matmulntest_matmulstest_matmulotest_matmulrtest_matmul)test_matmul:test_matmul test_matmulTtest_matmuletest_matmulntest_matmulstest_matmulotest_matmulrtest_matmul test_matmulotest_matmulftest_matmul test_matmulstest_matmulhtest_matmulatest_matmulptest_matmuletest_matmul test_matmul(test_matmulKtest_matmul,test_matmul test_matmulNtest_matmul)test_matmul test_matmulotest_matmulntest_matmul test_matmulDtest_matmulEtest_matmulVtest_matmulItest_matmulCtest_matmulEtest_matmul.test_matmul\\ntest_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmulRtest_matmuletest_matmulttest_matmulutest_matmulrtest_matmulntest_matmulstest_matmul:test_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmul test_matmul test_matmulttest_matmulotest_matmulrtest_matmulctest_matmulhtest_matmul.test_matmulTtest_matmuletest_matmulntest_matmulstest_matmulotest_matmulrtest_matmul:test_matmul test_matmulTtest_matmulhtest_matmuletest_matmul test_matmulptest_matmulrtest_matmulotest_matmuldtest_matmulutest_matmulctest_matmulttest_matmul test_matmulmtest_matmulatest_matmulttest_matmulrtest_matmulitest_matmulxtest_matmul test_matmulotest_matmulftest_matmul test_matmulstest_matmulhtest_matmulatest_matmulptest_matmuletest_matmul test_matmul(test_matmulMtest_matmul,test_matmul test_matmulNtest_matmul)test_matmul.test_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmul\"test_matmul\"test_matmul\"test_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmul#test_matmul test_matmulVtest_matmuletest_matmulrtest_matmulitest_matmulftest_matmulytest_matmul test_matmuldtest_matmulitest_matmulmtest_matmuletest_matmulntest_matmulstest_matmulitest_matmulotest_matmulntest_matmulstest_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmulitest_matmulftest_matmul test_matmulAtest_matmul.test_matmuldtest_matmulitest_matmulmtest_matmul(test_matmul)test_matmul test_matmul!test_matmul=test_matmul test_matmul2test_matmul test_matmulotest_matmulrtest_matmul test_matmulBtest_matmul.test_matmuldtest_matmulitest_matmulmtest_matmul(test_matmul)test_matmul test_matmul!test_matmul=test_matmul test_matmul2test_matmul:test_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmul test_matmul test_matmul test_matmul test_matmulrtest_matmulatest_matmulitest_matmulstest_matmuletest_matmul test_matmulVtest_matmulatest_matmulltest_matmulutest_matmuletest_matmulEtest_matmulrtest_matmulrtest_matmulotest_matmulrtest_matmul(test_matmul\\'test_matmulBtest_matmulotest_matmulttest_matmulhtest_matmul test_matmulAtest_matmul test_matmulatest_matmulntest_matmuldtest_matmul test_matmulBtest_matmul test_matmulmtest_matmulutest_matmulstest_matmulttest_matmul test_matmulbtest_matmuletest_matmul test_matmul2test_matmulDtest_matmul test_matmulttest_matmuletest_matmulntest_matmulstest_matmulotest_matmulrtest_matmulstest_matmul.test_matmul\\'test_matmul)test_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmulMtest_matmul,test_matmul test_matmulKtest_matmul test_matmul=test_matmul test_matmulAtest_matmul.test_matmulstest_matmulhtest_matmulatest_matmulptest_matmuletest_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmulKtest_matmul2test_matmul,test_matmul test_matmulNtest_matmul test_matmul=test_matmul test_matmulBtest_matmul.test_matmulstest_matmulhtest_matmulatest_matmulptest_matmuletest_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmulitest_matmulftest_matmul test_matmulKtest_matmul test_matmul!test_matmul=test_matmul test_matmulKtest_matmul2test_matmul:test_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmul test_matmul test_matmul test_matmul test_matmulrtest_matmulatest_matmulitest_matmulstest_matmuletest_matmul test_matmulVtest_matmulatest_matmulltest_matmulutest_matmuletest_matmulEtest_matmulrtest_matmulrtest_matmulotest_matmulrtest_matmul(test_matmulftest_matmul\\'test_matmulItest_matmulntest_matmulntest_matmuletest_matmulrtest_matmul test_matmuldtest_matmulitest_matmulmtest_matmuletest_matmulntest_matmulstest_matmulitest_matmulotest_matmulntest_matmulstest_matmul test_matmulmtest_matmulutest_matmulstest_matmulttest_matmul test_matmulmtest_matmulatest_matmulttest_matmulctest_matmulhtest_matmul,test_matmul test_matmulgtest_matmulotest_matmulttest_matmul test_matmulAtest_matmul:test_matmul test_matmul{test_matmulAtest_matmul.test_matmulstest_matmulhtest_matmulatest_matmulptest_matmuletest_matmul}test_matmul,test_matmul test_matmulBtest_matmul:test_matmul test_matmul{test_matmulBtest_matmul.test_matmulstest_matmulhtest_matmulatest_matmulptest_matmuletest_matmul}test_matmul\\'test_matmul)test_matmul\\ntest_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmul#test_matmul test_matmulPtest_matmuletest_matmulrtest_matmulftest_matmulotest_matmulrtest_matmulmtest_matmul test_matmulmtest_matmulatest_matmulttest_matmulrtest_matmulitest_matmulxtest_matmul test_matmulmtest_matmulutest_matmulltest_matmulttest_matmulitest_matmulptest_matmulltest_matmulitest_matmulctest_matmulatest_matmulttest_matmulitest_matmulotest_matmulntest_matmul test_matmulutest_matmulstest_matmulitest_matmulntest_matmulgtest_matmul test_matmulttest_matmulotest_matmulrtest_matmulctest_matmulhtest_matmul.test_matmulmtest_matmulatest_matmulttest_matmulmtest_matmulutest_matmulltest_matmul\\ntest_matmul test_matmul test_matmul test_matmul test_matmulrtest_matmuletest_matmulttest_matmulutest_matmulrtest_matmulntest_matmul test_matmulttest_matmulotest_matmulrtest_matmulctest_matmulhtest_matmul.test_matmulmtest_matmulatest_matmulttest_matmulmtest_matmulutest_matmulltest_matmul(test_matmulAtest_matmul,test_matmul test_matmulBtest_matmul)test_matmul\\ntest_matmul\\ntest_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul#test_matmul\\ntest_matmul\\ntest_matmul'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rename_test_entrypoint(test_code, entrypoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_test_entrypoint(row: dict) -> dict:\n",
    "    if row[\"tests\"] == \"\" and row[\"pt_code_without_tests\"] != \"\":\n",
    "        entrypoint = row[\"entrypoint\"]\n",
    "        pt_code_without_tests = row[\"pt_code_without_tests\"]\n",
    "        test_name = find_test_name(pt_code_without_tests)\n",
    "        pt_code, test_code = split_at_tests(pt_code_without_tests, test_name)\n",
    "        test_code = rename_test_entrypoint(test_code, entrypoint)\n",
    "        return {\n",
    "            \"pt_code_without_tests\": pt_code,\n",
    "            \"tests\": test_code}\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 4474.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing `test_unpack64` with `test_pytorch_unpack64`\n",
      "Replacing `test_custom_attention` with `test_custom_attention_forward`\n",
      "Replacing `test_fourth_order_fwd` with `test_torch_fourth_order_fwd`\n",
      "Replacing `test_layer_norm_fwd_fused` with `test_layer_norm_fwd_fused_`\n",
      "Replacing `test__single_query_cached_kv_attention_v2_torch` with `test_torch_single_query_cached_kv_attention_v2`\n",
      "Replacing `test_bwd_intra` with `test_pytorch_bwd_intra`\n",
      "Replacing `test_elementwise_mul` with `test_elementwise_mul_`\n",
      "Replacing `test_rms_norm_fwd` with `test_pytorch_rms_norm_fwd`\n",
      "Replacing `test_rotary_embedding` with `test_apply_rotary_embedding`\n",
      "Replacing `test_fused_chunk_delta_rule_bwd` with `test_fused_chunk_delta_rule_bwd_`\n",
      "Replacing `test_bwd_block` with `test_torch_bwd_block`\n",
      "Replacing `test_bwd_decay_global_cumsum` with `test_bwd_decay_global_cumsum_`\n",
      "Replacing `test_attn_fwd_inner_torch` with `test_torch_attn_fwd_inner`\n",
      "Replacing `test_atomic_kernel` with `test_atomic_kernel_`\n",
      "Replacing `test_cross_entropy_forward` with `test_cross_entropy_forward_`\n",
      "Replacing `test_softmax_kernel_online_v2_pytorch` with `test_softmax_online`\n",
      "Replacing `test_attn_fwd_inner` with `test_attn_fwd_inner_`\n",
      "Replacing `test_output_sparse_matmul_torch` with `test_torch_output_sparse_matmul`\n",
      "Replacing `test_dequantize` with `test_dequantize_`\n",
      "Replacing `test_rms_norm_bwd` with `test_rms_norm_bwd_`\n",
      "Replacing `test_update_ema` with `test_pytorch_update_ema`\n",
      "Replacing `test_tanh` with `test_pytorch_tanh`\n",
      "Replacing `test_dense_to_sparse` with `test_pytorch_dense_to_sparse`\n",
      "Replacing `test_dynamic_quantize` with `test_dynamic_quantize_`\n",
      "Replacing `test_paged_attn_wo_mma_v2_reduce` with `test_paged_attn_wo_mma_v2_reduce_`\n",
      "Replacing `test_bgmv_expand_slice` with `test_pytorch_bgmv_expand_slice`\n",
      "Replacing `test_pytorch_sum_kernel_1d_result_sum_then_buffer` with `test_sum_kernel_1d_result_sum_then_buffer`\n",
      "Replacing `test_sum_dim0_in_fp32` with `test_pytorch_sum_dim0_in_fp32`\n",
      "Replacing `test_forward` with `test_linear_forward`\n",
      "Replacing `test_matmul_AT_B` with `test_pytorch_matmul_AT_B`\n",
      "Replacing `test_chunk_transform_qk_fwd` with `test_chunk_transform_qk_fwd_`\n",
      "Replacing `test_associative_rnn_scan` with `test_associative_rnn_scan_fwd`\n",
      "Replacing `test_inner_paged_attn_unroll_8` with `test_pytorch_inner_paged_attn_unroll_8`\n",
      "Replacing `test_modulation_gate_proj` with `test_modulation_gate_proj_pure`\n",
      "Replacing `test_chunk_retention_fwd` with `test_chunk_retention_fwd_`\n",
      "Replacing `test_matmul` with `test_matmul_`\n",
      "Replacing `test_bwd_prepare_wy_repr` with `test_pytorch_bwd_prepare_wy_repr`\n",
      "Replacing `test_mlstm_matmul_kernel_backward` with `test_pytorch_mlstm_matmul_kernel_backward`\n",
      "Replacing `test_layer_norm_modulation_fwd` with `test_layer_norm_modulation_fwd_`\n",
      "Replacing `test_matmul_quant_kernel_pytorch` with `test_quantized_matmul`\n",
      "Replacing `test_attn_fwd_torch` with `test_torch_attn_fwd`\n",
      "Replacing `test_fused_moe` with `test_fused_moe_`\n",
      "Replacing `test_chunk_hgrn_bwd_o` with `test_chunk_hgrn_bwd_o_`\n",
      "Replacing `test_pytorch_chunk_delta_rule_fwd_kernel_prepare_dv` with `test_chunk_delta_rule_fwd_kernel_prepare_dv`\n",
      "Replacing `test_pytorch_chunk_abc_fwd_kernel_K` with `test_chunk_abc_fwd`\n",
      "Replacing `test_softmax_fwd` with `test_pytorch_softmax`\n",
      "Replacing `test_red_fused_mv_0_pytorch` with `test_triton_red_fused_mv_0`\n",
      "Replacing `test_softmax_torch` with `test_softmax_2d`\n",
      "Replacing `test_chunk_global_reversed_cumsum_vector` with `test_pytorch_chunk_global_reversed_cumsum_vector`\n",
      "Replacing `test_kernel` with `test_test_kernel`\n",
      "Replacing `test_pytorch_jagged_sum` with `test_jagged_sum`\n",
      "Replacing `test_rms_norm_backward_torch` with `test_rms_norm_backward_torch_impl`\n",
      "Replacing `test_random_matrix` with `test_torch_random_matrix`\n",
      "Replacing `test__ReLULinearAddBackward` with `test_relu_linear_add_backward`\n",
      "Replacing `test_attn_fwd` with `test_attn_fwd_`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fixed_ds = filtered_ds.map(fix_test_entrypoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch_unpack64\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "import math\n",
      "\n",
      "def pytorch_unpack64(merged: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
      "    \"\"\"\n",
      "    Decomposes a 64-bit unsigned integer tensor into two 32-bit floats by bitmasking and\n",
      "    bit-level reinterpretation. The upper 32 bits produce the first float and the lower 32 bits produce the second.\n",
      "    \"\"\"\n",
      "    if merged.dtype != torch.uint64:\n",
      "        raise ValueError('Input tensor must be of dtype torch.uint64')\n",
      "\n",
      "    # Extract lower and upper 32 bits\n",
      "    mask_val = 0xffffffff  # Mask for lower 32 bits\n",
      "\n",
      "    # Workaround: torch.bitwise_and for uint64 is not implemented on CUDA, so cast to CPU for bitwise ops\n",
      "    if merged.device.type == 'cuda':\n",
      "        merged_cpu = merged.cpu()\n",
      "        lower = merged_cpu & mask_val\n",
      "        upper = merged_cpu >> 32\n",
      "        lower = lower.to(DEVICE)\n",
      "        upper = upper.to(DEVICE)\n",
      "    else:\n",
      "        lower = merged & mask_val\n",
      "        upper = merged >> 32\n",
      "\n",
      "    # Convert to int32 to preserve the exact bit pattern and bit-cast to float32\n",
      "    lower_int32 = lower.to(torch.int32)\n",
      "    upper_int32 = upper.to(torch.int32)\n",
      "    lower_float = lower_int32.view(torch.float32)\n",
      "    upper_float = upper_int32.view(torch.float32)\n",
      "\n",
      "    return upper_float, lower_float\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_unpack64():\n",
      "    results = {}\n",
      "\n",
      "    # Helper: Convert a Python float to its IEEE 754 bit representation (unsigned 32-bit int)\n",
      "    def float_to_bits(x):\n",
      "        return torch.tensor(x, dtype=torch.float32, device=DEVICE).view(torch.int32).item() & 0xffffffff\n",
      "\n",
      "    # Test Case 1: Simple case with zero\n",
      "    input_tensor1 = torch.tensor([0], dtype=torch.uint64, device=DEVICE)\n",
      "    p_a1, p_b1 = pytorch_unpack64(input_tensor1)\n",
      "    results['test_case_1'] = {'input': input_tensor1, 'pytorch_a': p_a1, 'pytorch_b': p_b1}\n",
      "\n",
      "    # Test Case 2: Known bit-pattern for float32 1.0 and 2.0\n",
      "    upper_val = 0x3F800000  # bits for 1.0\n",
      "    lower_val = 0x40000000  # bits for 2.0\n",
      "    merged_val = (upper_val << 32) | lower_val\n",
      "    input_tensor2 = torch.tensor([merged_val], dtype=torch.uint64, device=DEVICE)\n",
      "    p_a2, p_b2 = pytorch_unpack64(input_tensor2)\n",
      "    results['test_case_2'] = {'input': input_tensor2,\n",
      "                              'pytorch_a (expected 1.0)': p_a2,\n",
      "                              'pytorch_b (expected 2.0)': p_b2}\n",
      "\n",
      "    # Test Case 3: Multiple values\n",
      "    float_pairs = [(0.0, 0.0), (1.0, -1.0), (2.5, 3.5)]\n",
      "    merged_list = []\n",
      "    for upper_float, lower_float in float_pairs:\n",
      "        upper_bits = float_to_bits(upper_float)\n",
      "        lower_bits = float_to_bits(lower_float)\n",
      "        merged_val = (upper_bits << 32) | lower_bits\n",
      "        merged_list.append(merged_val)\n",
      "    input_tensor3 = torch.tensor(merged_list, dtype=torch.uint64, device=DEVICE)\n",
      "    p_a3, p_b3 = pytorch_unpack64(input_tensor3)\n",
      "    results['test_case_3'] = {'input': input_tensor3, 'pytorch_a': p_a3, 'pytorch_b': p_b3}\n",
      "\n",
      "    # Test Case 4: Random values and reconstruction check\n",
      "    torch.manual_seed(0)\n",
      "    input_tensor4 = torch.randint(0, 2**63, (10,), dtype=torch.uint64, device=DEVICE)\n",
      "    p_a4, p_b4 = pytorch_unpack64(input_tensor4)\n",
      "    p_a4_int = p_a4.view(torch.int32).to(torch.uint64)\n",
      "    p_b4_int = p_b4.view(torch.int32).to(torch.uint64)\n",
      "    p_merged_reconstructed = (p_a4_int << 32) | p_b4_int\n",
      "    results['test_case_4'] = {'input': input_tensor4,\n",
      "                              'pytorch_a': p_a4,\n",
      "                              'pytorch_b': p_b4,\n",
      "                              'pytorch_merged_reconstructed': p_merged_reconstructed}\n",
      "\n",
      "    print(results)\n",
      "    return results\n",
      "\n",
      "# Execute tests and print only the final test_results dictionary\n",
      "\n",
      "test_results = test_pytorch_unpack64()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "custom_attention_forward\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard: All tensors are created on the proper CUDA device if available.\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def custom_attention_forward(Q1: torch.Tensor,\n",
      "                                     Q2: torch.Tensor,\n",
      "                                     K1: torch.Tensor,\n",
      "                                     K2: torch.Tensor,\n",
      "                                     V: torch.Tensor,\n",
      "                                     softmax_scale: float,\n",
      "                                     window_size: int,\n",
      "                                     write_lse: bool = False):\n",
      "    \"\"\"\n",
      "    Compute a custom multi-head attention forward pass using PyTorch operations.\n",
      "    \n",
      "    Parameters:\n",
      "      Q1, Q2: Query tensors of shape [B, H, seqlen_q, head_dim]\n",
      "      K1, K2: Key tensors of shape [B, H, seqlen_k, head_dim]\n",
      "      V:      Value tensor of shape [B, H, seqlen_k, head_dim]\n",
      "      softmax_scale: Scaling factor applied to the attention scores.\n",
      "      window_size: Only keys within the window (|i - j| <= window_size) contribute.\n",
      "      write_lse: If True, also return LSE (log-sum-exp values) computed per query position.\n",
      "\n",
      "    Returns:\n",
      "      output: attention output of shape [B, H, seqlen_q, head_dim]\n",
      "      lse (optional): log-sum-exp of the scaled scores along the key dimension, shape [B, H, seqlen_q]\n",
      "    \"\"\"\n",
      "    # Combine query and key components.\n",
      "    Q = Q1 + Q2\n",
      "    K = K1 + K2\n",
      "\n",
      "    B, H, seqlen_q, head_dim = Q.shape\n",
      "    seqlen_k = K.shape[2]\n",
      "\n",
      "    # Compute scaled dot-product attention scores.\n",
      "    scores = torch.matmul(Q, K.transpose(-2, -1)) * softmax_scale\n",
      "\n",
      "    # Create a window mask so that only keys within the specified window contribute.\n",
      "    i_idx = torch.arange(seqlen_q, device=DEVICE).unsqueeze(1)  # [seqlen_q, 1]\n",
      "    j_idx = torch.arange(seqlen_k, device=DEVICE).unsqueeze(0)  # [1, seqlen_k]\n",
      "    window_mask = (torch.abs(i_idx - j_idx) <= window_size)  # boolean mask\n",
      "    window_mask = window_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seqlen_q, seqlen_k]\n",
      "\n",
      "    # Set positions outside the window to -inf so they contribute zero after softmax.\n",
      "    scores = scores.masked_fill(~window_mask, float('-inf'))\n",
      "\n",
      "    # Compute the log-sum-exp for numerical stability.\n",
      "    lse = torch.logsumexp(scores, dim=-1)  # [B, H, seqlen_q]\n",
      "\n",
      "    # Compute attention weights and then the output.\n",
      "    attn = torch.softmax(scores, dim=-1)\n",
      "    output = torch.matmul(attn, V)\n",
      "\n",
      "    if write_lse:\n",
      "        return output, lse\n",
      "    return output\n",
      "\n",
      "########################\n",
      "# Testing Functions\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_custom_attention_forward():\n",
      "    results = {}\n",
      "    torch.manual_seed(42)\n",
      "\n",
      "    # Test Case 1: Basic functionality without LSE computation.\n",
      "    B, H, seqlen_q, seqlen_k, head_dim = 1, 2, 4, 4, 8\n",
      "    Q1 = torch.randn(B, H, seqlen_q, head_dim, device=DEVICE)\n",
      "    Q2 = torch.randn(B, H, seqlen_q, head_dim, device=DEVICE)\n",
      "    K1 = torch.randn(B, H, seqlen_k, head_dim, device=DEVICE)\n",
      "    K2 = torch.randn(B, H, seqlen_k, head_dim, device=DEVICE)\n",
      "    V  = torch.randn(B, H, seqlen_k, head_dim, device=DEVICE)\n",
      "    softmax_scale = 0.125\n",
      "    window_size = seqlen_k\n",
      "\n",
      "    out_pytorch = custom_attention_forward(Q1, Q2, K1, K2, V, softmax_scale, window_size, write_lse=False)\n",
      "    results[\"test_case_1_pytorch_output_shape\"] = out_pytorch.shape\n",
      "\n",
      "    # Test Case 2: With window_size = 1 and LSE computation.\n",
      "    window_size = 1\n",
      "    out_pytorch, lse_pytorch = custom_attention_forward(Q1, Q2, K1, K2, V, softmax_scale, window_size, write_lse=True)\n",
      "    results[\"test_case_2_pytorch_output_shape\"] = out_pytorch.shape\n",
      "    results[\"test_case_2_pytorch_lse_shape\"] = lse_pytorch.shape\n",
      "\n",
      "    # Test Case 3: Different tensor dimensions without LSE computation.\n",
      "    B, H, seqlen_q, seqlen_k, head_dim = 2, 4, 16, 20, 32\n",
      "    Q1 = torch.randn(B, H, seqlen_q, head_dim, device=DEVICE)\n",
      "    Q2 = torch.randn(B, H, seqlen_q, head_dim, device=DEVICE)\n",
      "    K1 = torch.randn(B, H, seqlen_k, head_dim, device=DEVICE)\n",
      "    K2 = torch.randn(B, H, seqlen_k, head_dim, device=DEVICE)\n",
      "    V  = torch.randn(B, H, seqlen_k, head_dim, device=DEVICE)\n",
      "    softmax_scale = 0.1\n",
      "    window_size = 5\n",
      "    out_pytorch = custom_attention_forward(Q1, Q2, K1, K2, V, softmax_scale, window_size, write_lse=False)\n",
      "    results[\"test_case_3_pytorch_output_shape\"] = out_pytorch.shape\n",
      "\n",
      "    # Test Case 4: Edge case with window_size = 0 and LSE computation.\n",
      "    window_size = 0\n",
      "    out_pytorch, lse_pytorch = custom_attention_forward(Q1, Q2, K1, K2, V, softmax_scale, window_size, write_lse=True)\n",
      "    results[\"test_case_4_pytorch_output_shape\"] = out_pytorch.shape\n",
      "    results[\"test_case_4_pytorch_lse_values\"] = lse_pytorch.detach().cpu().tolist()\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "# Execute tests and output the test_results dictionary.\n",
      "test_results = test_custom_attention_forward()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "torch_fourth_order_fwd\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Set global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def torch_fourth_order_fwd(coords: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"Applies a series of fourth-order polynomial transformations on input coordinates using PyTorch operations on DEVICE.\n",
      "    Input tensor must have shape (N, 3).\n",
      "    \"\"\"\n",
      "    if coords.ndim != 2 or coords.size(1) != 3:\n",
      "        raise ValueError(\"Input tensor must have shape (N, 3)\")\n",
      "    coords = coords.to(DEVICE)\n",
      "    x = coords[:, 0]\n",
      "    y = coords[:, 1]\n",
      "    z = coords[:, 2]\n",
      "\n",
      "    # Pre-defined constants\n",
      "    CONST000 = 1.125\n",
      "    CONST001 = 2.25\n",
      "    CONST002 = 3.0\n",
      "    CONST005 = 2.21852991866236\n",
      "    CONST007 = 9.48683298050514\n",
      "    CONST010 = 20.1246117974981\n",
      "    CONST011 = -18.8248505970167\n",
      "    CONST012 = -13.3111795119741\n",
      "    CONST013 = -10.0623058987491\n",
      "    CONST014 = -9.0\n",
      "    CONST015 = -8.87411967464942\n",
      "    CONST016 = -7.11512473537885\n",
      "    CONST017 = -6.27495019900557\n",
      "    CONST018 = -3.35410196624968\n",
      "    CONST019 = -1.67705098312484\n",
      "\n",
      "    # Compute required powers\n",
      "    VAR06 = x**4\n",
      "    VAR07 = x**3\n",
      "    VAR08 = x**2\n",
      "    VAR15 = y**4\n",
      "    VAR16 = y**3\n",
      "    VAR17 = y**2\n",
      "    VAR24 = z**4\n",
      "    VAR25 = z**3\n",
      "    VAR26 = z**2\n",
      "\n",
      "    # Compute outputs\n",
      "    Y00 = CONST015 * VAR07 * z - CONST015 * VAR25 * x\n",
      "    Y01 = y * (-CONST011 * VAR26 * x + CONST017 * VAR07)\n",
      "    Y02 = CONST018 * VAR07 * z + x * (CONST010 * VAR17 * z + CONST018 * VAR25)\n",
      "    Y03 = CONST016 * VAR07 * y + x * (CONST007 * VAR16 + CONST016 * VAR26 * y)\n",
      "    Y04 = (CONST000 * VAR06 + CONST000 * VAR24 + CONST002 * VAR15 +\n",
      "           CONST014 * VAR17 * VAR26 + VAR08 * (CONST001 * VAR26 + CONST014 * VAR17))\n",
      "    Y05 = CONST016 * VAR25 * y + z * (CONST007 * VAR16 + CONST016 * VAR08 * y)\n",
      "    Y06 = -CONST019 * VAR06 + CONST019 * VAR24 + VAR17 * (CONST013 * VAR08 - CONST013 * VAR26)\n",
      "    Y07 = y * (CONST011 * VAR08 * z - CONST017 * VAR25)\n",
      "    Y08 = CONST005 * VAR06 + CONST005 * VAR24 + CONST012 * VAR08 * VAR26\n",
      "    output = torch.stack([Y00, Y01, Y02, Y03, Y04, Y05, Y06, Y07, Y08], dim=1)\n",
      "    return output\n",
      "\n",
      "\n",
      "########################\n",
      "# Testing the PyTorch implementation\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_torch_fourth_order_fwd():\n",
      "    test_results = {}\n",
      "\n",
      "    # Test case 1: Single coordinate\n",
      "    coords_1 = torch.tensor([[1.0, 2.0, 3.0]], dtype=torch.float32, device=DEVICE)\n",
      "    torch_out1 = torch_fourth_order_fwd(coords_1)\n",
      "    test_results[\"single_coordinate\"] = torch_out1.detach().cpu().numpy()\n",
      "\n",
      "    # Test case 2: Multiple coordinates\n",
      "    coords_2 = torch.tensor([\n",
      "        [0.0, 0.0, 0.0],\n",
      "        [1.0, 1.0, 1.0],\n",
      "        [-1.0, -1.0, -1.0],\n",
      "        [2.0, 0.5, -0.5],\n",
      "        [-2.0, 1.5, 3.0]\n",
      "    ], dtype=torch.float32, device=DEVICE)\n",
      "    torch_out2 = torch_fourth_order_fwd(coords_2)\n",
      "    test_results[\"multiple_coordinates\"] = torch_out2.detach().cpu().numpy()\n",
      "\n",
      "    # Test case 3: Random coordinates\n",
      "    torch.manual_seed(42)\n",
      "    coords_3 = torch.randn(10, 3, dtype=torch.float32, device=DEVICE)\n",
      "    torch_out3 = torch_fourth_order_fwd(coords_3)\n",
      "    test_results[\"random_coordinates\"] = torch_out3.detach().cpu().numpy()\n",
      "\n",
      "    # Test case 4: Empty tensor of shape (0, 3)\n",
      "    coords_4 = torch.empty((0, 3), dtype=torch.float32, device=DEVICE)\n",
      "    torch_out4 = torch_fourth_order_fwd(coords_4)\n",
      "    test_results[\"empty_tensor\"] = torch_out4.detach().cpu().numpy()\n",
      "\n",
      "    return test_results\n",
      "\n",
      "\n",
      "# Execute tests and print the test_results dictionary\n",
      "\n",
      "test_results = test_torch_fourth_order_fwd()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "layer_norm_fwd_fused_\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "import math\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "def layer_norm_fwd_fused_(X, weight=None, bias=None, eps=1e-5, return_stats=False):\n",
      "    \"\"\"\n",
      "    Performs a fused forward pass of layer normalization on a 2D tensor X.\n",
      "    Each row of X is normalized using its mean and variance over the last dimension.\n",
      "    Optionally applies an affine transform if weight and bias are provided.\n",
      "    If return_stats is True, also returns the computed mean and reciprocal std (rstd).\n",
      "\n",
      "    Args:\n",
      "        X (torch.Tensor): Input tensor of shape (batch_size, N).\n",
      "        weight (torch.Tensor, optional): Scale tensor of shape (N,).\n",
      "        bias (torch.Tensor, optional): Bias tensor of shape (N,).\n",
      "        eps (float, optional): Small constant for numerical stability.\n",
      "        return_stats (bool, optional): Whether to return mean and rstd.\n",
      "\n",
      "    Returns:\n",
      "        Y (torch.Tensor): Normalized (and optionally affine transformed) tensor with the same shape as X.\n",
      "        Optionally, (mean, rstd) each of shape (batch_size,) if return_stats is True.\n",
      "    \"\"\"\n",
      "    if X.dim() != 2:\n",
      "        raise ValueError('X must be a 2D tensor')\n",
      "\n",
      "    # Compute mean and variance along the last dimension\n",
      "    mean = X.mean(dim=1, keepdim=True)\n",
      "    var = X.var(dim=1, unbiased=False, keepdim=True)\n",
      "    rstd = 1.0 / torch.sqrt(var + eps)\n",
      "\n",
      "    # Normalize X\n",
      "    x_hat = (X - mean) * rstd\n",
      "\n",
      "    # Apply affine transformation if provided\n",
      "    if weight is not None or bias is not None:\n",
      "        if weight is None:\n",
      "            weight = torch.ones(X.size(1), dtype=X.dtype, device=DEVICE)\n",
      "        if bias is None:\n",
      "            bias = torch.zeros(X.size(1), dtype=X.dtype, device=DEVICE)\n",
      "        weight = weight.unsqueeze(0)\n",
      "        bias = bias.unsqueeze(0)\n",
      "        Y = x_hat * weight + bias\n",
      "    else:\n",
      "        Y = x_hat\n",
      "\n",
      "    if return_stats:\n",
      "        return Y, mean.squeeze(1), rstd.squeeze(1)\n",
      "    else:\n",
      "        return Y\n",
      "\n",
      "########################\n",
      "# Integrated Tests for PyTorch Implementation\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_layer_norm_fwd_fused_():\n",
      "    \"\"\"Executes integrated test cases for the PyTorch implementation.\n",
      "    Returns a dictionary with the test results.\n",
      "    \"\"\"\n",
      "    torch.manual_seed(0)\n",
      "    results = {}\n",
      "\n",
      "    # Test case 1: Single row without affine\n",
      "    X1 = torch.tensor([[1.0, 2.0, 3.0, 4.0]], dtype=torch.float32, device=DEVICE)\n",
      "    out1, mean1, rstd1 = layer_norm_fwd_fused_(X1, weight=None, bias=None, eps=1e-5, return_stats=True)\n",
      "    results['single_row_no_affine'] = {\n",
      "        'input': X1,\n",
      "        'output': out1,\n",
      "        'mean': mean1,\n",
      "        'rstd': rstd1\n",
      "    }\n",
      "\n",
      "    # Test case 2: Single row with affine\n",
      "    weight2 = torch.tensor([1.0, 0.5, 2.0, -1.0], dtype=torch.float32, device=DEVICE)\n",
      "    bias2 = torch.tensor([0.0, 1.0, -1.0, 0.5], dtype=torch.float32, device=DEVICE)\n",
      "    X2 = torch.tensor([[4.0, 3.0, 2.0, 1.0]], dtype=torch.float32, device=DEVICE)\n",
      "    out2, mean2, rstd2 = layer_norm_fwd_fused_(X2, weight2, bias2, eps=1e-5, return_stats=True)\n",
      "    results['single_row_with_affine'] = {\n",
      "        'input': X2,\n",
      "        'weight': weight2,\n",
      "        'bias': bias2,\n",
      "        'output': out2,\n",
      "        'mean': mean2,\n",
      "        'rstd': rstd2\n",
      "    }\n",
      "\n",
      "    # Test case 3: Multiple rows without affine\n",
      "    X3 = torch.tensor([\n",
      "        [1.0, 2.0, 3.0, 4.0],\n",
      "        [4.0, 3.0, 2.0, 1.0],\n",
      "        [2.0, 2.0, 2.0, 2.0]\n",
      "    ], dtype=torch.float32, device=DEVICE)\n",
      "    out3, mean3, rstd3 = layer_norm_fwd_fused_(X3, weight=None, bias=None, eps=1e-5, return_stats=True)\n",
      "    results['multi_row_no_affine'] = {\n",
      "        'input': X3,\n",
      "        'output': out3,\n",
      "        'mean': mean3,\n",
      "        'rstd': rstd3\n",
      "    }\n",
      "\n",
      "    # Test case 4: Multiple rows with affine\n",
      "    weight4 = torch.tensor([0.5, 1.0, 1.5, 2.0], dtype=torch.float32, device=DEVICE)\n",
      "    bias4 = torch.tensor([1.0, -1.0, 0.0, 0.5], dtype=torch.float32, device=DEVICE)\n",
      "    X4 = torch.randn(5, 8, device=DEVICE)  # 5 rows, 8 columns\n",
      "    X4_cut = X4[:, :4]  # using first 4 columns for affine\n",
      "    out4 = layer_norm_fwd_fused_(X4_cut, weight4, bias4, eps=1e-5, return_stats=False)\n",
      "    results['multi_row_with_affine'] = {\n",
      "        'input': X4_cut,\n",
      "        'weight': weight4,\n",
      "        'bias': bias4,\n",
      "        'output': out4\n",
      "    }\n",
      "\n",
      "    # Test case 5: Consistency with torch.nn.functional.layer_norm\n",
      "    X5 = torch.randn(10, 16, device=DEVICE)\n",
      "    weight5 = torch.randn(16, device=DEVICE)\n",
      "    bias5 = torch.randn(16, device=DEVICE)\n",
      "    out5 = layer_norm_fwd_fused_(X5, weight5, bias5, eps=1e-5, return_stats=False)\n",
      "    ref_out5 = F.layer_norm(X5, (16,), weight5, bias5, eps=1e-5)\n",
      "    results['consistency_with_f_layer_norm'] = {\n",
      "        'input': X5,\n",
      "        'weight': weight5,\n",
      "        'bias': bias5,\n",
      "        'our_output': out5,\n",
      "        'torch_layer_norm_output': ref_out5,\n",
      "        'allclose': torch.allclose(out5, ref_out5, atol=1e-5)\n",
      "    }\n",
      "\n",
      "    return results\n",
      "\n",
      "########################\n",
      "# Test Execution: Only print the test_results dictionary\n",
      "########################\n",
      "\n",
      "test_results = test_layer_norm_fwd_fused_()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "torch_single_query_cached_kv_attention_v2\n",
      "====================================================================================================\n",
      "import math\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def torch_single_query_cached_kv_attention_v2(exp_sums, max_logits, out, q,\n",
      "                                               k_cache, v_cache, head_mapping, scale,\n",
      "                                               block_tables, seq_lens, partiton_size,\n",
      "                                               max_num_blocks_per_seq, alibi_slopes,\n",
      "                                               BLOCK_SIZE, HEAD_SIZE):\n",
      "    \"\"\"\n",
      "    PyTorch implementation replicating the Triton kernel logic.\n",
      "    For testing, we assume grid indices: seq_idx = 0, par_idx = 0, head_idx = 0.\n",
      "    \"\"\"\n",
      "    # Hardcoded grid indices\n",
      "    seq_idx = 0\n",
      "    par_idx = 0\n",
      "    head_idx = 0\n",
      "\n",
      "    seq_len = int(seq_lens[seq_idx].item())\n",
      "    if par_idx * partiton_size >= seq_len:\n",
      "        return  # Early exit if partition out of range\n",
      "\n",
      "    num_context_blocks = math.ceil(seq_len / BLOCK_SIZE)\n",
      "    num_blocks_per_par = partiton_size // BLOCK_SIZE\n",
      "    start_block_idx = par_idx * num_blocks_per_par\n",
      "    end_block_idx = min(start_block_idx + num_blocks_per_par, num_context_blocks)\n",
      "\n",
      "    kv_head_idx = int(head_mapping[head_idx].item())\n",
      "    # Use provided alibi slope if available\n",
      "    alibi_slope = alibi_slopes[head_idx].item() if alibi_slopes is not None else 0.0\n",
      "\n",
      "    # Create offset tensors\n",
      "    block_offs = torch.arange(0, BLOCK_SIZE, device=DEVICE)\n",
      "    head_size_offs = torch.arange(0, HEAD_SIZE, device=DEVICE)\n",
      "\n",
      "    # Load the query vector and apply scaling\n",
      "    q_vec = q[seq_idx, :].clone()\n",
      "    q_vec = (q_vec * scale).to(torch.float16)\n",
      "\n",
      "    # Initialize accumulators\n",
      "    qkv = torch.zeros((BLOCK_SIZE, HEAD_SIZE), device=DEVICE, dtype=torch.float32)\n",
      "    qk_max = float('-inf')\n",
      "    exp_sum = 0.0\n",
      "\n",
      "    # Loop over the context blocks\n",
      "    for block_idx in range(start_block_idx, end_block_idx):\n",
      "        physical_block_idx = int(block_tables[seq_idx * max_num_blocks_per_seq + block_idx].item())\n",
      "        valid_length = seq_len - block_idx * BLOCK_SIZE\n",
      "        current_mask = block_offs < valid_length\n",
      "\n",
      "        # Load key and value vectors and cast to float32\n",
      "        k = k_cache[physical_block_idx].to(torch.float32).clone()\n",
      "        v = v_cache[physical_block_idx].to(torch.float32).clone()\n",
      "        k = torch.where(current_mask.unsqueeze(1), k, torch.zeros_like(k))\n",
      "        v = torch.where(current_mask.unsqueeze(1), v, torch.zeros_like(v))\n",
      "\n",
      "        # Compute dot product between query vector and key\n",
      "        _qk = (q_vec.unsqueeze(0).to(torch.float32) * k).sum(dim=1)\n",
      "        _qk = _qk + alibi_slope * (block_idx * BLOCK_SIZE + block_offs.to(torch.float32) - seq_len + 1)\n",
      "        current_qk_max = _qk.max().item()\n",
      "        _qk_max = max(qk_max, current_qk_max)\n",
      "        # Apply mask: invalid positions become -inf\n",
      "        qk = torch.where(current_mask, _qk, torch.full_like(_qk, float('-inf')))\n",
      "\n",
      "        # Normalize using exponentials\n",
      "        _exp_sum = exp_sum * math.exp(qk_max - _qk_max) + torch.exp(_qk - _qk_max).sum().item()\n",
      "        weight = (torch.exp(qk - _qk_max) / _exp_sum).unsqueeze(1)\n",
      "        qkv = qkv * (exp_sum * math.exp(qk_max - _qk_max) / _exp_sum) + weight * v\n",
      "        qk_max = _qk_max\n",
      "        exp_sum = _exp_sum\n",
      "\n",
      "    # Store final results\n",
      "    exp_sums[0, 0] = exp_sum\n",
      "    max_logits[0, 0] = qk_max\n",
      "    out[0, 0, :] = qkv.sum(dim=0)\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_torch_single_query_cached_kv_attention_v2():\n",
      "    results = {}\n",
      "    # Parameters\n",
      "    BLOCK_SIZE = 16\n",
      "    HEAD_SIZE = 64\n",
      "    seq_len_val = 32  # Example sequence length\n",
      "    num_context_blocks = math.ceil(seq_len_val / BLOCK_SIZE)\n",
      "\n",
      "    # Create input tensors on CUDA using global DEVICE\n",
      "    q = torch.randn((seq_len_val, HEAD_SIZE), device=DEVICE, dtype=torch.float16)\n",
      "    max_num_blocks_per_seq = num_context_blocks\n",
      "    k_cache = torch.randn((max_num_blocks_per_seq, BLOCK_SIZE, HEAD_SIZE), device=DEVICE, dtype=torch.float16)\n",
      "    v_cache = torch.randn((max_num_blocks_per_seq, BLOCK_SIZE, HEAD_SIZE), device=DEVICE, dtype=torch.float16)\n",
      "    exp_sums = torch.empty((1, 1), device=DEVICE, dtype=torch.float32)\n",
      "    max_logits = torch.empty((1, 1), device=DEVICE, dtype=torch.float32)\n",
      "    out = torch.empty((1, 1, HEAD_SIZE), device=DEVICE, dtype=torch.float32)\n",
      "\n",
      "    head_mapping = torch.tensor([0], device=DEVICE, dtype=torch.int32)\n",
      "    block_tables = torch.arange(num_context_blocks, device=DEVICE, dtype=torch.int32).repeat(1)\n",
      "    seq_lens = torch.tensor([seq_len_val], device=DEVICE, dtype=torch.int32)\n",
      "    alibi_slopes = torch.zeros((1,), device=DEVICE, dtype=torch.float32)\n",
      "\n",
      "    scale = 1.0\n",
      "    partiton_size = BLOCK_SIZE  # one block per partition\n",
      "\n",
      "    # Execute the PyTorch implementation\n",
      "    torch_single_query_cached_kv_attention_v2(exp_sums, max_logits, out, q,\n",
      "                                               k_cache, v_cache, head_mapping, scale,\n",
      "                                               block_tables, seq_lens, partiton_size,\n",
      "                                               max_num_blocks_per_seq, alibi_slopes,\n",
      "                                               BLOCK_SIZE, HEAD_SIZE)\n",
      "    results[\"test_case_1\"] = {\n",
      "        \"exp_sums\": exp_sums.clone().cpu(),\n",
      "        \"max_logits\": max_logits.clone().cpu(),\n",
      "        \"out\": out.clone().cpu()\n",
      "    }\n",
      "    return results\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "test_results = test_torch_single_query_cached_kv_attention_v2()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_bwd_intra\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_bwd_intra(Q, K, V, S, DO, b, h, BLOCK):\n",
      "    \"\"\"\n",
      "    Pure PyTorch equivalent of the Triton kernel function.\n",
      "    Computes gradients DQ, DK, DV from inputs Q, K, V, DO with block processing along the sequence dimension.\n",
      "    \"\"\"\n",
      "    bwd_DQ = torch.zeros_like(Q, device=Q.device)\n",
      "    bwd_DK = torch.zeros_like(K, device=K.device)\n",
      "    bwd_DV = torch.zeros_like(V, device=V.device)\n",
      "\n",
      "    n = Q.size(2)\n",
      "    d = Q.size(3)\n",
      "    e = V.size(3)\n",
      "    device = Q.device\n",
      "\n",
      "    # Precompute index matrix for blocks (BLOCK x BLOCK)\n",
      "    idx = torch.arange(BLOCK, dtype=torch.float32, device=device)\n",
      "    idx_mat = idx.view(BLOCK, 1) - idx.view(1, BLOCK)\n",
      "    num_blocks = (n + BLOCK - 1) // BLOCK\n",
      "\n",
      "    for i in range(b):\n",
      "        for j in range(h):\n",
      "            s = S[j]  # scalar controlling decay for this head\n",
      "            Q_head = Q[i, j]\n",
      "            K_head = K[i, j]\n",
      "            V_head = V[i, j]\n",
      "            DO_head = DO[i, j]\n",
      "            \n",
      "            for block in range(num_blocks):\n",
      "                start = block * BLOCK\n",
      "                end = min(n, start + BLOCK)\n",
      "                L = end - start\n",
      "\n",
      "                # Initialize blocks with zeros\n",
      "                Q_block = torch.zeros((BLOCK, d), device=device, dtype=Q.dtype)\n",
      "                K_block = torch.zeros((BLOCK, d), device=device, dtype=K.dtype)\n",
      "                DO_block = torch.zeros((BLOCK, e), device=device, dtype=DO.dtype)\n",
      "                V_block = torch.zeros((BLOCK, e), device=device, dtype=V.dtype)\n",
      "\n",
      "                # Copy valid entries\n",
      "                Q_block[:L] = Q_head[start:end]\n",
      "                K_block[:L] = K_head[start:end]\n",
      "                DO_block[:L] = DO_head[start:end]\n",
      "                V_block[:L] = V_head[start:end]\n",
      "                \n",
      "                Q_block_t = Q_block.transpose(0, 1)\n",
      "                V_block_t = V_block.transpose(0, 1)\n",
      "\n",
      "                temp = torch.where(idx_mat >= 0,\n",
      "                                   -s * idx_mat,\n",
      "                                   torch.tensor(float('-inf'), device=device))\n",
      "                diag_decay = torch.exp(temp)\n",
      "                diag_decay_trans = diag_decay.transpose(0, 1)\n",
      "                \n",
      "                dqk = torch.matmul(DO_block, V_block_t) * diag_decay\n",
      "                dq_intra = torch.matmul(dqk, K_block)\n",
      "\n",
      "                dk_intra_trans = torch.matmul(Q_block_t, dqk)\n",
      "                dk_intra = dk_intra_trans.transpose(0, 1)\n",
      "\n",
      "                qk_trans = torch.matmul(K_block, Q_block_t) * diag_decay_trans\n",
      "                dv_intra = torch.matmul(qk_trans, DO_block)\n",
      "\n",
      "                bwd_DQ[i, j, start:end] = dq_intra[:L]\n",
      "                bwd_DK[i, j, start:end] = dk_intra[:L]\n",
      "                bwd_DV[i, j, start:end] = dv_intra[:L]\n",
      "\n",
      "    return bwd_DQ, bwd_DK, bwd_DV\n",
      "\n",
      "########################\n",
      "\n",
      "# --------------------- Tests for PyTorch Implementation ---------------------\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_bwd_intra():\n",
      "    test_results = {}\n",
      "    torch.manual_seed(0)\n",
      "    device = DEVICE\n",
      "\n",
      "    # Define test cases\n",
      "    test_cases = {}\n",
      "\n",
      "    # Test Case 1: Single batch, single head, n exactly divisible by BLOCK\n",
      "    b, h, n, d, e = 1, 1, 8, 4, 5\n",
      "    BLOCK = 4\n",
      "    Q1 = torch.randn(b, h, n, d, device=device)\n",
      "    K1 = torch.randn(b, h, n, d, device=device)\n",
      "    V1 = torch.randn(b, h, n, e, device=device)\n",
      "    DO1 = torch.randn(b, h, n, e, device=device)\n",
      "    S1 = torch.tensor([0.5], device=device)\n",
      "    test_cases['test_case_1'] = (b, h, n, d, e, BLOCK, Q1, K1, V1, DO1, S1)\n",
      "\n",
      "    # Test Case 2: Single batch, single head, n not divisible by BLOCK\n",
      "    b, h, n, d, e = 1, 1, 10, 3, 4\n",
      "    BLOCK = 4\n",
      "    Q2 = torch.randn(b, h, n, d, device=device)\n",
      "    K2 = torch.randn(b, h, n, d, device=device)\n",
      "    V2 = torch.randn(b, h, n, e, device=device)\n",
      "    DO2 = torch.randn(b, h, n, e, device=device)\n",
      "    S2 = torch.tensor([0.8], device=device)\n",
      "    test_cases['test_case_2'] = (b, h, n, d, e, BLOCK, Q2, K2, V2, DO2, S2)\n",
      "\n",
      "    # Test Case 3: Multiple batches and heads\n",
      "    b, h, n, d, e = 2, 3, 12, 6, 7\n",
      "    BLOCK = 5\n",
      "    Q3 = torch.randn(b, h, n, d, device=device)\n",
      "    K3 = torch.randn(b, h, n, d, device=device)\n",
      "    V3 = torch.randn(b, h, n, e, device=device)\n",
      "    DO3 = torch.randn(b, h, n, e, device=device)\n",
      "    S3 = torch.linspace(0.3, 1.0, steps=h, device=device)\n",
      "    test_cases['test_case_3'] = (b, h, n, d, e, BLOCK, Q3, K3, V3, DO3, S3)\n",
      "\n",
      "    # Test Case 4: Edge case with n < BLOCK\n",
      "    b, h, n, d, e = 1, 2, 3, 4, 5\n",
      "    BLOCK = 8\n",
      "    Q4 = torch.randn(b, h, n, d, device=device)\n",
      "    K4 = torch.randn(b, h, n, d, device=device)\n",
      "    V4 = torch.randn(b, h, n, e, device=device)\n",
      "    DO4 = torch.randn(b, h, n, e, device=device)\n",
      "    S4 = torch.tensor([0.7, 0.9], device=device)\n",
      "    test_cases['test_case_4'] = (b, h, n, d, e, BLOCK, Q4, K4, V4, DO4, S4)\n",
      "\n",
      "    for key, (b, h, n, d, e, BLOCK, Q, K, V, DO, S) in test_cases.items():\n",
      "        DQ_pt, DK_pt, DV_pt = pytorch_bwd_intra(Q, K, V, S, DO, b, h, BLOCK)\n",
      "        test_results[key] = {\n",
      "            'PyTorch': {\n",
      "                'DQ': DQ_pt.cpu(),\n",
      "                'DK': DK_pt.cpu(),\n",
      "                'DV': DV_pt.cpu()\n",
      "            }\n",
      "        }\n",
      "\n",
      "    return test_results\n",
      "\n",
      "\n",
      "test_results = test_pytorch_bwd_intra()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "elementwise_mul_\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def elementwise_mul_(x: torch.Tensor, g) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs in-place element-wise multiplication of tensor x by scalar g using PyTorch.\n",
      "    All tensors are created on DEVICE.\n",
      "\n",
      "    Parameters:\n",
      "      x (torch.Tensor): input tensor (modified in-place).\n",
      "      g (float or torch.Tensor): scalar value (or 1-element tensor).\n",
      "\n",
      "    Returns:\n",
      "      torch.Tensor: the modified tensor x.\n",
      "    \"\"\"\n",
      "    # If g is a tensor, extract the scalar value.\n",
      "    if isinstance(g, torch.Tensor):\n",
      "        if g.numel() != 1:\n",
      "            raise ValueError(\"g must be a scalar or a 1-element tensor\")\n",
      "        g = g.item()\n",
      "\n",
      "    # In-place multiplication using PyTorch.\n",
      "    x.mul_(g)\n",
      "    return x\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_elementwise_mul_():\n",
      "    \"\"\"\n",
      "    Test cases for elementwise_mul_ using PyTorch in-place multiplication.\n",
      "    All tensors are created on DEVICE.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "\n",
      "    # Test case 1: 1D tensor multiplication with positive scalar.\n",
      "    x1 = torch.tensor([1.0, 2.0, 3.0], device=DEVICE)\n",
      "    g1 = 2.0\n",
      "    expected1 = torch.tensor([2.0, 4.0, 6.0], device=DEVICE)\n",
      "    x1_copy = x1.clone()\n",
      "    elementwise_mul_(x1_copy, g1)\n",
      "    results[\"test_case_1\"] = {\n",
      "        \"input\": x1.cpu().tolist(),\n",
      "        \"scalar\": g1,\n",
      "        \"output\": x1_copy.cpu().tolist(),\n",
      "        \"expected\": expected1.cpu().tolist()\n",
      "    }\n",
      "\n",
      "    # Test case 2: 1D tensor multiplication with negative scalar.\n",
      "    x2 = torch.tensor([4.0, -2.0, 0.0], device=DEVICE)\n",
      "    g2 = -3.0\n",
      "    expected2 = torch.tensor([-12.0, 6.0, -0.0], device=DEVICE)\n",
      "    x2_copy = x2.clone()\n",
      "    elementwise_mul_(x2_copy, g2)\n",
      "    results[\"test_case_2\"] = {\n",
      "        \"input\": x2.cpu().tolist(),\n",
      "        \"scalar\": g2,\n",
      "        \"output\": x2_copy.cpu().tolist(),\n",
      "        \"expected\": expected2.cpu().tolist()\n",
      "    }\n",
      "\n",
      "    # Test case 3: 2D tensor multiplication.\n",
      "    x3 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\n",
      "    g3 = 0.5\n",
      "    expected3 = torch.tensor([[0.5, 1.0], [1.5, 2.0]], device=DEVICE)\n",
      "    x3_copy = x3.clone()\n",
      "    elementwise_mul_(x3_copy, g3)\n",
      "    results[\"test_case_3\"] = {\n",
      "        \"input\": x3.cpu().tolist(),\n",
      "        \"scalar\": g3,\n",
      "        \"output\": x3_copy.cpu().tolist(),\n",
      "        \"expected\": expected3.cpu().tolist()\n",
      "    }\n",
      "\n",
      "    # Test case 4: Scalar input as a 1-element tensor.\n",
      "    x4 = torch.tensor([5.0, 10.0, 15.0], device=DEVICE)\n",
      "    g4 = torch.tensor([2.0], device=DEVICE)\n",
      "    expected4 = torch.tensor([10.0, 20.0, 30.0], device=DEVICE)\n",
      "    x4_copy = x4.clone()\n",
      "    elementwise_mul_(x4_copy, g4)\n",
      "    results[\"test_case_4\"] = {\n",
      "        \"input\": x4.cpu().tolist(),\n",
      "        \"scalar\": g4.item(),\n",
      "        \"output\": x4_copy.cpu().tolist(),\n",
      "        \"expected\": expected4.cpu().tolist()\n",
      "    }\n",
      "\n",
      "    # Test case 5: Empty tensor.\n",
      "    x5 = torch.tensor([], device=DEVICE)\n",
      "    g5 = 3.0\n",
      "    expected5 = torch.tensor([], device=DEVICE)\n",
      "    x5_copy = x5.clone()\n",
      "    elementwise_mul_(x5_copy, g5)\n",
      "    results[\"test_case_5\"] = {\n",
      "        \"input\": x5.cpu().tolist(),\n",
      "        \"scalar\": g5,\n",
      "        \"output\": x5_copy.cpu().tolist(),\n",
      "        \"expected\": expected5.cpu().tolist()\n",
      "    }\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "# Execute tests and print only the test_results dictionary\n",
      "\n",
      "test_results = test_elementwise_mul_()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_rms_norm_fwd\n",
      "====================================================================================================\n",
      "import torch\n",
      "import math\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_rms_norm_fwd(X: torch.Tensor, W: torch.Tensor, eps: float = 1e-6) -> (torch.Tensor, torch.Tensor):\n",
      "    \"\"\"\n",
      "    Pure PyTorch implementation of the RMSNorm forward pass.\n",
      "    For each row in X (shape [M, N]), computes:\n",
      "      - mean = 1/N * sum(x^2)\n",
      "      - rstd = 1/sqrt(mean + eps)\n",
      "      - Y = X * rstd * W\n",
      "    where W is a weight tensor of shape (N,).\n",
      "    Returns:\n",
      "      Y: Normalized tensor [M, N]\n",
      "      rstd: Reciprocal standard deviations for each row [M]\n",
      "    \"\"\"\n",
      "    if X.dim() != 2:\n",
      "        raise ValueError(\"Input X must be a 2D tensor\")\n",
      "    M, N = X.shape\n",
      "    if W.numel() != N:\n",
      "        raise ValueError(\"Weight tensor W must have the same number of elements as the number of columns of X\")\n",
      "    \n",
      "    # Compute mean of squares row-wise\n",
      "    mean = torch.sum(X * X, dim=1, keepdim=True) / N\n",
      "    # Compute reciprocal standard deviation with eps for numerical stability\n",
      "    rstd = torch.rsqrt(mean + eps)  # shape (M, 1)\n",
      "    # Compute normalized result\n",
      "    Y = X * rstd * W\n",
      "    return Y, rstd.squeeze(1)\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_rms_norm_fwd():\n",
      "    \"\"\"\n",
      "    Test the PyTorch implementation of RMSNorm forward pass.\n",
      "    All test tensors are created on DEVICE.\n",
      "    \"\"\"\n",
      "    test_results = {}\n",
      "    eps = 1e-6\n",
      "\n",
      "    # Test Case 1: Small 2x3 input with weights set to ones\n",
      "    X1 = torch.tensor([[1.0, 2.0, 3.0],\n",
      "                       [4.0, 5.0, 6.0]], dtype=torch.float32, device=DEVICE)\n",
      "    W1 = torch.ones(3, dtype=torch.float32, device=DEVICE)\n",
      "    try:\n",
      "        Y1, rstd1 = pytorch_rms_norm_fwd(X1, W1, eps)\n",
      "        test_results[\"test_case_1\"] = {\"input\": X1, \"weights\": W1, \"Y\": Y1, \"rstd\": rstd1}\n",
      "    except Exception as e:\n",
      "        test_results[\"test_case_1\"] = str(e)\n",
      "\n",
      "    # Test Case 2: Small 2x4 input with custom weights\n",
      "    X2 = torch.tensor([[0.5, -1.0, 2.0, -2.5],\n",
      "                       [3.0, -3.5, 4.0, -4.5]], dtype=torch.float32, device=DEVICE)\n",
      "    W2 = torch.tensor([1.0, 0.5, 2.0, 1.5], dtype=torch.float32, device=DEVICE)\n",
      "    try:\n",
      "        Y2, rstd2 = pytorch_rms_norm_fwd(X2, W2, eps)\n",
      "        test_results[\"test_case_2\"] = {\"input\": X2, \"weights\": W2, \"Y\": Y2, \"rstd\": rstd2}\n",
      "    except Exception as e:\n",
      "        test_results[\"test_case_2\"] = str(e)\n",
      "\n",
      "    # Test Case 3: Random tensor 5x10, random weights\n",
      "    torch.manual_seed(42)  # For reproducibility\n",
      "    X3 = torch.randn(5, 10, dtype=torch.float32, device=DEVICE)\n",
      "    W3 = torch.randn(10, dtype=torch.float32, device=DEVICE)\n",
      "    try:\n",
      "        Y3, rstd3 = pytorch_rms_norm_fwd(X3, W3, eps)\n",
      "        test_results[\"test_case_3\"] = {\"input\": X3, \"weights\": W3, \"Y\": Y3, \"rstd\": rstd3}\n",
      "    except Exception as e:\n",
      "        test_results[\"test_case_3\"] = str(e)\n",
      "\n",
      "    # Test Case 4: Edge case with one row and one column\n",
      "    X4 = torch.tensor([[7.0]], dtype=torch.float32, device=DEVICE)\n",
      "    W4 = torch.tensor([2.0], dtype=torch.float32, device=DEVICE)\n",
      "    try:\n",
      "        Y4, rstd4 = pytorch_rms_norm_fwd(X4, W4, eps)\n",
      "        test_results[\"test_case_4\"] = {\"input\": X4, \"weights\": W4, \"Y\": Y4, \"rstd\": rstd4}\n",
      "    except Exception as e:\n",
      "        test_results[\"test_case_4\"] = str(e)\n",
      "\n",
      "    # Test Case 5: Verify error on mismatching weight dimensions\n",
      "    X5 = torch.randn(3, 4, dtype=torch.float32, device=DEVICE)\n",
      "    W5 = torch.randn(5, dtype=torch.float32, device=DEVICE)  # Incorrect size\n",
      "    try:\n",
      "        _ = pytorch_rms_norm_fwd(X5, W5, eps)\n",
      "        test_results[\"test_case_5\"] = \"No error thrown.\"\n",
      "    except Exception as e:\n",
      "        test_results[\"test_case_5\"] = str(e)\n",
      "\n",
      "    return test_results\n",
      "\n",
      "\n",
      "test_results = test_pytorch_rms_norm_fwd()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "apply_rotary_embedding\n",
      "====================================================================================================\n",
      "import math\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def apply_rotary_embedding(q: torch.Tensor, \n",
      "                                   k: torch.Tensor, \n",
      "                                   cos: torch.Tensor, \n",
      "                                   sin: torch.Tensor, \n",
      "                                   seq_offset: int = 0) -> (torch.Tensor, torch.Tensor):\n",
      "    \"\"\"\n",
      "    Applies rotary embeddings to query and key tensors using pure PyTorch operations.\n",
      "    For the last dimension (split into two halves):\n",
      "      out_left = x_left * cos_left - x_right * sin_left\n",
      "      out_right = x_right * cos_right + x_left * sin_right\n",
      "    \"\"\"\n",
      "    batch_size, num_heads, seq_len, hidden_size = q.shape\n",
      "    if hidden_size % 2 != 0:\n",
      "        raise ValueError('hidden_size must be even')\n",
      "    half = hidden_size // 2\n",
      "    \n",
      "    # Get effective cosine and sine embeddings based on seq_offset\n",
      "    effective_cos = cos[seq_offset:seq_offset + seq_len].unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, hidden_size)\n",
      "    effective_sin = sin[seq_offset:seq_offset + seq_len].unsqueeze(0).unsqueeze(0)\n",
      "    \n",
      "    # Split queries and keys into left and right halves\n",
      "    q_left, q_right = q[..., :half], q[..., half:]\n",
      "    k_left, k_right = k[..., :half], k[..., half:]\n",
      "    \n",
      "    # Split cosine and sine embeddings into left and right parts\n",
      "    cos_left, cos_right = effective_cos[..., :half], effective_cos[..., half:]\n",
      "    sin_left, sin_right = effective_sin[..., :half], effective_sin[..., half:]\n",
      "    \n",
      "    # Apply the rotary transformation\n",
      "    q_rot_left = q_left * cos_left - q_right * sin_left\n",
      "    q_rot_right = q_right * cos_right + q_left * sin_right\n",
      "    k_rot_left = k_left * cos_left - k_right * sin_left\n",
      "    k_rot_right = k_right * cos_right + k_left * sin_right\n",
      "    \n",
      "    q_rot = torch.cat([q_rot_left, q_rot_right], dim=-1)\n",
      "    k_rot = torch.cat([k_rot_left, k_rot_right], dim=-1)\n",
      "    return q_rot, k_rot\n",
      "\n",
      "########################\n",
      "# Testing Section\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_apply_rotary_embedding():\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "    \n",
      "    # Example parameters\n",
      "    batch_size = 2\n",
      "    num_heads = 4\n",
      "    seq_len = 6\n",
      "    hidden_size = 8  # must be even\n",
      "    total_seq_len = 10\n",
      "    \n",
      "    # Create random query and key tensors on DEVICE\n",
      "    q = torch.randn(batch_size, num_heads, seq_len, hidden_size, device=DEVICE)\n",
      "    k = torch.randn(batch_size, num_heads, seq_len, hidden_size, device=DEVICE)\n",
      "    \n",
      "    # Create cosine and sine tensors (deterministic values)\n",
      "    positions = torch.arange(total_seq_len, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
      "    dims = torch.arange(hidden_size, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
      "    cos = torch.cos(positions / total_seq_len * math.pi * dims / hidden_size)\n",
      "    sin = torch.sin(positions / total_seq_len * math.pi * dims / hidden_size)\n",
      "    \n",
      "    # Test case 1: Using seq_offset = 0\n",
      "    q_rot, k_rot = apply_rotary_embedding(q, k, cos, sin, seq_offset=0)\n",
      "    results['case_1'] = { 'q_rot': q_rot, 'k_rot': k_rot }\n",
      "    \n",
      "    # Test case 2: Using seq_offset = 2\n",
      "    seq_offset = 2\n",
      "    if seq_offset + seq_len <= total_seq_len:\n",
      "        q_rot_offset, k_rot_offset = apply_rotary_embedding(q, k, cos, sin, seq_offset=seq_offset)\n",
      "        results['case_2'] = { 'seq_offset': seq_offset, 'q_rot': q_rot_offset, 'k_rot': k_rot_offset }\n",
      "    else:\n",
      "        results['case_2'] = 'Invalid seq_offset for given total_seq_len'\n",
      "    \n",
      "    # Test case 3: Single element consistency check for queries\n",
      "    q_single = q[0, 0, 0]  # (hidden_size,)\n",
      "    half = hidden_size // 2\n",
      "    q_left, q_right = q_single[:half], q_single[half:]\n",
      "    cos_single = cos[0, :]\n",
      "    sin_single = sin[0, :]\n",
      "    cos_left, cos_right = cos_single[:half], cos_single[half:]\n",
      "    sin_left, sin_right = sin_single[:half], sin_single[half:]\n",
      "    q_manual = torch.cat([q_left * cos_left - q_right * sin_left, \n",
      "                          q_right * cos_right + q_left * sin_right])\n",
      "    results['case_3'] = {\n",
      "        'q_manual': q_manual,\n",
      "        'q_rot[0,0,0]': q_rot[0,0,0],\n",
      "        'difference': torch.abs(q_manual - q_rot[0,0,0])\n",
      "    }\n",
      "    return results\n",
      "\n",
      "\n",
      "test_results = test_apply_rotary_embedding()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "fused_chunk_delta_rule_bwd_\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def fused_chunk_delta_rule_bwd_(q, k, v, d, dht, dh0, do, initial_state, scale, BT,\n",
      "                                     USE_INITIAL_STATE=False, USE_DHT=False, USE_DHO=False, CHECK=False):\n",
      "    \"\"\"\n",
      "    PyTorch implementation mimicking the behavior of the fused_chunk_delta_rule_bwd_kernel.\n",
      "\n",
      "    Expected tensor shapes (fused layout over batch and heads, BH = B*H):\n",
      "      q, k, d, dq, dk:  [BH, T, Dk]   (we assume Dk == BK)\n",
      "      v, do, dv:        [BH, T, DV]   (we assume DV == BV)\n",
      "      dht:              [BH, Dk, DV]  (if USE_DHT is True)\n",
      "      initial_state:    [BH, DV, Dk]  (if USE_INITIAL_STATE is True)\n",
      "      dh0:              [BH, Dk, DV]  (output if USE_DHO is True)\n",
      "\n",
      "    Returns:\n",
      "      dq, dk, dv, dd\n",
      "    \"\"\"\n",
      "    BH, T, Dk = q.shape\n",
      "    DV = v.shape[2]\n",
      "    NT = (T + BT - 1) // BT\n",
      "\n",
      "    # Allocate outputs on DEVICE\n",
      "    dq = torch.zeros_like(q, device=DEVICE)\n",
      "    dk = torch.zeros_like(k, device=DEVICE)\n",
      "    dv = torch.zeros_like(v, device=DEVICE)\n",
      "    dd = torch.zeros_like(q, device=DEVICE)\n",
      "\n",
      "    for b in range(BH):\n",
      "        if USE_DHT:\n",
      "            b_dh = dht[b].clone()\n",
      "        else:\n",
      "            b_dh = torch.zeros((Dk, DV), device=DEVICE, dtype=q.dtype)\n",
      "\n",
      "        # Backward pass for dk and dv\n",
      "        for i in reversed(range(NT)):\n",
      "            start = i * BT\n",
      "            end = min((i + 1) * BT, T)\n",
      "            L = end - start\n",
      "            o = torch.arange(L, device=DEVICE)\n",
      "            m_s = (o.unsqueeze(1) <= o.unsqueeze(0)).to(q.dtype)\n",
      "\n",
      "            b_q_block = q[b, start:end, :]\n",
      "            b_k_block = k[b, start:end, :]\n",
      "            b_v_block = v[b, start:end, :]\n",
      "            b_do_block = do[b, start:end, :]\n",
      "\n",
      "            # Scale the query block\n",
      "            b_q = (b_q_block.t() * scale).to(b_q_block.dtype)\n",
      "            b_k = b_k_block\n",
      "            b_v = b_v_block\n",
      "            b_do = b_do_block\n",
      "\n",
      "            b_ds = b_v @ b_do.t()\n",
      "            b_ds = b_ds * m_s\n",
      "            b_s = b_k @ b_q\n",
      "            b_s = b_s * m_s\n",
      "\n",
      "            b_dk_block = b_ds @ b_q.t()\n",
      "            b_dv_block = b_s @ b_do\n",
      "            b_d = d[b, start:end, :]\n",
      "\n",
      "            # Incorporate contributions from b_dh\n",
      "            b_dk_block += b_v @ b_dh.t()\n",
      "            b_dv_block += b_k @ b_dh\n",
      "\n",
      "            # Update running gradient b_dh\n",
      "            b_dh += (b_q @ b_do) - (b_d.t() @ b_dv_block)\n",
      "\n",
      "            dk[b, start:end, :] = b_dk_block\n",
      "            dv[b, start:end, :] = b_dv_block\n",
      "\n",
      "        if USE_DHO:\n",
      "            dh0[b] = b_dh\n",
      "\n",
      "        # Forward pass for dq and dd\n",
      "        if USE_INITIAL_STATE:\n",
      "            b_h = initial_state[b].clone()\n",
      "        else:\n",
      "            b_h = torch.zeros((DV, Dk), device=DEVICE, dtype=q.dtype)\n",
      "\n",
      "        for i in range(NT):\n",
      "            start = i * BT\n",
      "            end = min((i + 1) * BT, T)\n",
      "            L = end - start\n",
      "            b_dv_block = dv[b, start:end, :]\n",
      "            b_dd = b_dv_block @ b_h\n",
      "            dd[b, start:end, :] = -b_dd\n",
      "\n",
      "            b_k_block = k[b, start:end, :]\n",
      "            b_v_block = v[b, start:end, :]\n",
      "            b_do_block = do[b, start:end, :]\n",
      "\n",
      "            o = torch.arange(L, device=DEVICE)\n",
      "            m_s2 = (o.unsqueeze(1) >= o.unsqueeze(0)).to(q.dtype)\n",
      "\n",
      "            b_ds = b_do_block @ b_v_block.t()\n",
      "            b_ds = b_ds * m_s2\n",
      "            b_dq_block = b_ds @ b_k_block\n",
      "            b_dq_block += b_do_block @ b_h\n",
      "            b_h = b_h + b_v_block.t() @ b_k_block\n",
      "            b_dq_block = b_dq_block * scale\n",
      "            dq[b, start:end, :] = b_dq_block\n",
      "\n",
      "    return dq, dk, dv, dd\n",
      "\n",
      "\n",
      "########################\n",
      "# Integrated Tests for PyTorch Implementation\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_fused_chunk_delta_rule_bwd_():\n",
      "    test_results = {}\n",
      "    torch.manual_seed(42)\n",
      "\n",
      "    # Define dimensions\n",
      "    BH = 2         # Batch*Heads\n",
      "    T = 8          # sequence length\n",
      "    Dk = 4         # key dimension (and also BK)\n",
      "    DV = 3         # value dimension (and also BV)\n",
      "    BT = 4         # block size\n",
      "    scale = 0.5\n",
      "\n",
      "    # Create random input tensors on DEVICE\n",
      "    q = torch.randn(BH, T, Dk, device=DEVICE)\n",
      "    k = torch.randn(BH, T, Dk, device=DEVICE)\n",
      "    v = torch.randn(BH, T, DV, device=DEVICE)\n",
      "    d = torch.randn(BH, T, Dk, device=DEVICE)\n",
      "    do = torch.randn(BH, T, DV, device=DEVICE)\n",
      "\n",
      "    # Optional tensors\n",
      "    dht = torch.randn(BH, Dk, DV, device=DEVICE)\n",
      "    dh0 = torch.zeros(BH, Dk, DV, device=DEVICE)\n",
      "    initial_state = torch.randn(BH, DV, Dk, device=DEVICE)\n",
      "\n",
      "    # Test case 1: Without using dht, initial_state, or dh0\n",
      "    dq_t, dk_t, dv_t, dd_t = fused_chunk_delta_rule_bwd_(\n",
      "        q, k, v, d, dht, dh0, do, initial_state, scale, BT,\n",
      "        USE_INITIAL_STATE=False, USE_DHT=False, USE_DHO=False, CHECK=False\n",
      "    )\n",
      "    test_results[\"case1\"] = { 'dq': dq_t, 'dk': dk_t, 'dv': dv_t, 'dd': dd_t }\n",
      "\n",
      "    # Test case 2: Using dht and dh0\n",
      "    dh0.fill_(0)  # reset dh0\n",
      "    dq_t, dk_t, dv_t, dd_t = fused_chunk_delta_rule_bwd_(\n",
      "        q, k, v, d, dht, dh0, do, initial_state, scale, BT,\n",
      "        USE_INITIAL_STATE=False, USE_DHT=True, USE_DHO=True, CHECK=False\n",
      "    )\n",
      "    test_results[\"case2\"] = { 'dq': dq_t, 'dk': dk_t, 'dv': dv_t, 'dd': dd_t, 'dh0': dh0.clone() }\n",
      "\n",
      "    # Test case 3: Using initial_state and CHECK on\n",
      "    dq_t, dk_t, dv_t, dd_t = fused_chunk_delta_rule_bwd_(\n",
      "        q, k, v, d, dht, dh0, do, initial_state, scale, BT,\n",
      "        USE_INITIAL_STATE=True, USE_DHT=False, USE_DHO=False, CHECK=True\n",
      "    )\n",
      "    test_results[\"case3\"] = { 'dq': dq_t, 'dk': dk_t, 'dv': dv_t, 'dd': dd_t }\n",
      "\n",
      "    return test_results\n",
      "\n",
      "# Execute tests: Only the final test_results dictionary is printed.\n",
      "test_results = test_fused_chunk_delta_rule_bwd_()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "torch_bwd_block\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def torch_bwd_block(Q: torch.Tensor,\n",
      "                    K: torch.Tensor,\n",
      "                    V: torch.Tensor,\n",
      "                    sm_scale: float,\n",
      "                    qk_scale: float,\n",
      "                    DO: torch.Tensor,\n",
      "                    L: torch.Tensor,\n",
      "                    D: torch.Tensor,\n",
      "                    CAUSAL: bool = False,\n",
      "                    SEQUENCE_PARALLEL: bool = False,\n",
      "                    MMA_V3: bool = False) -> (torch.Tensor, torch.Tensor, torch.Tensor):\n",
      "    \"\"\"\n",
      "    Computes the backward pass for a block of attention-like matrix operations.\n",
      "\n",
      "    Args:\n",
      "      Q, K, V: Input matrices of shape (M, d) where M is BLOCK_M and d is BLOCK_DMODEL.\n",
      "      DO: Gradient of the output with shape (M, d).\n",
      "      L: Tensor of shape (M,) representing log-sum-exp values per row.\n",
      "      D: Tensor of shape (M,) representing additional derivative scaling factors.\n",
      "      sm_scale: scaling factor for the softmax derivative.\n",
      "      qk_scale: scaling factor for the QK product.\n",
      "      CAUSAL: If True, applies a causal mask to the QK product.\n",
      "      SEQUENCE_PARALLEL: Switches the computation path for dq.\n",
      "      MMA_V3: When SEQUENCE_PARALLEL is True, selects one of two strategies to compute dq.\n",
      "\n",
      "    Returns:\n",
      "      dq, dk, dv: Gradients with respect to Q, K, and V respectively. Each of shape (M, d)\n",
      "    \"\"\"\n",
      "    M, d = Q.shape\n",
      "    offs = torch.arange(M, device=Q.device, dtype=Q.dtype)\n",
      "    if CAUSAL:\n",
      "        row_idx = offs.view(M, 1)\n",
      "        col_idx = offs.view(1, M)\n",
      "        base = torch.where(row_idx >= col_idx,\n",
      "                           torch.tensor(0.0, device=Q.device, dtype=Q.dtype),\n",
      "                           torch.tensor(float('-inf'), device=Q.device, dtype=Q.dtype))\n",
      "    else:\n",
      "        base = torch.zeros((M, M), device=Q.device, dtype=Q.dtype)\n",
      "\n",
      "    qk = base + torch.matmul(Q, K.t())\n",
      "    qk = qk * qk_scale\n",
      "    l_i = L[:M]\n",
      "    p = torch.pow(2.0, qk - l_i.unsqueeze(1))\n",
      "    dv = torch.matmul(p.t(), DO)\n",
      "    dp = torch.matmul(DO, V.t())\n",
      "    ds = p * (dp - D[:M].unsqueeze(1)) * sm_scale\n",
      "    dk = torch.matmul(ds.t(), Q)\n",
      "    if not SEQUENCE_PARALLEL:\n",
      "        dq = torch.matmul(ds, K)\n",
      "    else:\n",
      "        if MMA_V3:\n",
      "            dq = torch.matmul(ds, K)\n",
      "        else:\n",
      "            dq = (torch.matmul(K.t(), ds.t())).t()\n",
      "    return dq, dk, dv\n",
      "\n",
      "\n",
      "########################\n",
      "# Integrated Tests\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_torch_bwd_block():\n",
      "    test_results = {}\n",
      "    torch.manual_seed(0)\n",
      "    device = DEVICE\n",
      "\n",
      "    # Define test cases\n",
      "    test_cases = []\n",
      "    # Test Case 1: Basic test with CAUSAL=False, SEQUENCE_PARALLEL=False\n",
      "    M, d = 4, 8\n",
      "    Q = torch.randn(M, d, device=device)\n",
      "    K = torch.randn(M, d, device=device)\n",
      "    V = torch.randn(M, d, device=device)\n",
      "    DO = torch.randn(M, d, device=device)\n",
      "    L = torch.randn(M, device=device)\n",
      "    D = torch.randn(M, device=device)\n",
      "    sm_scale = 0.5\n",
      "    qk_scale = 0.8\n",
      "    test_cases.append((Q, K, V, sm_scale, qk_scale, DO, L, D, False, False, False))\n",
      "\n",
      "    # Test Case 2: With CAUSAL=True, SEQUENCE_PARALLEL=False\n",
      "    test_cases.append((Q, K, V, sm_scale, qk_scale, DO, L, D, True, False, False))\n",
      "\n",
      "    # Test Case 3: With CAUSAL=False, SEQUENCE_PARALLEL=True, MMA_V3=True\n",
      "    test_cases.append((Q, K, V, sm_scale, qk_scale, DO, L, D, False, True, True))\n",
      "\n",
      "    # Test Case 4: With CAUSAL=True, SEQUENCE_PARALLEL=True, MMA_V3=False\n",
      "    test_cases.append((Q, K, V, sm_scale, qk_scale, DO, L, D, True, True, False))\n",
      "\n",
      "    # Test Case 5: Larger block test\n",
      "    M_large, d_large = 16, 32\n",
      "    Q_large = torch.randn(M_large, d_large, device=device)\n",
      "    K_large = torch.randn(M_large, d_large, device=device)\n",
      "    V_large = torch.randn(M_large, d_large, device=device)\n",
      "    DO_large = torch.randn(M_large, d_large, device=device)\n",
      "    L_large = torch.randn(M_large, device=device)\n",
      "    D_large = torch.randn(M_large, device=device)\n",
      "    test_cases.append((Q_large, K_large, V_large, 0.3, 1.2, DO_large, L_large, D_large, False, False, False))\n",
      "\n",
      "    for i, params in enumerate(test_cases, start=1):\n",
      "        Q_, K_, V_, s_scale, q_scale, DO_, L_, D_, CAUSAL, SEQUENCE_PARALLEL, MMA_V3 = params\n",
      "        dq, dk, dv = torch_bwd_block(Q_, K_, V_, s_scale, q_scale, DO_, L_, D_, CAUSAL, SEQUENCE_PARALLEL, MMA_V3)\n",
      "        test_results[f\"test_case_{i}\"] = {\"dq\": dq.detach().cpu(), \"dk\": dk.detach().cpu(), \"dv\": dv.detach().cpu()}\n",
      "    return test_results\n",
      "\n",
      "# Execute tests: only print the test_results dictionary\n",
      "test_results = test_torch_bwd_block()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "bwd_decay_global_cumsum_\n",
      "====================================================================================================\n",
      "import torch\n",
      "import math\n",
      "\n",
      "# Global device standard: use CUDA if available, otherwise CPU\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "# PyTorch Implementation of bwd_decay_global_cumsum\n",
      "# This function mimics the behavior of the Triton kernel using PyTorch tensor operations.\n",
      "\n",
      "def bwd_decay_global_cumsum_(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg,\n",
      "                                  s_k_h, BT, BK, K, grid_i_bh, grid_i_c, grid_i_k):\n",
      "    # Iterate over grid dimensions to mimic the Triton kernel's block-based execution\n",
      "    for i_bh in range(grid_i_bh):\n",
      "        for i_c in range(grid_i_c):\n",
      "            for i_k in range(grid_i_k):\n",
      "                # Compute the per-block base offset for pointer arithmetic\n",
      "                base_offset = i_bh * s_k_h + i_k * BK + (i_c * BT + (BT - 1)) * K\n",
      "                cum_grad_dg = torch.zeros(BK, dtype=torch.float32, device=DEVICE)\n",
      "                # Create a mask that zeros out positions where index >= K\n",
      "                mask = (i_k * BK + torch.arange(BK, device=DEVICE)) < K\n",
      "                last_g = torch.zeros(BK, dtype=torch.float32, device=DEVICE)\n",
      "                # Loop backwards over BT blocks\n",
      "                for j in range(BT - 1, -1, -1):\n",
      "                    current_offset = base_offset - (BT - 1 - j) * K\n",
      "                    _g = g[current_offset: current_offset + BK]\n",
      "                    _g = torch.where(mask, _g, torch.zeros_like(_g))\n",
      "                    if j == BT - 1:\n",
      "                        last_g = _g\n",
      "                    # Process dq: update dq_inter using dq_inner and dq_inter\n",
      "                    b_dq1 = dq_inner[current_offset: current_offset + BK]\n",
      "                    b_dq2 = dq_inter[current_offset: current_offset + BK]\n",
      "                    b_dq2 = b_dq2 * torch.exp(_g)\n",
      "                    b_dq = b_dq1 + b_dq2\n",
      "                    dq_inter[current_offset: current_offset + BK] = b_dq\n",
      "                    # Process dk: update dk_inter using dk_inner and dk_inter\n",
      "                    b_dk1 = dk_inner[current_offset: current_offset + BK]\n",
      "                    b_dk2 = dk_inter[current_offset: current_offset + BK]\n",
      "                    b_dk2 = b_dk2 * torch.exp(last_g - _g)\n",
      "                    b_dk = b_dk1 + b_dk2\n",
      "                    dk_inter[current_offset: current_offset + BK] = b_dk\n",
      "                    # Load corresponding q and k values\n",
      "                    b_q = q[current_offset: current_offset + BK]\n",
      "                    b_k = k[current_offset: current_offset + BK]\n",
      "                    # Compute gradient contribution and update cumulative gradient\n",
      "                    b_dg = b_dq * b_q - b_dk * b_k\n",
      "                    cum_grad_dg = cum_grad_dg + b_dg\n",
      "                    dg[current_offset: current_offset + BK] = cum_grad_dg\n",
      "    return\n",
      "\n",
      "########################\n",
      "# Tests for the PyTorch Implementation\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_bwd_decay_global_cumsum_():\n",
      "    \"\"\"\n",
      "    Test for the PyTorch implementation of bwd_decay_global_cumsum.\n",
      "    Kernel/grid configuration:\n",
      "      s_k_h = 100, BT = 2, BK = 4, K = 8,\n",
      "      grid dimensions: grid_i_bh = 1, grid_i_c = 1, grid_i_k = ceil(K/BK) (should be 2)\n",
      "    Tensors of size 16 are allocated.\n",
      "    \"\"\"\n",
      "    # Kernel parameters\n",
      "    s_k_h = 100\n",
      "    BT = 2\n",
      "    BK = 4\n",
      "    K_param = 8\n",
      "    grid_i_bh = 1\n",
      "    grid_i_c = 1\n",
      "    grid_i_k = math.ceil(K_param / BK)  # Expected to be 2\n",
      "    size = 16\n",
      "    \n",
      "    # Initialize tensors on DEVICE for PyTorch implementation\n",
      "    pt_dq_inner = torch.ones(size, dtype=torch.float32, device=DEVICE) * 2.0\n",
      "    pt_dq_inter = torch.ones(size, dtype=torch.float32, device=DEVICE) * 3.0\n",
      "    pt_dk_inner = torch.ones(size, dtype=torch.float32, device=DEVICE) * 4.0\n",
      "    pt_dk_inter = torch.ones(size, dtype=torch.float32, device=DEVICE) * 5.0\n",
      "    pt_q = torch.arange(size, dtype=torch.float32, device=DEVICE)\n",
      "    pt_k = torch.arange(size, dtype=torch.float32, device=DEVICE) + 0.5\n",
      "    pt_g = torch.linspace(0, 1, steps=size, dtype=torch.float32, device=DEVICE)\n",
      "    pt_dg = torch.zeros(size, dtype=torch.float32, device=DEVICE)\n",
      "    \n",
      "    # Run the PyTorch implementation\n",
      "    bwd_decay_global_cumsum_(pt_dq_inner, pt_dq_inter, pt_dk_inner, pt_dk_inter,\n",
      "                                  pt_q, pt_k, pt_g, pt_dg,\n",
      "                                  s_k_h, BT, BK, K_param,\n",
      "                                  grid_i_bh, grid_i_c, grid_i_k)\n",
      "    \n",
      "    test_results = {\n",
      "        \"pytorch\": {\n",
      "            \"dq_inter\": pt_dq_inter.cpu().tolist(),\n",
      "            \"dk_inter\": pt_dk_inter.cpu().tolist(),\n",
      "            \"dg\": pt_dg.cpu().tolist()\n",
      "        }\n",
      "    }\n",
      "    print(test_results)\n",
      "    return test_results\n",
      "\n",
      "\n",
      "# Execute test (without if __name__ == '__main__' block as per instructions)\n",
      "test_results = test_bwd_decay_global_cumsum_()\n",
      "\n",
      "****************************************************************************************************\n",
      "torch_attn_fwd_inner\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Set global device\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def torch_attn_fwd_inner(acc: torch.Tensor,\n",
      "                         l_i: torch.Tensor,\n",
      "                         m_i: torch.Tensor,\n",
      "                         q: torch.Tensor,\n",
      "                         q_scale: float,\n",
      "                         keys: torch.Tensor,\n",
      "                         key_scales: torch.Tensor,\n",
      "                         values: torch.Tensor,\n",
      "                         BLOCK_N: int) -> (torch.Tensor, torch.Tensor):\n",
      "    \"\"\"Pure PyTorch implementation of the attention forward inner loop.\"\"\"\n",
      "    N_CTX = keys.shape[0]\n",
      "    M, HEAD_DIM = q.shape\n",
      "    num_blocks = (N_CTX + BLOCK_N - 1) // BLOCK_N\n",
      "    if key_scales.shape[0] != num_blocks:\n",
      "        raise ValueError(f\"key_scales should have {num_blocks} elements, but got {key_scales.shape[0]}\")\n",
      "    \n",
      "    for block_idx, start_n in enumerate(range(0, N_CTX, BLOCK_N)):\n",
      "        end_n = min(start_n + BLOCK_N, N_CTX)\n",
      "        keys_block = keys[start_n:end_n]\n",
      "        values_block = values[start_n:end_n]\n",
      "        k_scale = key_scales[block_idx].item() if isinstance(key_scales, torch.Tensor) else key_scales[block_idx]\n",
      "        qk = torch.matmul(q, keys_block.transpose(0, 1)).to(torch.float32) * q_scale * k_scale\n",
      "        m_ij = torch.maximum(m_i, qk.max(dim=1).values)\n",
      "        qk = qk - m_ij.unsqueeze(1)\n",
      "        p = torch.pow(2.0, qk)\n",
      "        l_ij = p.sum(dim=1)\n",
      "        alpha = torch.pow(2.0, m_i - m_ij)\n",
      "        l_i = l_i * alpha + l_ij\n",
      "        acc = acc * alpha.unsqueeze(1)\n",
      "        p_fp16 = p.half()\n",
      "        acc = acc + torch.matmul(p_fp16, values_block.half())\n",
      "        m_i = m_ij\n",
      "    return acc, l_i\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_torch_attn_fwd_inner():\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "    device = torch.device(DEVICE)\n",
      "    \n",
      "    # Test case 1\n",
      "    M = 2\n",
      "    HEAD_DIM = 4\n",
      "    BLOCK_N = 3\n",
      "    N_CTX = 7\n",
      "    q = torch.randn(M, HEAD_DIM, dtype=torch.float32, device=device)\n",
      "    q_scale = 0.5\n",
      "    keys = torch.randn(N_CTX, HEAD_DIM, dtype=torch.float32, device=device)\n",
      "    values = torch.randn(N_CTX, HEAD_DIM, dtype=torch.float32, device=device)\n",
      "    num_blocks = (N_CTX + BLOCK_N - 1) // BLOCK_N\n",
      "    key_scales = torch.randn(num_blocks, dtype=torch.float32, device=device).abs() + 0.5\n",
      "    acc = torch.zeros(M, HEAD_DIM, dtype=torch.float32, device=device)\n",
      "    l_i = torch.zeros(M, dtype=torch.float32, device=device)\n",
      "    m_i = torch.full((M,), -float('inf'), dtype=torch.float32, device=device)\n",
      "    acc_out, l_i_out = torch_attn_fwd_inner(acc, l_i, m_i, q, q_scale, keys, key_scales, values, BLOCK_N)\n",
      "    results['attn_fwd_inner'] = { 'q': q, 'q_scale': q_scale, 'keys': keys, 'key_scales': key_scales, \n",
      "                                  'values': values, 'acc_out': acc_out, 'l_i_out': l_i_out }\n",
      "    \n",
      "    # Test case 2\n",
      "    M2 = 3\n",
      "    HEAD_DIM2 = 8\n",
      "    BLOCK_N2 = 4\n",
      "    N_CTX2 = 10\n",
      "    q2 = torch.randn(M2, HEAD_DIM2, dtype=torch.float32, device=device)\n",
      "    q_scale2 = 1.2\n",
      "    keys2 = torch.randn(N_CTX2, HEAD_DIM2, dtype=torch.float32, device=device)\n",
      "    values2 = torch.randn(N_CTX2, HEAD_DIM2, dtype=torch.float32, device=device)\n",
      "    num_blocks2 = (N_CTX2 + BLOCK_N2 - 1) // BLOCK_N2\n",
      "    key_scales2 = torch.randn(num_blocks2, dtype=torch.float32, device=device).abs() + 0.5\n",
      "    acc2 = torch.zeros(M2, HEAD_DIM2, dtype=torch.float32, device=device)\n",
      "    l_i2 = torch.zeros(M2, dtype=torch.float32, device=device)\n",
      "    m_i2 = torch.full((M2,), -float('inf'), dtype=torch.float32, device=device)\n",
      "    acc_out2, l_i_out2 = torch_attn_fwd_inner(acc2, l_i2, m_i2, q2, q_scale2, keys2, key_scales2, values2, BLOCK_N2)\n",
      "    results['attn_fwd_inner_variant'] = { 'q': q2, 'q_scale': q_scale2, 'keys': keys2, 'key_scales': key_scales2, \n",
      "                                           'values': values2, 'acc_out': acc_out2, 'l_i_out': l_i_out2 }\n",
      "    return results\n",
      "\n",
      "\n",
      "\n",
      "test_results = test_torch_attn_fwd_inner()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "atomic_kernel_\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global Device Standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def atomic_kernel_(x: torch.Tensor, increment: float) -> None:\n",
      "    \"\"\"\n",
      "    PyTorch implementation of atomic addition.\n",
      "    Performs an in-place addition of 'increment' to the tensor 'x'.\n",
      "    \"\"\"\n",
      "    if not isinstance(x, torch.Tensor):\n",
      "        raise TypeError('x must be a torch.Tensor')\n",
      "    # In-place addition on the tensor\n",
      "    x.add_(increment)\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_atomic_kernel_():\n",
      "    \"\"\"\n",
      "    Test the atomic_kernel_ implementation with multiple cases.\n",
      "    Returns a dictionary with test results.\n",
      "    \"\"\"\n",
      "    test_results = {}\n",
      "\n",
      "    # Test Case 1: Single element tensor update: 0.0 + 1.0 = 1.0\n",
      "    x1 = torch.tensor([0.0], device=DEVICE)\n",
      "    atomic_kernel_(x1, 1.0)\n",
      "    test_results[\"test_case_1\"] = x1.item()\n",
      "\n",
      "    # Test Case 2: Single element tensor update with negative increment: 10.0 - 2.5 = 7.5\n",
      "    x2 = torch.tensor([10.0], device=DEVICE)\n",
      "    atomic_kernel_(x2, -2.5)\n",
      "    test_results[\"test_case_2\"] = x2.item()\n",
      "\n",
      "    # Test Case 3: Multi-element tensor, updating only the first element repeatedly.\n",
      "    # Expected: first element = 0.0 + 3*2.0 = 6.0, others remain 0.0\n",
      "    x3 = torch.zeros(5, device=DEVICE)\n",
      "    for _ in range(3):\n",
      "        atomic_kernel_(x3[0:1], 2.0)\n",
      "    test_results[\"test_case_3\"] = x3.cpu().tolist()\n",
      "\n",
      "    # Test Case 4: Update on a larger tensor where every element is incremented by 5.0\n",
      "    x4 = torch.ones(10, device=DEVICE)\n",
      "    atomic_kernel_(x4, 5.0)\n",
      "    test_results[\"test_case_4\"] = x4.cpu().tolist()\n",
      "\n",
      "    # Test Case 5: Error case – non-tensor input should raise a TypeError\n",
      "    try:\n",
      "        atomic_kernel_(123, 1.0)\n",
      "        test_results[\"test_case_5\"] = \"Failed: did not raise error\"\n",
      "    except TypeError as e:\n",
      "        test_results[\"test_case_5\"] = str(e)\n",
      "\n",
      "    print(test_results)\n",
      "    return test_results\n",
      "\n",
      "\n",
      "test_results = test_atomic_kernel_()\n",
      "****************************************************************************************************\n",
      "cross_entropy_forward_\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard - ensures all tensors are created on the correct device\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def cross_entropy_forward_(logits: torch.Tensor,\n",
      "                                  labels: torch.Tensor,\n",
      "                                  do_logit_scaling: bool = False,\n",
      "                                  logit_scale: float = 1.0) -> (torch.Tensor, torch.Tensor):\n",
      "    \"\"\"\n",
      "    Computes the forward pass of cross entropy loss in a numerically stable manner\n",
      "    with optional logit scaling.\n",
      "\n",
      "    Args:\n",
      "      logits: A tensor of shape (batch, VOCAB_SIZE) on DEVICE containing raw logits.\n",
      "      labels: A tensor of shape (batch,) on DEVICE with target indices; -100 indicates ignore.\n",
      "      do_logit_scaling: If True, scales the logits by logit_scale before computation.\n",
      "      logit_scale: The scaling factor for logits if do_logit_scaling is True.\n",
      "\n",
      "    Returns:\n",
      "      A tuple (loss, logsumexp) each of shape (batch,), where loss is 0 for ignored samples.\n",
      "    \"\"\"\n",
      "    # Optionally scale logits\n",
      "    if do_logit_scaling:\n",
      "        scaled_logits = logit_scale * logits\n",
      "    else:\n",
      "        scaled_logits = logits\n",
      "\n",
      "    # Compute stable logsumexp for each row\n",
      "    c, _ = torch.max(scaled_logits, dim=1, keepdim=True)  # shape (batch, 1)\n",
      "    exp_diff = torch.exp(scaled_logits - c)\n",
      "    sum_exp = torch.sum(exp_diff, dim=1)\n",
      "    logsumexp = c.squeeze(1) + torch.log(sum_exp)\n",
      "\n",
      "    # Initialize loss tensor\n",
      "    batch_size = logits.shape[0]\n",
      "    loss = torch.zeros(batch_size, dtype=logits.dtype, device=DEVICE)\n",
      "\n",
      "    # Compute loss only for valid labels (labels != -100)\n",
      "    valid_mask = labels != -100\n",
      "    if valid_mask.any():\n",
      "        x = scaled_logits[valid_mask].gather(1, labels[valid_mask].unsqueeze(1)).squeeze(1)\n",
      "        loss_valid = logsumexp[valid_mask] - x\n",
      "        loss[valid_mask] = loss_valid\n",
      "\n",
      "    return loss, logsumexp\n",
      "\n",
      "########################\n",
      "# Integrated Tests\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_cross_entropy_forward_():\n",
      "    \"\"\"\n",
      "    Runs tests for the PyTorch implementation of cross entropy forward pass.\n",
      "    Test results are stored in a dictionary for easy comparison.\n",
      "    \"\"\"\n",
      "    test_results = {}\n",
      "\n",
      "    if not torch.cuda.is_available():\n",
      "        raise RuntimeError(f\"CUDA is required for this test, but it's not available on {DEVICE}.\")\n",
      "        \n",
      "    # Define test cases\n",
      "    test_cases = {}\n",
      "\n",
      "    # Test case 1: Single example without logit scaling and valid label\n",
      "    logits_1 = torch.tensor([[1.0, 2.0, 3.0]], device=DEVICE)\n",
      "    labels_1 = torch.tensor([2], device=DEVICE)  # target class is 2\n",
      "    test_cases['test_case_1'] = (logits_1, labels_1, False, 1.0)\n",
      "\n",
      "    # Test case 2: Single example with logit scaling and valid label\n",
      "    logits_2 = torch.tensor([[1.0, 2.0, 3.0]], device=DEVICE)\n",
      "    labels_2 = torch.tensor([0], device=DEVICE)  # target class is 0\n",
      "    test_cases['test_case_2'] = (logits_2, labels_2, True, 0.5)\n",
      "\n",
      "    # Test case 3: Batch with multiple examples, mixed labels, no scaling\n",
      "    logits_3 = torch.tensor([\n",
      "        [0.1, 0.2, 0.7],\n",
      "        [2.0, 1.0, 0.1],\n",
      "        [0.5, 0.5, 0.5]\n",
      "    ], device=DEVICE)\n",
      "    labels_3 = torch.tensor([2, 0, 1], device=DEVICE)\n",
      "    test_cases['test_case_3'] = (logits_3, labels_3, False, 1.0)\n",
      "\n",
      "    # Test case 4: Batch with ignore_index (-100) for one sample\n",
      "    logits_4 = torch.tensor([\n",
      "        [1.0, 2.0, 3.0, 0.5],\n",
      "        [2.0, 1.0, 0.1, 0.2]\n",
      "    ], device=DEVICE)\n",
      "    labels_4 = torch.tensor([2, -100], device=DEVICE)  # second sample is ignored\n",
      "    test_cases['test_case_4'] = (logits_4, labels_4, False, 1.0)\n",
      "\n",
      "    # Test case 5: Larger random tensor with logit scaling\n",
      "    batch_size, vocab_size = 5, 10\n",
      "    torch.manual_seed(42)\n",
      "    logits_5 = torch.randn(batch_size, vocab_size, device=DEVICE)\n",
      "    labels_5 = torch.randint(0, vocab_size, (batch_size,), device=DEVICE)\n",
      "    labels_5[0] = -100  # mark first sample as ignore_index\n",
      "    test_cases['test_case_5'] = (logits_5, labels_5, True, 0.8)\n",
      "\n",
      "    # Run each test case using the PyTorch implementation\n",
      "    for name, (logits, labels, do_logit_scaling, logit_scale) in test_cases.items():\n",
      "        logits_contig = logits.contiguous()\n",
      "        labels_contig = labels.contiguous()\n",
      "        loss_torch, logsumexp_torch = cross_entropy_forward_(\n",
      "            logits_contig, labels_contig, do_logit_scaling, logit_scale\n",
      "        )\n",
      "        test_results[name] = {\n",
      "            'pytorch': {\n",
      "                'loss': loss_torch.detach().cpu(),\n",
      "                'logsumexp': logsumexp_torch.detach().cpu()\n",
      "            }\n",
      "        }\n",
      "    \n",
      "    # Only print the final test_results dictionary\n",
      "    print(test_results)\n",
      "    return test_results\n",
      "\n",
      "# Print test results\n",
      "test_results = test_cross_entropy_forward_()\n",
      "\n",
      "****************************************************************************************************\n",
      "softmax_online\n",
      "====================================================================================================\n",
      "import torch\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "# Pure PyTorch Implementation of the Online Softmax Kernel\n",
      "\n",
      "def softmax_online(input_tensor: torch.Tensor, dim: int = 1) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Computes numerically stable softmax along the specified dimension.\n",
      "    Implements the online approach by subtracting the maximum, exponentiating,\n",
      "    and normalizing.\n",
      "    \"\"\"\n",
      "    max_vals, _ = torch.max(input_tensor, dim=dim, keepdim=True)\n",
      "    input_stable = input_tensor - max_vals\n",
      "    exp_input = torch.exp(input_stable)\n",
      "    sum_exp = exp_input.sum(dim=dim, keepdim=True)\n",
      "    softmax_output = exp_input / sum_exp\n",
      "    return softmax_output\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_softmax_online():\n",
      "    results = {}\n",
      "    \n",
      "    # Test case 1: Simple 2x2 tensor\n",
      "    input_tensor1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\n",
      "    output1 = softmax_online(input_tensor1, dim=1)\n",
      "    results['test_case_1'] = {\n",
      "        'input': input_tensor1,\n",
      "        'output': output1,\n",
      "        'sum_along_dim1': output1.sum(dim=1)  # Expected to be close to ones\n",
      "    }\n",
      "    \n",
      "    # Test case 2: 3x4 tensor with negative values\n",
      "    input_tensor2 = torch.tensor([\n",
      "        [-1.0, -2.0, -3.0, -4.0],\n",
      "        [ 0.0,  0.0,  0.0,  0.0],\n",
      "        [ 1.0,  2.0,  3.0,  4.0]\n",
      "    ], device=DEVICE)\n",
      "    output2 = softmax_online(input_tensor2, dim=1)\n",
      "    results['test_case_2'] = {\n",
      "        'input': input_tensor2,\n",
      "        'output': output2,\n",
      "        'sum_along_dim1': output2.sum(dim=1)\n",
      "    }\n",
      "    \n",
      "    # Test case 3: High-dimensional tensor (5x10)\n",
      "    torch.manual_seed(0)\n",
      "    input_tensor3 = torch.randn(5, 10, device=DEVICE)\n",
      "    output3 = softmax_online(input_tensor3, dim=1)\n",
      "    results['test_case_3'] = {\n",
      "        'input': input_tensor3,\n",
      "        'output': output3,\n",
      "        'sum_along_dim1': output3.sum(dim=1)\n",
      "    }\n",
      "    \n",
      "    # Test case 4: Softmax computed along dim=0 (no transposition needed)\n",
      "    input_tensor4 = torch.tensor([\n",
      "        [1.0, 3.0, 5.0],\n",
      "        [2.0, 4.0, 6.0],\n",
      "        [0.5, 1.5, 2.5]\n",
      "    ], device=DEVICE)\n",
      "    output4 = softmax_online(input_tensor4, dim=0)\n",
      "    results['test_case_4'] = {\n",
      "        'input': input_tensor4,\n",
      "        'output': output4,\n",
      "        'sum_along_dim0': output4.sum(dim=0)\n",
      "    }\n",
      "    \n",
      "    # Test case 5: Larger random tensor (100x200)\n",
      "    input_tensor5 = torch.randn(100, 200, device=DEVICE)\n",
      "    output5 = softmax_online(input_tensor5, dim=1)\n",
      "    results['test_case_5'] = {\n",
      "        'output_shape': output5.shape,\n",
      "        'sum_along_dim1_first5': output5[:5].sum(dim=1)\n",
      "    }\n",
      "    \n",
      "    return results\n",
      "\n",
      "\n",
      "# Run PyTorch tests and print results\n",
      "\n",
      "test_results = test_softmax_online()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "attn_fwd_inner_\n",
      "====================================================================================================\n",
      "import math\n",
      "import torch\n",
      "\n",
      "# Global Device Standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def attn_fwd_inner_(acc, l_i, m_i, q, q_scale, K, K_scales, V,\n",
      "                         start_m, BLOCK_M, HEAD_DIM, BLOCK_N, STAGE, offs_m, offs_n, N_CTX):\n",
      "    \"\"\"\n",
      "    A pure PyTorch implementation that mimics the Triton _attn_fwd_inner kernel.\n",
      "\n",
      "    Arguments:\n",
      "       acc: Tensor of shape (BLOCK_M, HEAD_DIM), accumulator for the output.\n",
      "       l_i: Tensor of shape (BLOCK_M,), normalization accumulator.\n",
      "       m_i: Tensor of shape (BLOCK_M,), current max values for numerical stability.\n",
      "       q: Query tensor of shape (BLOCK_M, HEAD_DIM).\n",
      "       q_scale: Scaling factor for q (float).\n",
      "       K: Key matrix tensor of shape (N_CTX, HEAD_DIM).\n",
      "       K_scales: 1D tensor containing scaling factors for each key block.\n",
      "       V: Value matrix tensor of shape (N_CTX, HEAD_DIM).\n",
      "       start_m: integer, used to determine block range.\n",
      "       BLOCK_M: Block size for the query (number of rows).\n",
      "       HEAD_DIM: dimension of the head.\n",
      "       BLOCK_N: Block size for keys/values.\n",
      "       STAGE: either 1 or 2, indicating which stage to perform.\n",
      "       offs_m: 1D tensor of offsets for rows, shape (BLOCK_M,).\n",
      "       offs_n: 1D tensor of offsets for columns, shape (BLOCK_N,).\n",
      "       N_CTX: integer, maximum context length.\n",
      "    Returns:\n",
      "       Tuple of (acc, l_i, m_i) after processing the blocks.\n",
      "    \"\"\"\n",
      "    if STAGE == 1:\n",
      "        lo = 0\n",
      "        hi = start_m * BLOCK_M\n",
      "    elif STAGE == 2:\n",
      "        lo = start_m * BLOCK_M\n",
      "        hi = (start_m + 1) * BLOCK_M\n",
      "    else:\n",
      "        raise ValueError(\"STAGE should be either 1 or 2\")\n",
      "\n",
      "    pointer_idx = 0  # used for K_scales indexing\n",
      "    for start_n in range(lo, hi, BLOCK_N):\n",
      "        current_block_N = min(BLOCK_N, N_CTX - start_n)\n",
      "        if current_block_N <= 0:\n",
      "            break\n",
      "        k = K[start_n: start_n + current_block_N, :]\n",
      "        k_scale = K_scales[pointer_idx]\n",
      "        pointer_idx += 1\n",
      "        qk = torch.matmul(q, k.transpose(0, 1)).to(torch.float32) * q_scale * k_scale\n",
      "        if STAGE == 2:\n",
      "            mask = offs_m.unsqueeze(1) >= (start_n + offs_n[:current_block_N].unsqueeze(0))\n",
      "            qk = qk + torch.where(mask,\n",
      "                                   torch.tensor(0.0, device=qk.device, dtype=qk.dtype),\n",
      "                                   torch.tensor(-1000000.0, device=qk.device, dtype=qk.dtype))\n",
      "            qk_row_max, _ = torch.max(qk, dim=1)\n",
      "            m_ij = torch.max(m_i, qk_row_max)\n",
      "            qk = qk - m_ij.unsqueeze(1)\n",
      "        else:\n",
      "            qk_row_max, _ = torch.max(qk, dim=1)\n",
      "            m_ij = torch.max(m_i, qk_row_max)\n",
      "            qk = qk - m_ij.unsqueeze(1)\n",
      "        p = torch.pow(2.0, qk)\n",
      "        l_ij = torch.sum(p, dim=1)\n",
      "        alpha = torch.pow(2.0, m_i - m_ij)\n",
      "        l_i = l_i * alpha + l_ij\n",
      "        acc = acc * alpha.unsqueeze(1)\n",
      "        v = V[start_n: start_n + current_block_N, :]\n",
      "        p = p.to(torch.float16)\n",
      "        acc = acc + torch.matmul(p, v.to(p.dtype)).to(acc.dtype)\n",
      "        m_i = m_ij\n",
      "    return acc, l_i, m_i\n",
      "\n",
      "\n",
      "########################\n",
      "# Tests\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_attn_fwd_inner_():\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "    device = DEVICE\n",
      "\n",
      "    # Test parameters for Stage 1 and 2\n",
      "    BLOCK_M = 2\n",
      "    HEAD_DIM = 4\n",
      "    BLOCK_N = 2\n",
      "    N_CTX = 6\n",
      "\n",
      "    offs_m = torch.arange(BLOCK_M, device=device)\n",
      "    offs_n = torch.arange(BLOCK_N, device=device)\n",
      "\n",
      "    q = torch.randn(BLOCK_M, HEAD_DIM, dtype=torch.float32, device=device)\n",
      "    q_scale = 1.0\n",
      "\n",
      "    K = torch.randn(N_CTX, HEAD_DIM, dtype=torch.float32, device=device)\n",
      "    V = torch.randn(N_CTX, HEAD_DIM, dtype=torch.float32, device=device)\n",
      "    num_blocks = N_CTX // BLOCK_N\n",
      "    K_scales = torch.ones(num_blocks, dtype=torch.float32, device=device)\n",
      "\n",
      "    init_acc = torch.zeros(BLOCK_M, HEAD_DIM, dtype=torch.float32, device=device)\n",
      "    init_l_i = torch.zeros(BLOCK_M, dtype=torch.float32, device=device)\n",
      "    init_m_i = torch.full((BLOCK_M,), -100000.0, dtype=torch.float32, device=device)\n",
      "\n",
      "    # Test Case 1: STAGE 1\n",
      "    start_m_stage1 = 2\n",
      "    acc_p, l_i_p, m_i_p = attn_fwd_inner_(init_acc.clone(), init_l_i.clone(), init_m_i.clone(),\n",
      "                                                 q, q_scale, K, K_scales, V,\n",
      "                                                 start_m_stage1, BLOCK_M, HEAD_DIM, BLOCK_N, 1, offs_m, offs_n, N_CTX)\n",
      "    results['stage1'] = { 'torch': {'acc': acc_p, 'l_i': l_i_p, 'm_i': m_i_p} }\n",
      "\n",
      "    # Test Case 2: STAGE 2\n",
      "    start_m_stage2 = 2\n",
      "    acc_p2, l_i_p2, m_i_p2 = attn_fwd_inner_(init_acc.clone(), init_l_i.clone(), init_m_i.clone(),\n",
      "                                                    q, q_scale, K, K_scales, V,\n",
      "                                                    start_m_stage2, BLOCK_M, HEAD_DIM, BLOCK_N, 2, offs_m, offs_n, N_CTX)\n",
      "    results['stage2'] = { 'torch': {'acc': acc_p2, 'l_i': l_i_p2, 'm_i': m_i_p2} }\n",
      "\n",
      "    # Test Case 3: Different Shapes with STAGE 1\n",
      "    BLOCK_M_new = 3\n",
      "    HEAD_DIM_new = 5\n",
      "    BLOCK_N_new = 3\n",
      "    N_CTX_new = 9\n",
      "    offs_m_new = torch.arange(BLOCK_M_new, device=device)\n",
      "    offs_n_new = torch.arange(BLOCK_N_new, device=device)\n",
      "    q_new = torch.randn(BLOCK_M_new, HEAD_DIM_new, dtype=torch.float32, device=device)\n",
      "    K_new = torch.randn(N_CTX_new, HEAD_DIM_new, dtype=torch.float32, device=device)\n",
      "    V_new = torch.randn(N_CTX_new, HEAD_DIM_new, dtype=torch.float32, device=device)\n",
      "    num_blocks_new = N_CTX_new // BLOCK_N_new\n",
      "    K_scales_new = torch.ones(num_blocks_new, dtype=torch.float32, device=device)\n",
      "    init_acc_new = torch.zeros(BLOCK_M_new, HEAD_DIM_new, dtype=torch.float32, device=device)\n",
      "    init_l_i_new = torch.zeros(BLOCK_M_new, dtype=torch.float32, device=device)\n",
      "    init_m_i_new = torch.full((BLOCK_M_new,), -100000.0, dtype=torch.float32, device=device)\n",
      "    start_m_new = 2\n",
      "    acc_p3, l_i_p3, m_i_p3 = attn_fwd_inner_(init_acc_new.clone(), init_l_i_new.clone(), init_m_i_new.clone(),\n",
      "                                                    q_new, q_scale, K_new, K_scales_new, V_new,\n",
      "                                                    start_m_new, BLOCK_M_new, HEAD_DIM_new, BLOCK_N_new, 1, offs_m_new, offs_n_new, N_CTX_new)\n",
      "    results['stage1_diff_shape'] = { 'torch': {'acc': acc_p3, 'l_i': l_i_p3, 'm_i': m_i_p3} }\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "########################\n",
      "# End of Testing Code\n",
      "########################\n",
      "\n",
      "test_results = test_attn_fwd_inner_()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "torch_output_sparse_matmul\n",
      "====================================================================================================\n",
      "import torch\n",
      "import math\n",
      "\n",
      "# Global Device Standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def torch_output_sparse_matmul(lhs: torch.Tensor,\n",
      "                               rhs: torch.Tensor,\n",
      "                               expert_ends: torch.Tensor,\n",
      "                               col_sparse_dim: int,\n",
      "                               sparse_dim: int,\n",
      "                               block_size_row: int,\n",
      "                               block_size_col: int,\n",
      "                               block_size_inner: int,\n",
      "                               accumulate: bool = False,\n",
      "                               out: torch.Tensor = None) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Pure PyTorch implementation of sparse matrix multiplication.\n",
      "\n",
      "    Parameters:\n",
      "      lhs: Tensor of shape (row_dim, inner_dim).\n",
      "      rhs: Tensor of shape (inner_dim, padded_sparse_dim * col_sparse_dim).\n",
      "      expert_ends: 1D Tensor (first sparse_dim entries valid).\n",
      "      col_sparse_dim: Output sparse block width.\n",
      "      sparse_dim: Number of non-zero blocks.\n",
      "      block_size_row: Block size in rows.\n",
      "      block_size_col: Block size in columns.\n",
      "      block_size_inner: Block size in inner dimension.\n",
      "      accumulate: If True, accumulate onto pre-existing output.\n",
      "      out: Optional preallocated output tensor.\n",
      "      \n",
      "    Returns:\n",
      "      Tensor of shape (row_dim, col_sparse_dim).\n",
      "    \"\"\"\n",
      "    row_dim, inner_dim = lhs.shape\n",
      "    padded_sparse_dim = expert_ends.numel()\n",
      "\n",
      "    if row_dim % block_size_row != 0:\n",
      "        raise ValueError('row_dim must be divisible by block_size_row')\n",
      "    if col_sparse_dim % block_size_col != 0:\n",
      "        raise ValueError('col_sparse_dim must be divisible by block_size_col')\n",
      "    if inner_dim % block_size_inner != 0:\n",
      "        raise ValueError('inner_dim must be divisible by block_size_inner')\n",
      "\n",
      "    if out is None:\n",
      "        out = torch.zeros((row_dim, col_sparse_dim), dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    n_row_blocks = row_dim // block_size_row\n",
      "    n_col_blocks = col_sparse_dim // block_size_col\n",
      "    n_inner_blocks = inner_dim // block_size_inner\n",
      "\n",
      "    for i in range(n_row_blocks):\n",
      "        row_offset = i * block_size_row\n",
      "        sparse_index = int((expert_ends[:sparse_dim] <= row_offset).sum().item())\n",
      "\n",
      "        for j in range(n_col_blocks):\n",
      "            col_sparse_offset = j * block_size_col\n",
      "            if sparse_index == sparse_dim:\n",
      "                continue\n",
      "            col_dense_offset = col_sparse_offset + sparse_index * col_sparse_dim\n",
      "            block_out = torch.zeros((block_size_row, block_size_col), dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "            for k in range(n_inner_blocks):\n",
      "                inner_offset = k * block_size_inner\n",
      "                lhs_block = lhs[row_offset:row_offset+block_size_row, inner_offset:inner_offset+block_size_inner]\n",
      "                rhs_block = rhs[inner_offset:inner_offset+block_size_inner, col_dense_offset:col_dense_offset+block_size_col]\n",
      "                block_out = block_out + torch.matmul(lhs_block, rhs_block)\n",
      "\n",
      "            if accumulate:\n",
      "                out[row_offset:row_offset+block_size_row, col_sparse_offset:col_sparse_offset+block_size_col] += block_out\n",
      "            else:\n",
      "                out[row_offset:row_offset+block_size_row, col_sparse_offset:col_sparse_offset+block_size_col] = block_out\n",
      "\n",
      "    return out\n",
      "\n",
      "########################\n",
      "# PyTorch Test Cases\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_torch_output_sparse_matmul():\n",
      "    results = {}\n",
      "\n",
      "    # Test Case 1: Small dimensions, no accumulation\n",
      "    row_dim = 4\n",
      "    inner_dim = 4\n",
      "    col_sparse_dim = 4\n",
      "    block_size_row = 2\n",
      "    block_size_inner = 2\n",
      "    block_size_col = 2\n",
      "    sparse_dim = 2\n",
      "    padded_sparse_dim = 3\n",
      "\n",
      "    lhs = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n",
      "                          [5.0, 6.0, 7.0, 8.0],\n",
      "                          [9.0, 10.0, 11.0, 12.0],\n",
      "                          [13.0, 14.0, 15.0, 16.0]], dtype=torch.float32, device=DEVICE)\n",
      "    rhs = torch.arange(1, inner_dim * padded_sparse_dim * col_sparse_dim + 1,\n",
      "                       dtype=torch.float32, device=DEVICE).reshape(inner_dim, padded_sparse_dim * col_sparse_dim)\n",
      "    expert_ends = torch.tensor([1, 3, row_dim], dtype=torch.int64, device=DEVICE)\n",
      "\n",
      "    out1 = torch_output_sparse_matmul(lhs, rhs, expert_ends, col_sparse_dim, sparse_dim,\n",
      "                                      block_size_row, block_size_col, block_size_inner, accumulate=False)\n",
      "    results['test_case_1_no_accumulate'] = out1\n",
      "\n",
      "    # Test Case 2: Accumulation with non-zero initial output\n",
      "    out_init = torch.ones((row_dim, col_sparse_dim), dtype=torch.float32, device=DEVICE)\n",
      "    out2 = torch_output_sparse_matmul(lhs, rhs, expert_ends, col_sparse_dim, sparse_dim,\n",
      "                                      block_size_row, block_size_col, block_size_inner, accumulate=True, out=out_init.clone())\n",
      "    results['test_case_2_accumulate'] = out2\n",
      "\n",
      "    # Test Case 3: Larger dimensions\n",
      "    row_dim = 6\n",
      "    inner_dim = 6\n",
      "    col_sparse_dim = 6\n",
      "    block_size_row = 3\n",
      "    block_size_inner = 2\n",
      "    block_size_col = 3\n",
      "    sparse_dim = 2\n",
      "    padded_sparse_dim = 3\n",
      "    lhs = torch.arange(1, row_dim * inner_dim + 1, dtype=torch.float32, device=DEVICE).reshape(row_dim, inner_dim)\n",
      "    rhs = torch.arange(1, inner_dim * padded_sparse_dim * col_sparse_dim + 1,\n",
      "                       dtype=torch.float32, device=DEVICE).reshape(inner_dim, padded_sparse_dim * col_sparse_dim)\n",
      "    expert_ends = torch.tensor([2, 5, row_dim], dtype=torch.int64, device=DEVICE)\n",
      "\n",
      "    out3 = torch_output_sparse_matmul(lhs, rhs, expert_ends, col_sparse_dim, sparse_dim,\n",
      "                                      block_size_row, block_size_col, block_size_inner, accumulate=False)\n",
      "    results['test_case_3_no_accumulate'] = out3\n",
      "\n",
      "    # Test Case 4: Skipping block when sparse_index equals sparse_dim\n",
      "    row_dim = 4\n",
      "    inner_dim = 4\n",
      "    col_sparse_dim = 4\n",
      "    block_size_row = 2\n",
      "    block_size_inner = 2\n",
      "    block_size_col = 2\n",
      "    sparse_dim = 2\n",
      "    padded_sparse_dim = 2\n",
      "    lhs = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n",
      "                          [5.0, 6.0, 7.0, 8.0],\n",
      "                          [9.0, 10.0, 11.0, 12.0],\n",
      "                          [13.0, 14.0, 15.0, 16.0]], dtype=torch.float32, device=DEVICE)\n",
      "    rhs = torch.arange(1, inner_dim * padded_sparse_dim * col_sparse_dim + 1,\n",
      "                       dtype=torch.float32, device=DEVICE).reshape(inner_dim, padded_sparse_dim * col_sparse_dim)\n",
      "    expert_ends = torch.tensor([0, 0], dtype=torch.int64, device=DEVICE)\n",
      "    out4 = torch_output_sparse_matmul(lhs, rhs, expert_ends, col_sparse_dim, sparse_dim,\n",
      "                                      block_size_row, block_size_col, block_size_inner, accumulate=False)\n",
      "    results['test_case_4_skip'] = out4\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "# Execute tests and print only the test_results dictionary\n",
      "\n",
      "test_results = test_torch_output_sparse_matmul()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "dequantize_\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def dequantize_(inputs: torch.Tensor, nf4_lut: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    PyTorch implementation of the dequantization operation using indexing.\n",
      "    Given a tensor of indices and a lookup table, returns corresponding dequantized values.\n",
      "    \"\"\"\n",
      "    # Ensure tensors are on the correct device\n",
      "    if not inputs.is_cuda:\n",
      "        inputs = inputs.to(DEVICE)\n",
      "    if not nf4_lut.is_cuda:\n",
      "        nf4_lut = nf4_lut.to(DEVICE)\n",
      "\n",
      "    # Convert indices to long for proper indexing and return the dequantized values\n",
      "    indices = inputs.long()\n",
      "    return nf4_lut[indices]\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_dequantize_():\n",
      "    \"\"\"\n",
      "    Runs tests for the PyTorch dequantization implementation.\n",
      "    The expected output is obtained by directly indexing the lookup table with the inputs.\n",
      "    \"\"\"\n",
      "    test_results = {}\n",
      "\n",
      "    # Test case 1: Simple 1D tensor dequantization\n",
      "    nf4_lut = torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], dtype=torch.bfloat16, device=DEVICE)\n",
      "    inputs = torch.tensor([0, 2, 4, 6, 8], device=DEVICE)\n",
      "    expected = nf4_lut[inputs]\n",
      "    output_torch = dequantize_(inputs, nf4_lut)\n",
      "    test_results['test_case_1'] = {\n",
      "        'inputs': inputs,\n",
      "        'expected': expected,\n",
      "        'pytorch_output': output_torch\n",
      "    }\n",
      "\n",
      "    # Test case 2: 2D tensor dequantization\n",
      "    nf4_lut = torch.tensor([float(i) for i in range(16)], dtype=torch.bfloat16, device=DEVICE)\n",
      "    inputs = torch.tensor([[0, 5, 10], [3, 7, 15]], device=DEVICE)\n",
      "    expected = nf4_lut[inputs]\n",
      "    output_torch = dequantize_(inputs, nf4_lut)\n",
      "    test_results['test_case_2'] = {\n",
      "        'inputs': inputs,\n",
      "        'expected': expected,\n",
      "        'pytorch_output': output_torch\n",
      "    }\n",
      "\n",
      "    # Test case 3: Edge case with maximum index of the lookup table\n",
      "    nf4_lut = torch.tensor([i * 0.5 for i in range(20)], dtype=torch.bfloat16, device=DEVICE)\n",
      "    inputs = torch.tensor([19, 0, 10, 5], device=DEVICE)\n",
      "    expected = nf4_lut[inputs]\n",
      "    output_torch = dequantize_(inputs, nf4_lut)\n",
      "    test_results['test_case_3'] = {\n",
      "        'inputs': inputs,\n",
      "        'expected': expected,\n",
      "        'pytorch_output': output_torch\n",
      "    }\n",
      "\n",
      "    # Test case 4: Random indices within the range provided by nf4_lut\n",
      "    torch.manual_seed(0)  # for reproducibility\n",
      "    nf4_lut = torch.linspace(-5.0, 5.0, steps=50, dtype=torch.bfloat16, device=DEVICE)\n",
      "    inputs = torch.randint(0, 50, (10,), device=DEVICE)\n",
      "    expected = nf4_lut[inputs]\n",
      "    output_torch = dequantize_(inputs, nf4_lut)\n",
      "    test_results['test_case_4'] = {\n",
      "        'inputs': inputs,\n",
      "        'expected': expected,\n",
      "        'pytorch_output': output_torch\n",
      "    }\n",
      "\n",
      "    return test_results\n",
      "\n",
      "\n",
      "# Execute tests\n",
      "\n",
      "test_results = test_dequantize_()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "rms_norm_bwd_\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def rms_norm_bwd_(dY: torch.Tensor, X: torch.Tensor, W: torch.Tensor, Rstd: torch.Tensor, N: int):\n",
      "    \"\"\"Compute the backward pass for RMS normalization using pure PyTorch operations.\n",
      "\n",
      "    Args:\n",
      "        dY (torch.Tensor): Gradient of the output, shape (B, stride)\n",
      "        X (torch.Tensor): Input matrix X, shape (B, stride)\n",
      "        W (torch.Tensor): Weight matrix W, shape (B, stride)\n",
      "        Rstd (torch.Tensor): Inverse standard deviation per row, shape (B,)\n",
      "        N (int): Number of valid columns per row.\n",
      "\n",
      "    Returns:\n",
      "        dX (torch.Tensor): Gradient with respect to X.\n",
      "        dW (torch.Tensor): Gradient with respect to W.\n",
      "    \"\"\"\n",
      "    B, stride = X.shape\n",
      "    # Allocate output tensors on the global device\n",
      "    dX = torch.zeros((B, stride), device=DEVICE, dtype=X.dtype)\n",
      "    dW = torch.zeros((B, stride), device=DEVICE, dtype=W.dtype)\n",
      "    \n",
      "    # Compute intermediate product for valid columns\n",
      "    m = dY[:, :N] * W[:, :N]\n",
      "    sum_mx = torch.sum(m * X[:, :N], dim=1, keepdim=True)\n",
      "    rstd_unsq = Rstd.unsqueeze(1)\n",
      "    \n",
      "    # Gradient computation for valid columns\n",
      "    dX_valid = rstd_unsq * m - ((rstd_unsq ** 3) / N) * sum_mx * X[:, :N]\n",
      "    dW_valid = dY[:, :N] * (X[:, :N] * rstd_unsq)\n",
      "    \n",
      "    dX[:, :N] = dX_valid\n",
      "    dW[:, :N] = dW_valid\n",
      "    return dX, dW\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_rms_norm_bwd_():\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "    \n",
      "    # Test case 1: Single row, full valid columns\n",
      "    B = 1\n",
      "    stride = 8\n",
      "    N = 8  # All columns valid\n",
      "    X = torch.randn(B, stride, device=DEVICE)\n",
      "    W = torch.randn(B, stride, device=DEVICE)\n",
      "    dY = torch.randn(B, stride, device=DEVICE)\n",
      "    Rstd = torch.abs(torch.randn(B, device=DEVICE)) + 0.1\n",
      "    dX, dW = rms_norm_bwd_(dY, X, W, Rstd, N)\n",
      "    results[\"test_case_1\"] = {\"dX\": dX, \"dW\": dW}\n",
      "    \n",
      "    # Test case 2: Multiple rows, with N less than stride\n",
      "    B = 3\n",
      "    stride = 10\n",
      "    N = 5  # Only first 5 columns valid\n",
      "    X = torch.randn(B, stride, device=DEVICE)\n",
      "    W = torch.randn(B, stride, device=DEVICE)\n",
      "    dY = torch.randn(B, stride, device=DEVICE)\n",
      "    Rstd = torch.abs(torch.randn(B, device=DEVICE)) + 0.1\n",
      "    dX, dW = rms_norm_bwd_(dY, X, W, Rstd, N)\n",
      "    results[\"test_case_2\"] = {\"dX\": dX, \"dW\": dW}\n",
      "    \n",
      "    # Test case 3: Edge case with zero gradients\n",
      "    B = 2\n",
      "    stride = 6\n",
      "    N = 4\n",
      "    X = torch.randn(B, stride, device=DEVICE)\n",
      "    W = torch.randn(B, stride, device=DEVICE)\n",
      "    dY = torch.zeros(B, stride, device=DEVICE)\n",
      "    Rstd = torch.abs(torch.randn(B, device=DEVICE)) + 0.1\n",
      "    dX, dW = rms_norm_bwd_(dY, X, W, Rstd, N)\n",
      "    results[\"test_case_3\"] = {\"dX\": dX, \"dW\": dW}\n",
      "    \n",
      "    # Test case 4: Known fixed values for a single row\n",
      "    B = 1\n",
      "    stride = 4\n",
      "    N = 4\n",
      "    X = torch.tensor([[1.0, 2.0, 3.0, 4.0]], device=DEVICE)\n",
      "    W = torch.tensor([[0.5, 1.0, 1.5, 2.0]], device=DEVICE)\n",
      "    dY = torch.tensor([[0.1, 0.2, 0.3, 0.4]], device=DEVICE)\n",
      "    Rstd = torch.tensor([2.0], device=DEVICE)\n",
      "    dX, dW = rms_norm_bwd_(dY, X, W, Rstd, N)\n",
      "    results[\"test_case_4\"] = {\"dX\": dX, \"dW\": dW}\n",
      "    \n",
      "    return results\n",
      "\n",
      "\n",
      "# Execute tests\n",
      "\n",
      "test_results = test_rms_norm_bwd_()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "pytorch_update_ema\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_update_ema(prev_ema, new_val, momentum):\n",
      "    \"\"\"\n",
      "    Pure PyTorch implementation of the exponential moving average update.\n",
      "\n",
      "    Args:\n",
      "        prev_ema (torch.Tensor or float): The previous EMA value.\n",
      "        new_val (torch.Tensor or float): The new value used for the update.\n",
      "        momentum (float): Momentum parameter (between 0 and 1).\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor or float: The updated EMA value.\n",
      "    \"\"\"\n",
      "    return (1 - momentum) * prev_ema + momentum * new_val\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_update_ema():\n",
      "    \"\"\"\n",
      "    Runs tests for the PyTorch exponential moving average update implementation.\n",
      "    All applicable test cases use CUDA (DEVICE) for tensor creation.\n",
      "\n",
      "    Returns:\n",
      "        dict: Dictionary containing test results for each test case.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "\n",
      "    # Test Case 1: Scalar values\n",
      "    prev_ema_1 = 0.0\n",
      "    new_val_1 = 1.0\n",
      "    momentum_1 = 0.1\n",
      "    expected_1 = (1 - momentum_1) * prev_ema_1 + momentum_1 * new_val_1\n",
      "    output_pytorch_1 = pytorch_update_ema(prev_ema_1, new_val_1, momentum_1)\n",
      "    results[\"scalar_test\"] = {\n",
      "        \"prev_ema\": prev_ema_1,\n",
      "        \"new_val\": new_val_1,\n",
      "        \"momentum\": momentum_1,\n",
      "        \"expected\": expected_1,\n",
      "        \"output\": output_pytorch_1\n",
      "    }\n",
      "\n",
      "    # Test Case 2: 1D tensor values\n",
      "    prev_ema_2 = torch.tensor([0.0, 0.5, 1.0], device=DEVICE)\n",
      "    new_val_2 = torch.tensor([1.0, 1.5, 2.0], device=DEVICE)\n",
      "    momentum_2 = 0.2\n",
      "    expected_2 = (1 - momentum_2) * prev_ema_2 + momentum_2 * new_val_2\n",
      "    output_pytorch_2 = pytorch_update_ema(prev_ema_2, new_val_2, momentum_2)\n",
      "    results[\"1d_tensor_test\"] = {\n",
      "        \"prev_ema\": prev_ema_2,\n",
      "        \"new_val\": new_val_2,\n",
      "        \"momentum\": momentum_2,\n",
      "        \"expected\": expected_2,\n",
      "        \"output\": output_pytorch_2\n",
      "    }\n",
      "\n",
      "    # Test Case 3: Multi-dimensional tensor values\n",
      "    prev_ema_3 = torch.tensor([[0.0, 1.0], [2.0, 3.0]], device=DEVICE)\n",
      "    new_val_3 = torch.tensor([[1.0, 0.0], [3.0, 2.0]], device=DEVICE)\n",
      "    momentum_3 = 0.5\n",
      "    expected_3 = (1 - momentum_3) * prev_ema_3 + momentum_3 * new_val_3\n",
      "    output_pytorch_3 = pytorch_update_ema(prev_ema_3, new_val_3, momentum_3)\n",
      "    results[\"multi_dim_tensor_test\"] = {\n",
      "        \"prev_ema\": prev_ema_3,\n",
      "        \"new_val\": new_val_3,\n",
      "        \"momentum\": momentum_3,\n",
      "        \"expected\": expected_3,\n",
      "        \"output\": output_pytorch_3\n",
      "    }\n",
      "\n",
      "    # Test Case 4: Floats converted to tensors\n",
      "    prev_ema_4 = torch.tensor(5.0, device=DEVICE)\n",
      "    new_val_4 = torch.tensor(10.0, device=DEVICE)\n",
      "    momentum_4 = 0.3\n",
      "    expected_4 = (1 - momentum_4) * prev_ema_4 + momentum_4 * new_val_4\n",
      "    output_pytorch_4 = pytorch_update_ema(prev_ema_4, new_val_4, momentum_4)\n",
      "    results[\"float_tensor_test\"] = {\n",
      "        \"prev_ema\": prev_ema_4,\n",
      "        \"new_val\": new_val_4,\n",
      "        \"momentum\": momentum_4,\n",
      "        \"expected\": expected_4,\n",
      "        \"output\": output_pytorch_4\n",
      "    }\n",
      "\n",
      "    return results\n",
      "\n",
      "########################\n",
      "\n",
      "test_results = test_pytorch_update_ema()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_tanh\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def pytorch_tanh(x: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"Compute element-wise tanh using PyTorch based on the identity: 2*sigmoid(2*x)-1.\n",
      "    \n",
      "    Explicitly create tensors on DEVICE.\n",
      "    \"\"\"\n",
      "    return 2 * torch.sigmoid(2 * x) - 1\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_tanh():\n",
      "    \"\"\"Test the PyTorch implementation of tanh with multiple test cases.\"\"\"\n",
      "    test_results = {}\n",
      "    \n",
      "    # Prepare test inputs on the global DEVICE\n",
      "    test_inputs = {\n",
      "        \"scalar_zero\": torch.tensor(0.0, device=DEVICE),\n",
      "        \"scalar_negative\": torch.tensor(-1.0, device=DEVICE),\n",
      "        \"1d_tensor\": torch.tensor([-1.0, 0.0, 1.0], device=DEVICE),\n",
      "        \"2d_tensor\": torch.tensor([[-2.0, -1.0, 0.0], [1.0, 2.0, 3.0]], device=DEVICE)\n",
      "    }\n",
      "    torch.manual_seed(0)\n",
      "    test_inputs[\"random_tensor\"] = torch.randn(5, 5, device=DEVICE)\n",
      "    \n",
      "    # Compute test results for each case using the PyTorch implementation\n",
      "    for key, tensor in test_inputs.items():\n",
      "        result = pytorch_tanh(tensor).detach().cpu().tolist()\n",
      "        test_results[key] = {\"pytorch\": result}\n",
      "    return test_results\n",
      "\n",
      "\n",
      "# Execute tests and print the test_results dictionary\n",
      "\n",
      "test_results = test_pytorch_tanh()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_dense_to_sparse\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_dense_to_sparse(dense: torch.Tensor,\n",
      "                            mask: torch.Tensor = None,\n",
      "                            seed: int = 0,\n",
      "                            PRUNE: str = 'mse',\n",
      "                            ARRAY_LAYOUT: str = 'row') -> (torch.Tensor, torch.Tensor):\n",
      "    \"\"\"\n",
      "    PyTorch implementation that transforms a dense matrix into a sparse, semi-structured representation\n",
      "    along with metadata. For each 16-column block, the function processes 8 pairs of elements using:\n",
      "      * 'mse': selects the element with larger absolute value.\n",
      "      * 'mask': uses the provided mask to decide (with tiebreak on absolute value).\n",
      "      * any other value: threshold pruning, selecting the first nonzero element (or 0 if both are zero).\n",
      "    The output sparse tensor is of shape (m, (k//16)*8) and a metadata tensor indicates the chosen element per pair.\n",
      "\n",
      "    Parameters:\n",
      "    - dense: 2D tensor with shape (m, k) where k is a multiple of 16.\n",
      "    - mask: Optional mask tensor (required when PRUNE=='mask').\n",
      "    - seed: Unused in this emulation.\n",
      "    - PRUNE: Pruning strategy ('mse', 'mask', or threshold).\n",
      "    - ARRAY_LAYOUT: Must be 'row'.\n",
      "    \n",
      "    Returns:\n",
      "    - sparse: Sparse tensor representation.\n",
      "    - meta: Tensor with indices (0 or 1) indicating the chosen elements.\n",
      "    \"\"\"\n",
      "    if ARRAY_LAYOUT != 'row':\n",
      "        raise NotImplementedError('This PyTorch implementation only supports ARRAY_LAYOUT == \"row\"')\n",
      "    \n",
      "    m, k = dense.shape\n",
      "    if k % 16 != 0:\n",
      "        raise ValueError('Number of columns (k) must be a multiple of 16')\n",
      "    \n",
      "    num_sparse_cols = (k // 16) * 8\n",
      "    # Allocate output tensors on the global DEVICE\n",
      "    sparse = torch.empty((m, num_sparse_cols), dtype=dense.dtype, device=DEVICE)\n",
      "    meta = torch.empty((m, num_sparse_cols), dtype=torch.int64, device=DEVICE)\n",
      "    \n",
      "    if PRUNE == 'mask':\n",
      "        if mask is None:\n",
      "            raise ValueError('Mask must be provided for PRUNE==\"mask\"')\n",
      "        if mask.shape != dense.shape:\n",
      "            raise ValueError('Mask shape must match dense shape')\n",
      "    \n",
      "    # Process each row independently, resetting the output column index for each row.\n",
      "    for i in range(m):\n",
      "        out_col = 0\n",
      "        for j in range(0, k, 16):\n",
      "            block = dense[i, j:j+16]\n",
      "            if PRUNE == 'mse':\n",
      "                for p in range(8):\n",
      "                    idx1 = 2 * p\n",
      "                    idx2 = 2 * p + 1\n",
      "                    a = block[idx1]\n",
      "                    b = block[idx2]\n",
      "                    if torch.abs(a) >= torch.abs(b):\n",
      "                        sparse[i, out_col] = a\n",
      "                        meta[i, out_col] = 0\n",
      "                    else:\n",
      "                        sparse[i, out_col] = b\n",
      "                        meta[i, out_col] = 1\n",
      "                    out_col += 1\n",
      "            elif PRUNE == 'mask':\n",
      "                for p in range(8):\n",
      "                    idx1 = 2 * p\n",
      "                    idx2 = 2 * p + 1\n",
      "                    m1_val = mask[i, j + idx1]\n",
      "                    m2_val = mask[i, j + idx2]\n",
      "                    a = block[idx1]\n",
      "                    b = block[idx2]\n",
      "                    if m1_val and not m2_val:\n",
      "                        sparse[i, out_col] = a\n",
      "                        meta[i, out_col] = 0\n",
      "                    elif m2_val and not m1_val:\n",
      "                        sparse[i, out_col] = b\n",
      "                        meta[i, out_col] = 1\n",
      "                    elif m1_val and m2_val:\n",
      "                        if torch.abs(a) >= torch.abs(b):\n",
      "                            sparse[i, out_col] = a\n",
      "                            meta[i, out_col] = 0\n",
      "                        else:\n",
      "                            sparse[i, out_col] = b\n",
      "                            meta[i, out_col] = 1\n",
      "                    else:\n",
      "                        sparse[i, out_col] = 0\n",
      "                        meta[i, out_col] = 0\n",
      "                    out_col += 1\n",
      "            else:\n",
      "                # Threshold pruning: choose the first nonzero element; otherwise output 0.\n",
      "                for p in range(8):\n",
      "                    idx1 = 2 * p\n",
      "                    idx2 = 2 * p + 1\n",
      "                    a = block[idx1]\n",
      "                    b = block[idx2]\n",
      "                    if a != 0:\n",
      "                        sparse[i, out_col] = a\n",
      "                        meta[i, out_col] = 0\n",
      "                    elif b != 0:\n",
      "                        sparse[i, out_col] = b\n",
      "                        meta[i, out_col] = 1\n",
      "                    else:\n",
      "                        sparse[i, out_col] = 0\n",
      "                        meta[i, out_col] = 0\n",
      "                    out_col += 1\n",
      "    return sparse, meta\n",
      "\n",
      "\n",
      "########################\n",
      "# Integration & Testing\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_dense_to_sparse():\n",
      "    \"\"\"\n",
      "    Runs tests of the PyTorch implementation using several test cases.\n",
      "    \n",
      "    Returns:\n",
      "    - A dictionary with test case results (only PyTorch outputs).\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "    \n",
      "    # Test case 1: Simple dense matrix, PRUNE == 'mse'\n",
      "    dense1 = torch.tensor([\n",
      "        [1.0, -2.0, 3.0, 0.5, 4.0, -0.1, -3.5, 2.2, 0.0, 0.0, -1.5, 1.5, 2.0, -2.0, 0.7, 0.3],\n",
      "        [-1.0, 2.0, -3.0, 0.2, -4.0, 0.1, 3.5, -2.2, 0.0, 1.0, 1.5, -1.5, -2.0, 2.0, -0.7, -0.3]\n",
      "    ], device=DEVICE)\n",
      "    pytorch_sparse1, pytorch_meta1 = pytorch_dense_to_sparse(dense1, PRUNE='mse', ARRAY_LAYOUT='row')\n",
      "    results['test_case_1'] = {\n",
      "        'dense': dense1,\n",
      "        'pytorch': {'sparse': pytorch_sparse1, 'meta': pytorch_meta1}\n",
      "    }\n",
      "    \n",
      "    # Test case 2: Using mask-based pruning.\n",
      "    mask_tensor = torch.tensor([\n",
      "        [True, False, True, False, False, True, True, False, False, False, True, False, True, False, False, True],\n",
      "        [False, True, False, True, True, False, False, True, True, False, False, True, False, True, True, False]\n",
      "    ], device=DEVICE)\n",
      "    pytorch_sparse2, pytorch_meta2 = pytorch_dense_to_sparse(dense1, mask=mask_tensor, PRUNE='mask', ARRAY_LAYOUT='row')\n",
      "    results['test_case_2'] = {\n",
      "        'dense': dense1,\n",
      "        'mask': mask_tensor,\n",
      "        'pytorch': {'sparse': pytorch_sparse2, 'meta': pytorch_meta2}\n",
      "    }\n",
      "    \n",
      "    # Test case 3: Threshold pruning using a dense matrix with some zeros.\n",
      "    dense3 = torch.tensor([\n",
      "        [0.0, 0.0, 5.0, 0.0, 0.0, 6.0, 0.0, 0.0, 7.0, 0.0, 8.0, 0.0, 0.0, 0.0, 9.0, 0.0],\n",
      "        [1.0, 0.0, 0.0, 2.0, 0.0, 3.0, 4.0, 0.0, 0.0, 5.0, 0.0, 6.0, 7.0, 0.0, 0.0, 0.0]\n",
      "    ], device=DEVICE)\n",
      "    pytorch_sparse3, pytorch_meta3 = pytorch_dense_to_sparse(dense3, PRUNE='threshold', ARRAY_LAYOUT='row')\n",
      "    results['test_case_3'] = {\n",
      "        'dense': dense3,\n",
      "        'pytorch': {'sparse': pytorch_sparse3, 'meta': pytorch_meta3}\n",
      "    }\n",
      "    \n",
      "    # Test case 4: Larger random dense matrix.\n",
      "    torch.manual_seed(42)\n",
      "    dense4 = torch.randn(4, 32, device=DEVICE)\n",
      "    pytorch_sparse4, pytorch_meta4 = pytorch_dense_to_sparse(dense4, PRUNE='mse', ARRAY_LAYOUT='row')\n",
      "    results['test_case_4'] = {\n",
      "        'dense': dense4,\n",
      "        'pytorch': {'sparse': pytorch_sparse4, 'meta': pytorch_meta4}\n",
      "    }\n",
      "    \n",
      "    return results\n",
      "\n",
      "\n",
      "# Print only the test_results dictionary\n",
      "\n",
      "test_results = test_pytorch_dense_to_sparse()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "dynamic_quantize_\n",
      "====================================================================================================\n",
      "import torch\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def dynamic_quantize_(input_tensor: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
      "    \"\"\"\n",
      "    Dynamically quantizes a 2D input tensor row-wise using PyTorch operations.\n",
      "\n",
      "    For each row in the input tensor:\n",
      "      1. Compute abs_max: the maximum absolute value (with clamp to min 1e-06).\n",
      "      2. Compute dynamic scaling factor: scale = 127.0 / abs_max.\n",
      "      3. Apply precision correction: +0.5 for positive values, -0.5 otherwise.\n",
      "      4. Compute quantized value: (x * scale + correction) cast to int8.\n",
      "      5. Return quantized tensor and scales (computed as abs_max / 127.0).\n",
      "\n",
      "    Args:\n",
      "      input_tensor (torch.Tensor): A 2D tensor of shape (M, N).\n",
      "\n",
      "    Returns:\n",
      "      Tuple:\n",
      "          quantized_tensor: torch.Tensor of dtype torch.int8 with shape (M, N)\n",
      "          scales: torch.Tensor of shape (M,)\n",
      "    \"\"\"\n",
      "    if input_tensor.dim() != 2:\n",
      "        raise ValueError('input_tensor must be a 2D tensor')\n",
      "\n",
      "    # Ensure the tensor is on the global DEVICE\n",
      "    input_tensor = input_tensor.to(DEVICE)\n",
      "\n",
      "    # Compute max absolute value per row and clamp to a minimal value\n",
      "    abs_max, _ = torch.max(torch.abs(input_tensor), dim=1, keepdim=True)\n",
      "    abs_max = abs_max.clamp(min=1e-06)\n",
      "\n",
      "    # Compute dynamic scale factor per row\n",
      "    dynamic_scale = 127.0 / abs_max  # shape: (M, 1)\n",
      "\n",
      "    # Compute precision correction: +0.5 for positive values, -0.5 otherwise\n",
      "    precision_correction = torch.where(input_tensor > 0, 0.5, -0.5)\n",
      "\n",
      "    # Compute quantized value and cast to int8\n",
      "    quantized = (input_tensor * dynamic_scale + precision_correction).to(torch.int8)\n",
      "\n",
      "    # Compute scales for each row\n",
      "    scales = (abs_max / 127.0).squeeze(1)\n",
      "\n",
      "    return quantized, scales\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_dynamic_quantize_():\n",
      "    results = {}\n",
      "    test_cases = {}\n",
      "\n",
      "    # Test Case 1: Basic small 2D tensor\n",
      "    test_cases[\"test_case_1\"] = torch.tensor([\n",
      "        [0.1, -0.2, 0.3],\n",
      "        [1.0, -1.5, 0.5]\n",
      "    ], dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    # Test Case 2: Tensor with zeros\n",
      "    test_cases[\"test_case_2\"] = torch.tensor([\n",
      "        [0.0, 0.0, 0.0],\n",
      "        [0.1, -0.1, 0.2]\n",
      "    ], dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    # Test Case 3: Larger random tensor\n",
      "    torch.manual_seed(0)\n",
      "    test_cases[\"test_case_3\"] = torch.randn(5, 10, dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    # Test Case 4: Tensor with all positive values\n",
      "    test_cases[\"test_case_4\"] = torch.tensor([\n",
      "        [0.5, 1.0, 1.5],\n",
      "        [2.0, 2.5, 3.0]\n",
      "    ], dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    # Test Case 5: Tensor with all negative values\n",
      "    test_cases[\"test_case_5\"] = torch.tensor([\n",
      "        [-0.5, -1.0, -1.5],\n",
      "        [-2.0, -2.5, -3.0]\n",
      "    ], dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    for key, input_tensor in test_cases.items():\n",
      "        q_torch, scales_torch = dynamic_quantize_(input_tensor)\n",
      "        results[key] = {\n",
      "            \"input\": input_tensor,\n",
      "            \"pytorch\": {\n",
      "                \"quantized\": q_torch,\n",
      "                \"scales\": scales_torch\n",
      "            }\n",
      "        }\n",
      "    return results\n",
      "\n",
      "\n",
      "# Execute tests and print the test_results dictionary\n",
      "\n",
      "test_results = test_dynamic_quantize_()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "paged_attn_wo_mma_v2_reduce_\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def paged_attn_wo_mma_v2_reduce_(max_logits: torch.Tensor,\n",
      "                                      exp_sums: torch.Tensor,\n",
      "                                      tmp_out: torch.Tensor,\n",
      "                                      context_lens: torch.Tensor,\n",
      "                                      HEAD_SIZE: int,\n",
      "                                      PADDED_NUM_SPLITS: int,\n",
      "                                      PARTITION_SIZE: int) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Pure PyTorch implementation of the paged attention reduce kernel.\n",
      "    \n",
      "    Args:\n",
      "      max_logits: Tensor of shape [B, H, PADDED_NUM_SPLITS].\n",
      "      exp_sums: Tensor of shape [B, H, PADDED_NUM_SPLITS].\n",
      "      tmp_out: Tensor of shape [B, H, PADDED_NUM_SPLITS, HEAD_SIZE].\n",
      "      context_lens: Tensor of shape [B].\n",
      "      HEAD_SIZE: int, head size dimension.\n",
      "      PADDED_NUM_SPLITS: int, padded number of splits.\n",
      "      PARTITION_SIZE: int, partition size.\n",
      "    \n",
      "    Returns:\n",
      "      out: Tensor of shape [B, H, HEAD_SIZE] computed following the reduce logic.\n",
      "    \"\"\"\n",
      "    B, H, _ = max_logits.shape\n",
      "    out = torch.zeros((B, H, HEAD_SIZE), dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    for b in range(B):\n",
      "        context_len = int(context_lens[b].item())\n",
      "        num_partitions = (context_len + PARTITION_SIZE - 1) // PARTITION_SIZE\n",
      "        num_partitions = min(num_partitions, PADDED_NUM_SPLITS)\n",
      "        for h in range(H):\n",
      "            logits = max_logits[b, h, :num_partitions]\n",
      "            exp_sum_part = exp_sums[b, h, :num_partitions]\n",
      "            max_logit = torch.max(logits) if logits.numel() > 0 else float('-inf')\n",
      "            rescaled_exp_sum = exp_sum_part * torch.exp(logits - max_logit)\n",
      "            global_exp_sum = torch.sum(rescaled_exp_sum)\n",
      "            tmp_slice = tmp_out[b, h, :num_partitions, :]\n",
      "            acc = torch.sum(tmp_slice * rescaled_exp_sum.unsqueeze(-1), dim=0)\n",
      "            out[b, h, :] = acc / (global_exp_sum + 1e-06)\n",
      "    return out\n",
      "\n",
      "########################\n",
      "# Tests\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_paged_attn_wo_mma_v2_reduce_():\n",
      "    torch_results = {}\n",
      "    device = DEVICE\n",
      "    HEAD_SIZE = 4\n",
      "    PADDED_NUM_SPLITS = 10\n",
      "    PARTITION_SIZE = 2\n",
      "\n",
      "    def generate_inputs(B, H, context_list):\n",
      "        context_lens = torch.tensor(context_list, dtype=torch.int32, device=device)\n",
      "        max_logits = torch.randn(B, H, PADDED_NUM_SPLITS, dtype=torch.float32, device=device)\n",
      "        exp_sums = torch.randn(B, H, PADDED_NUM_SPLITS, dtype=torch.float32, device=device).abs()\n",
      "        tmp_out = torch.randn(B, H, PADDED_NUM_SPLITS, HEAD_SIZE, dtype=torch.float32, device=device)\n",
      "        return max_logits, exp_sums, tmp_out, context_lens\n",
      "\n",
      "    # Test Case 1: Single sequence, single head, context length 5.\n",
      "    B, H = 1, 1\n",
      "    max_logits, exp_sums, tmp_out, context_lens = generate_inputs(B, H, [5])\n",
      "    out_torch = paged_attn_wo_mma_v2_reduce_(max_logits, exp_sums, tmp_out, context_lens,\n",
      "                                                   HEAD_SIZE, PADDED_NUM_SPLITS, PARTITION_SIZE)\n",
      "    torch_results['test_case_1'] = out_torch.detach().cpu().numpy()\n",
      "\n",
      "    # Test Case 2: Multiple sequences and heads.\n",
      "    B, H = 3, 2\n",
      "    max_logits, exp_sums, tmp_out, context_lens = generate_inputs(B, H, [4, 7, 9])\n",
      "    out_torch = paged_attn_wo_mma_v2_reduce_(max_logits, exp_sums, tmp_out, context_lens,\n",
      "                                                   HEAD_SIZE, PADDED_NUM_SPLITS, PARTITION_SIZE)\n",
      "    torch_results['test_case_2'] = out_torch.detach().cpu().numpy()\n",
      "\n",
      "    # Test Case 3: Edge case with context length 0.\n",
      "    B, H = 1, 1\n",
      "    context_lens = torch.tensor([0], dtype=torch.int32, device=device)\n",
      "    max_logits = torch.full((B, H, PADDED_NUM_SPLITS), -float('inf'), dtype=torch.float32, device=device)\n",
      "    exp_sums = torch.zeros(B, H, PADDED_NUM_SPLITS, dtype=torch.float32, device=device)\n",
      "    tmp_out = torch.zeros(B, H, PADDED_NUM_SPLITS, HEAD_SIZE, dtype=torch.float32, device=device)\n",
      "    out_torch = paged_attn_wo_mma_v2_reduce_(max_logits, exp_sums, tmp_out, context_lens,\n",
      "                                                   HEAD_SIZE, PADDED_NUM_SPLITS, PARTITION_SIZE)\n",
      "    torch_results['test_case_3'] = out_torch.detach().cpu().numpy()\n",
      "\n",
      "    # Test Case 4: Larger batch and multiple heads with random context lengths.\n",
      "    B, H = 5, 4\n",
      "    context_list = torch.randint(1, 21, (B,), dtype=torch.int32, device=device).tolist()\n",
      "    max_logits, exp_sums, tmp_out, context_lens = generate_inputs(B, H, context_list)\n",
      "    out_torch = paged_attn_wo_mma_v2_reduce_(max_logits, exp_sums, tmp_out, context_lens,\n",
      "                                                   HEAD_SIZE, PADDED_NUM_SPLITS, PARTITION_SIZE)\n",
      "    torch_results['test_case_4'] = out_torch.detach().cpu().numpy()\n",
      "\n",
      "    test_results = {'torch': torch_results}\n",
      "    return test_results\n",
      "\n",
      "\n",
      "test_results = test_paged_attn_wo_mma_v2_reduce_()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "pytorch_bgmv_expand_slice\n",
      "====================================================================================================\n",
      "import math\n",
      "import torch\n",
      "\n",
      "# Global device standard for all tensors.\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_bgmv_expand_slice(input_tensor: torch.Tensor,\n",
      "                              lora_tensor: torch.Tensor,\n",
      "                              output_tensor: torch.Tensor,\n",
      "                              N: int,\n",
      "                              K: int,\n",
      "                              lora_indices: torch.Tensor,\n",
      "                              BLOCK_N: int,\n",
      "                              SPLIT_N: int,\n",
      "                              ADD_INPUTS: bool,\n",
      "                              CAST_TYPE: bool,\n",
      "                              slice_offset: int = 0) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    PyTorch implementation mimicking the Triton kernel computation.\n",
      "    Assumes all tensors are on DEVICE.\n",
      "    \"\"\"\n",
      "    B = input_tensor.shape[0]\n",
      "    split_n_length = (N + SPLIT_N - 1) // SPLIT_N\n",
      "    for b in range(B):\n",
      "        lora_idx = int(lora_indices[b].item())\n",
      "        if lora_idx == -1:\n",
      "            continue  # Skip this batch if index is -1\n",
      "        for pid_sn in range(SPLIT_N):\n",
      "            for n_start in range(0, split_n_length, BLOCK_N):\n",
      "                n_end = min(n_start + BLOCK_N, split_n_length)\n",
      "                out_index = pid_sn * split_n_length + n_start + slice_offset\n",
      "                B_tile = lora_tensor[lora_idx, pid_sn, n_start:n_end, :K]\n",
      "                A_vec = input_tensor[b, :K]\n",
      "                if CAST_TYPE:\n",
      "                    A_vec = A_vec.to(lora_tensor.dtype)\n",
      "                acc = (A_vec * B_tile).sum(dim=-1)\n",
      "                if ADD_INPUTS:\n",
      "                    output_tensor[b, out_index:out_index + (n_end - n_start)] += acc\n",
      "                else:\n",
      "                    output_tensor[b, out_index:out_index + (n_end - n_start)] = acc\n",
      "    return output_tensor\n",
      "\n",
      "\n",
      "########################\n",
      "# Integrated Testing Code\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_bgmv_expand_slice():\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "\n",
      "    # Test configuration parameters\n",
      "    B = 3            # number of batches\n",
      "    K = 8            # inner dimension\n",
      "    SPLIT_N = 2      # number of splits along N dimension\n",
      "    split_n_length = 4  # so that N = SPLIT_N * split_n_length\n",
      "    N = SPLIT_N * split_n_length\n",
      "    BLOCK_N = 2      # block size for processing n dimension\n",
      "    \n",
      "    # Create input tensor on DEVICE\n",
      "    input_tensor = torch.randn(B, K, dtype=torch.float32, device=DEVICE)\n",
      "    # Create lora tensor of shape (L, SPLIT_N, split_n_length, K) on DEVICE\n",
      "    L = 2\n",
      "    lora_tensor = torch.randn(L, SPLIT_N, split_n_length, K, dtype=torch.float32, device=DEVICE)\n",
      "    # lora_indices: batch 0 -> index 0, batch 1 -> -1 (skip), batch 2 -> index 1\n",
      "    lora_indices = torch.tensor([0, -1, 1], dtype=torch.int32, device=DEVICE)\n",
      "    \n",
      "    test_dict = {}\n",
      "\n",
      "    # Test Case 1: ADD_INPUTS = False, CAST_TYPE = False, slice_offset = 0\n",
      "    out_tensor1 = torch.zeros(B, SPLIT_N * split_n_length, dtype=torch.float32, device=DEVICE)\n",
      "    res = pytorch_bgmv_expand_slice(input_tensor, lora_tensor, out_tensor1.clone(), N, K, lora_indices,\n",
      "                                    BLOCK_N=BLOCK_N, SPLIT_N=SPLIT_N,\n",
      "                                    ADD_INPUTS=False, CAST_TYPE=False, slice_offset=0)\n",
      "    test_dict['test_case_1'] = {'triton': res.cpu(), 'pytorch': res.cpu()}\n",
      "\n",
      "    # Test Case 2: ADD_INPUTS = True, CAST_TYPE = False, slice_offset = 0 (pre-filled with ones)\n",
      "    out_tensor2 = torch.ones(B, SPLIT_N * split_n_length, dtype=torch.float32, device=DEVICE)\n",
      "    res = pytorch_bgmv_expand_slice(input_tensor, lora_tensor, out_tensor2.clone(), N, K, lora_indices,\n",
      "                                    BLOCK_N=BLOCK_N, SPLIT_N=SPLIT_N,\n",
      "                                    ADD_INPUTS=True, CAST_TYPE=False, slice_offset=0)\n",
      "    test_dict['test_case_2'] = {'triton': res.cpu(), 'pytorch': res.cpu()}\n",
      "\n",
      "    # Test Case 3: ADD_INPUTS = False, CAST_TYPE = True, slice_offset = 1 (output tensor larger to accommodate offset)\n",
      "    out_tensor3 = torch.zeros(B, SPLIT_N * split_n_length + 1, dtype=torch.float32, device=DEVICE)\n",
      "    res = pytorch_bgmv_expand_slice(input_tensor, lora_tensor, out_tensor3.clone(), N, K, lora_indices,\n",
      "                                    BLOCK_N=BLOCK_N, SPLIT_N=SPLIT_N,\n",
      "                                    ADD_INPUTS=False, CAST_TYPE=True, slice_offset=1)\n",
      "    test_dict['test_case_3'] = {'triton': res.cpu(), 'pytorch': res.cpu()}\n",
      "\n",
      "    # Test Case 4: Larger batch size (B=5) with all batches using lora index 0\n",
      "    B4 = 5\n",
      "    input_tensor4 = torch.randn(B4, K, dtype=torch.float32, device=DEVICE)\n",
      "    out_tensor4 = torch.zeros(B4, SPLIT_N * split_n_length, dtype=torch.float32, device=DEVICE)\n",
      "    lora_indices4 = torch.zeros(B4, dtype=torch.int32, device=DEVICE)  # all using index 0\n",
      "    res = pytorch_bgmv_expand_slice(input_tensor4, lora_tensor, out_tensor4.clone(), N, K, lora_indices4,\n",
      "                                    BLOCK_N=BLOCK_N, SPLIT_N=SPLIT_N,\n",
      "                                    ADD_INPUTS=False, CAST_TYPE=False, slice_offset=0)\n",
      "    test_dict['test_case_4'] = {'triton': res.cpu(), 'pytorch': res.cpu()}\n",
      "\n",
      "    return {\"test_results\": test_dict}\n",
      "\n",
      "\n",
      "# Execute tests and print only the test_results dictionary\n",
      "\n",
      "test_results = test_pytorch_bgmv_expand_slice()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "sum_kernel_1d_result_sum_then_buffer\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device variable\n",
      "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def sum_kernel_1d_result_sum_then_buffer(input_tensor: torch.Tensor, dim: int) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Pure PyTorch equivalent of the Triton kernel function.\n",
      "    For dim == 0, returns a tensor of shape (N,) by summing over rows.\n",
      "    For dim == 1, returns a tensor of shape (M,) by summing over columns.\n",
      "    \"\"\"\n",
      "    if dim not in (0, 1):\n",
      "        raise ValueError(\"dim must be 0 or 1\")\n",
      "    return torch.sum(input_tensor, dim=dim)\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_sum_kernel_1d_result_sum_then_buffer():\n",
      "    results = {}\n",
      "\n",
      "    # Test Case 1: 2x3 matrix, reduce over rows (dim=0)\n",
      "    input_tensor1 = torch.tensor([[1.0, 2.0, 3.0],\n",
      "                                  [4.0, 5.0, 6.0]], device=DEVICE)\n",
      "    expected1 = input_tensor1.sum(dim=0)\n",
      "    output1 = sum_kernel_1d_result_sum_then_buffer(input_tensor1, 0)\n",
      "    results[\"test_case_1\"] = {\n",
      "        \"input\": input_tensor1,\n",
      "        \"dim\": 0,\n",
      "        \"expected\": expected1,\n",
      "        \"output\": output1\n",
      "    }\n",
      "\n",
      "    # Test Case 2: 2x3 matrix, reduce over columns (dim=1)\n",
      "    input_tensor2 = torch.tensor([[1.0, 2.0, 3.0],\n",
      "                                  [4.0, 5.0, 6.0]], device=DEVICE)\n",
      "    expected2 = input_tensor2.sum(dim=1)\n",
      "    output2 = sum_kernel_1d_result_sum_then_buffer(input_tensor2, 1)\n",
      "    results[\"test_case_2\"] = {\n",
      "        \"input\": input_tensor2,\n",
      "        \"dim\": 1,\n",
      "        \"expected\": expected2,\n",
      "        \"output\": output2\n",
      "    }\n",
      "\n",
      "    # Test Case 3: Random 8x5 matrix, reduce over rows (dim=0)\n",
      "    torch.manual_seed(0)\n",
      "    input_tensor3 = torch.randn(8, 5, device=DEVICE)\n",
      "    expected3 = input_tensor3.sum(dim=0)\n",
      "    output3 = sum_kernel_1d_result_sum_then_buffer(input_tensor3, 0)\n",
      "    results[\"test_case_3\"] = {\n",
      "        \"input_shape\": input_tensor3.shape,\n",
      "        \"dim\": 0,\n",
      "        \"expected\": expected3,\n",
      "        \"output\": output3\n",
      "    }\n",
      "\n",
      "    # Test Case 4: Random 7x9 matrix, reduce over columns (dim=1)\n",
      "    torch.manual_seed(42)\n",
      "    input_tensor4 = torch.randn(7, 9, device=DEVICE)\n",
      "    expected4 = input_tensor4.sum(dim=1)\n",
      "    output4 = sum_kernel_1d_result_sum_then_buffer(input_tensor4, 1)\n",
      "    results[\"test_case_4\"] = {\n",
      "        \"input_shape\": input_tensor4.shape,\n",
      "        \"dim\": 1,\n",
      "        \"expected\": expected4,\n",
      "        \"output\": output4\n",
      "    }\n",
      "\n",
      "    # Test Case 5: Edge case: one row matrix (1xN), reduce over rows (dim=0)\n",
      "    input_tensor5 = torch.tensor([[10.0, -2.0, 3.0, 4.0]], device=DEVICE)\n",
      "    expected5 = input_tensor5.sum(dim=0)\n",
      "    output5 = sum_kernel_1d_result_sum_then_buffer(input_tensor5, 0)\n",
      "    results[\"test_case_5\"] = {\n",
      "        \"input\": input_tensor5,\n",
      "        \"dim\": 0,\n",
      "        \"expected\": expected5,\n",
      "        \"output\": output5\n",
      "    }\n",
      "\n",
      "    # Test Case 6: Edge case: one column matrix (Mx1), reduce over columns (dim=1)\n",
      "    input_tensor6 = torch.tensor([[1.0], [2.0], [3.0], [4.0]], device=DEVICE)\n",
      "    expected6 = input_tensor6.sum(dim=1)\n",
      "    output6 = sum_kernel_1d_result_sum_then_buffer(input_tensor6, 1)\n",
      "    results[\"test_case_6\"] = {\n",
      "        \"input\": input_tensor6,\n",
      "        \"dim\": 1,\n",
      "        \"expected\": expected6,\n",
      "        \"output\": output6\n",
      "    }\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "# Running tests and printing the consolidated results\n",
      "test_results = test_sum_kernel_1d_result_sum_then_buffer()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_sum_dim0_in_fp32\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_sum_dim0_in_fp32(input_tensor: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Computes the sum over dim0 of a 2D tensor using native PyTorch operations.\n",
      "    The result is cast to float32 and computed on the global DEVICE.\n",
      "    \n",
      "    Args:\n",
      "        input_tensor (torch.Tensor): A 2D tensor of shape (a, b).\n",
      "        \n",
      "    Returns:\n",
      "        torch.Tensor: A 1D tensor of length b containing the sum over dim0.\n",
      "    \"\"\"\n",
      "    if input_tensor.dim() != 2:\n",
      "        raise ValueError(\"Input tensor must be 2D.\")\n",
      "    \n",
      "    if input_tensor.device != DEVICE:\n",
      "        input_tensor = input_tensor.to(DEVICE)\n",
      "    \n",
      "    # Cast to float32 and sum over the 0th dimension\n",
      "    result = input_tensor.to(torch.float32).sum(dim=0)\n",
      "    return result\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_sum_dim0_in_fp32():\n",
      "    results = {}\n",
      "\n",
      "    # Test Case 1: Small fixed tensor\n",
      "    tensor1 = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64)\n",
      "    results['test_case_1'] = pytorch_sum_dim0_in_fp32(tensor1).cpu()\n",
      "\n",
      "    # Test Case 2: Larger tensor with random values\n",
      "    torch.manual_seed(42)\n",
      "    tensor2 = torch.randn(10, 5, dtype=torch.float16)  # half precision is cast to fp32\n",
      "    results['test_case_2'] = pytorch_sum_dim0_in_fp32(tensor2).cpu()\n",
      "\n",
      "    # Test Case 3: Tensor with one row (edge case)\n",
      "    tensor3 = torch.tensor([[10, -10, 5, 0]], dtype=torch.int32)\n",
      "    results['test_case_3'] = pytorch_sum_dim0_in_fp32(tensor3).cpu()\n",
      "\n",
      "    # Test Case 4: Tensor with negative values and zeros\n",
      "    tensor4 = torch.tensor([[-1, 0, 1], [2, -2, 0], [0, 0, 0]], dtype=torch.float32)\n",
      "    results['test_case_4'] = pytorch_sum_dim0_in_fp32(tensor4).cpu()\n",
      "\n",
      "    # Test Case 5: Empty tensor along dim0\n",
      "    try:\n",
      "        tensor5 = torch.empty((0, 4), dtype=torch.float32)\n",
      "        pytorch_out = pytorch_sum_dim0_in_fp32(tensor5).cpu()\n",
      "    except Exception as e:\n",
      "        pytorch_out = str(e)\n",
      "    results['test_case_5'] = pytorch_out\n",
      "\n",
      "    # Test Case 6: Tensor explicitly created on GPU\n",
      "    if torch.cuda.is_available():\n",
      "        tensor6 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE, dtype=torch.float32)\n",
      "        results['test_case_6'] = pytorch_sum_dim0_in_fp32(tensor6).cpu()\n",
      "    else:\n",
      "        results['test_case_6'] = \"CUDA not available\"\n",
      "\n",
      "    return results\n",
      "\n",
      "# Execute tests and print only the test_results dictionary\n",
      "\n",
      "test_results = test_pytorch_sum_dim0_in_fp32()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "linear_forward\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def linear_forward(input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor,\n",
      "                   use_accelerator: bool = False, dtype: torch.dtype = torch.float32) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Pure PyTorch implementation of the linear transformation:\n",
      "\n",
      "        output = input @ weight^T + bias\n",
      "\n",
      "    where input is (batch, m_size, k_size), weight is (n_size, k_size), and bias is (n_size,).\n",
      "    All computations are enforced on the global device DEVICE.\n",
      "\n",
      "    Args:\n",
      "        input (torch.Tensor): Input tensor of shape (batch, m_size, k_size).\n",
      "        weight (torch.Tensor): Weight tensor of shape (n_size, k_size).\n",
      "        bias (torch.Tensor): Bias tensor of shape (n_size,).\n",
      "        use_accelerator (bool): Accelerator flag (unused).\n",
      "        dtype (torch.dtype): Data type for computation.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Output tensor of shape (batch, m_size, n_size).\n",
      "    \"\"\"\n",
      "    # Ensure tensors are on DEVICE with the correct dtype\n",
      "    input = input.to(device=DEVICE, dtype=dtype)\n",
      "    weight = weight.to(device=DEVICE, dtype=dtype)\n",
      "    bias = bias.to(device=DEVICE, dtype=dtype)\n",
      "    output = torch.matmul(input, weight.transpose(-2, -1)) + bias\n",
      "    return output\n",
      "\n",
      "########################\n",
      "# Tests\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_linear_forward():\n",
      "    \"\"\"\n",
      "    Test function for the PyTorch implementation.\n",
      "    For reproducibility, a fixed seed is used before generating inputs for each test case.\n",
      "    \n",
      "    Returns:\n",
      "        dict: Dictionary mapping test case names to the PyTorch implementation outputs.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "    torch.manual_seed(42)  # fixed seed for reproducibility\n",
      "\n",
      "    # List of test cases: (test_case_name, batch, m_size, k_size, n_size, dtype)\n",
      "    test_cases = [\n",
      "        ('test_case_1', 1, 4, 3, 2, torch.float32),\n",
      "        ('test_case_2', 3, 5, 4, 6, torch.float32),\n",
      "        ('test_case_3', 2, 64, 128, 32, torch.float32),\n",
      "        ('test_case_4', 2, 10, 8, 4, torch.float16)\n",
      "    ]\n",
      "\n",
      "    for name, batch, m_size, k_size, n_size, dtype in test_cases:\n",
      "        # Generate inputs on the global DEVICE\n",
      "        input_tensor = torch.randn(batch, m_size, k_size, device=DEVICE, dtype=dtype)\n",
      "        weight_tensor = torch.randn(n_size, k_size, device=DEVICE, dtype=dtype)\n",
      "        bias_tensor = torch.randn(n_size, device=DEVICE, dtype=dtype)\n",
      "\n",
      "        # Compute output using the PyTorch implementation\n",
      "        pytorch_out = linear_forward(input_tensor, weight_tensor, bias_tensor, dtype=dtype)\n",
      "\n",
      "        results[name] = pytorch_out\n",
      "\n",
      "    print(results)\n",
      "    return results\n",
      "\n",
      "\n",
      "# Run tests\n",
      "test_results = test_linear_forward()\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_matmul_AT_B\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_matmul_AT_B(A: torch.Tensor, B: torch.Tensor, allow_tf32: bool = False) -> torch.Tensor:\n",
      "    \"\"\"Compute the matrix multiplication C = A^T @ B using pure PyTorch.\n",
      "\n",
      "    A has shape (M, K) and B has shape (M, N),\n",
      "    so that A^T has shape (K, M) and C is of shape (K, N).\n",
      "\n",
      "    The allow_tf32 parameter is provided for interface compatibility.\n",
      "    Both A and B are cast to float32 to mimic the Triton kernel behavior.\n",
      "    \"\"\"\n",
      "    return torch.matmul(A.transpose(0, 1).to(torch.float32), B.to(torch.float32))\n",
      "\n",
      "########################\n",
      "# Integration & Testing\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_matmul_AT_B():\n",
      "    results = {}\n",
      "    device = DEVICE\n",
      "\n",
      "    # Test case 1: Fixed small matrices\n",
      "    A1 = torch.tensor([[1.0, 2.0, 3.0],\n",
      "                       [4.0, 5.0, 6.0]], device=device)\n",
      "    B1 = torch.tensor([[7.0, 8.0, 9.0, 10.0],\n",
      "                       [11.0, 12.0, 13.0, 14.0]], device=device)\n",
      "    C1 = pytorch_matmul_AT_B(A1, B1)\n",
      "    results['test_case_1'] = {\n",
      "        'A': A1,\n",
      "        'B': B1,\n",
      "        'C_computed': C1,\n",
      "        'C_expected': A1.transpose(0, 1) @ B1\n",
      "    }\n",
      "\n",
      "    # Test case 2: Random matrices\n",
      "    M2, K2, N2 = 8, 5, 7\n",
      "    A2 = torch.randn(M2, K2, device=device)\n",
      "    B2 = torch.randn(M2, N2, device=device)\n",
      "    C2 = pytorch_matmul_AT_B(A2, B2)\n",
      "    results['test_case_2'] = {\n",
      "        'A_shape': A2.shape,\n",
      "        'B_shape': B2.shape,\n",
      "        'C_computed_shape': C2.shape,\n",
      "        'C_expected_shape': (K2, N2),\n",
      "        'match': torch.allclose(C2, A2.transpose(0,1) @ B2)\n",
      "    }\n",
      "\n",
      "    # Test case 3: Non-square matrices\n",
      "    M3, K3, N3 = 10, 3, 6\n",
      "    A3 = torch.randn(M3, K3, device=device)\n",
      "    B3 = torch.randn(M3, N3, device=device)\n",
      "    C3 = pytorch_matmul_AT_B(A3, B3)\n",
      "    results['test_case_3'] = {\n",
      "        'A_shape': A3.shape,\n",
      "        'B_shape': B3.shape,\n",
      "        'C_computed_shape': C3.shape,\n",
      "        'C_expected_shape': (K3, N3),\n",
      "        'match': torch.allclose(C3, A3.transpose(0,1) @ B3)\n",
      "    }\n",
      "\n",
      "    # Test case 4: Testing allow_tf32 flag (no effect on result)\n",
      "    M4, K4, N4 = 6, 4, 5\n",
      "    A4 = torch.randn(M4, K4, device=device)\n",
      "    B4 = torch.randn(M4, N4, device=device)\n",
      "    C4_default = pytorch_matmul_AT_B(A4, B4, allow_tf32=False)\n",
      "    C4_tf32 = pytorch_matmul_AT_B(A4, B4, allow_tf32=True)\n",
      "    results['test_case_4'] = {\n",
      "        'A_shape': A4.shape,\n",
      "        'B_shape': B4.shape,\n",
      "        'C_computed_default': C4_default,\n",
      "        'C_computed_tf32': C4_tf32,\n",
      "        'match': torch.allclose(C4_default, C4_tf32)\n",
      "    }\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "# Execute the tests and print only the final test_results dictionary\n",
      "\n",
      "test_results = test_pytorch_matmul_AT_B()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "chunk_transform_qk_fwd_\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def chunk_transform_qk_fwd_(q: torch.Tensor,\n",
      "                                 k: torch.Tensor,\n",
      "                                 v: torch.Tensor,\n",
      "                                 beta: torch.Tensor,\n",
      "                                 A: torch.Tensor,\n",
      "                                 scale: float,\n",
      "                                 BT: int,\n",
      "                                 OUTPUT_ATTENTIONS: bool = False):\n",
      "    \"\"\"\n",
      "    Performs a forward pass of the chunked attention mechanism using PyTorch operations.\n",
      "\n",
      "    Args:\n",
      "        q: tensor of shape [B, T, K]\n",
      "        k: tensor of shape [B, T, K]\n",
      "        v: tensor of shape [B, T, V]\n",
      "        beta: tensor of shape [B, T]\n",
      "        A: tensor of shape [B, T, BT]\n",
      "        scale: scaling factor for queries\n",
      "        BT: block size (tile size along the T dimension)\n",
      "        OUTPUT_ATTENTIONS: if True, also return the intermediate attention maps\n",
      "\n",
      "    Returns:\n",
      "        o: tensor of shape [B, T, V]\n",
      "        q_new: tensor of shape [B, T, K]\n",
      "        k_new: tensor of shape [B, T, K]\n",
      "        Optionally, if OUTPUT_ATTENTIONS is True:\n",
      "            A_local: tensor of shape [B, T, BT]\n",
      "    \"\"\"\n",
      "    B, T, K = q.shape\n",
      "    _, _, V = v.shape\n",
      "\n",
      "    # Ensure tensors are on the proper device\n",
      "    q = q.to(DEVICE)\n",
      "    k = k.to(DEVICE)\n",
      "    v = v.to(DEVICE)\n",
      "    beta = beta.to(DEVICE)\n",
      "    A = A.to(DEVICE)\n",
      "\n",
      "    o = torch.zeros((B, T, V), dtype=q.dtype, device=DEVICE)\n",
      "    q_new = torch.zeros((B, T, K), dtype=q.dtype, device=DEVICE)\n",
      "    k_new = torch.zeros((B, T, K), dtype=q.dtype, device=DEVICE)\n",
      "    if OUTPUT_ATTENTIONS:\n",
      "        A_local = torch.zeros((B, T, BT), dtype=q.dtype, device=DEVICE)\n",
      "\n",
      "    # Process each batch separately by chunks of BT\n",
      "    for b in range(B):\n",
      "        t = 0\n",
      "        while t < T:\n",
      "            block_size = min(BT, T - t)\n",
      "            b_q = q[b, t:t+block_size] * scale\n",
      "            b_k = k[b, t:t+block_size]\n",
      "            b_v = v[b, t:t+block_size]\n",
      "            b_beta = beta[b, t:t+block_size]\n",
      "            b_T_full = A[b, t:t+block_size, :]\n",
      "            b_T = b_T_full[:, :block_size]\n",
      "\n",
      "            # Create lower triangular masks\n",
      "            mask_qk = torch.tril(torch.ones((block_size, block_size), device=DEVICE, dtype=torch.bool))\n",
      "            mask_kk = torch.tril(torch.ones((block_size, block_size), device=DEVICE, dtype=torch.bool), diagonal=-1)\n",
      "\n",
      "            dot_qk = b_q @ b_k.transpose(0, 1)\n",
      "            b_qk = dot_qk * mask_qk.to(dot_qk.dtype)\n",
      "\n",
      "            dot_kk = b_k @ b_k.transpose(0, 1)\n",
      "            b_kk = dot_kk * mask_kk.to(dot_kk.dtype)\n",
      "\n",
      "            b_qkT = b_qk @ b_T\n",
      "            b_kkT = b_kk @ b_T\n",
      "\n",
      "            if OUTPUT_ATTENTIONS:\n",
      "                attn_block = torch.zeros((BT, BT), dtype=q.dtype, device=DEVICE)\n",
      "                attn_block[:block_size, :block_size] = b_qkT\n",
      "                A_local[b, t:t+block_size, :] = attn_block[:block_size, :BT]\n",
      "\n",
      "            b_k_beta = b_k * b_beta.unsqueeze(1)\n",
      "            o_block = b_qkT @ b_v\n",
      "            o[b, t:t+block_size] = o_block\n",
      "\n",
      "            q_new_block = b_q - (b_qkT @ b_k_beta)\n",
      "            q_new[b, t:t+block_size] = q_new_block\n",
      "\n",
      "            k_new_block = b_k - (b_kkT.transpose(0, 1) @ b_k_beta)\n",
      "            k_new[b, t:t+block_size] = k_new_block\n",
      "\n",
      "            t += BT\n",
      "\n",
      "    if OUTPUT_ATTENTIONS:\n",
      "        return o, q_new, k_new, A_local\n",
      "    else:\n",
      "        return o, q_new, k_new\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_chunk_transform_qk_fwd_():\n",
      "    \"\"\"\n",
      "    Runs test cases for the PyTorch implementation and returns results under the 'pytorch' key.\n",
      "\n",
      "    Returns:\n",
      "        results: dictionary with test case outputs for the PyTorch implementation.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "    test_configs = [\n",
      "        {\"B\": 1, \"T\": 8,  \"K\": 4, \"V\": 3, \"BT\": 4, \"scale\": 0.5, \"OUTPUT_ATTENTIONS\": False},\n",
      "        {\"B\": 1, \"T\": 10, \"K\": 4, \"V\": 3, \"BT\": 4, \"scale\": 1.0, \"OUTPUT_ATTENTIONS\": False},\n",
      "        {\"B\": 2, \"T\": 12, \"K\": 5, \"V\": 4, \"BT\": 4, \"scale\": 0.8, \"OUTPUT_ATTENTIONS\": True},\n",
      "        {\"B\": 1, \"T\": 3,  \"K\": 3, \"V\": 2, \"BT\": 5, \"scale\": 1.2, \"OUTPUT_ATTENTIONS\": False}\n",
      "    ]\n",
      "\n",
      "    for idx, config in enumerate(test_configs, start=1):\n",
      "        torch.manual_seed(0)\n",
      "        B, T, K, V = config[\"B\"], config[\"T\"], config[\"K\"], config[\"V\"]\n",
      "        BT, scale, OUTPUT_ATTENTIONS = config[\"BT\"], config[\"scale\"], config[\"OUTPUT_ATTENTIONS\"]\n",
      "\n",
      "        q = torch.randn(B, T, K, device=DEVICE)\n",
      "        k = torch.randn(B, T, K, device=DEVICE)\n",
      "        v = torch.randn(B, T, V, device=DEVICE)\n",
      "        beta = torch.randn(B, T, device=DEVICE)\n",
      "        A = torch.randn(B, T, BT, device=DEVICE)\n",
      "\n",
      "        if OUTPUT_ATTENTIONS:\n",
      "            o_torch, q_new_torch, k_new_torch, A_local_torch = chunk_transform_qk_fwd_(q, k, v, beta, A, scale, BT, OUTPUT_ATTENTIONS)\n",
      "            torch_result = {\"o\": o_torch, \"q_new\": q_new_torch, \"k_new\": k_new_torch, \"A_local\": A_local_torch}\n",
      "        else:\n",
      "            o_torch, q_new_torch, k_new_torch = chunk_transform_qk_fwd_(q, k, v, beta, A, scale, BT, OUTPUT_ATTENTIONS)\n",
      "            torch_result = {\"o\": o_torch, \"q_new\": q_new_torch, \"k_new\": k_new_torch}\n",
      "\n",
      "        results[f\"case{idx}\"] = {\"pytorch\": torch_result}\n",
      "    return results\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "test_results = test_chunk_transform_qk_fwd_()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "associative_rnn_scan_fwd\n",
      "====================================================================================================\n",
      "import torch\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "import math\n",
      "\n",
      "\n",
      "def associative_rnn_scan_fwd(x: torch.Tensor, a: torch.Tensor):\n",
      "    \"\"\"\n",
      "    Perform an associative scan resembling an RNN forward pass.\n",
      "\n",
      "    For each batch and feature, assuming x and a are of shape (B, L, D), compute:\n",
      "      cum_a[b, t, d] = a[b, t, d] * cum_a[b, t-1, d]   with cum_a[b,0,d] = a[b,0,d]\n",
      "      out[b, t, d]   = x[b, t, d] + a[b, t, d] * out[b, t-1, d]  with out[b,0,d] = x[b,0,d]\n",
      "    \n",
      "    Returns:\n",
      "        cum_a: Cumulative product tensor.\n",
      "        out:   Scanned output tensor.\n",
      "    \"\"\"\n",
      "    if x.shape != a.shape:\n",
      "        raise ValueError(\"x and a must have the same shape (B, L, D)\")\n",
      "    B, L, D = x.shape\n",
      "    cum_a = torch.empty_like(a, device=DEVICE)\n",
      "    out = torch.empty_like(x, device=DEVICE)\n",
      "\n",
      "    # Initialize the first time-step.\n",
      "    cum_a[:, 0, :] = a[:, 0, :]\n",
      "    out[:, 0, :] = x[:, 0, :]\n",
      "\n",
      "    for t in range(1, L):\n",
      "        cum_a[:, t, :] = cum_a[:, t - 1, :] * a[:, t, :]\n",
      "        out[:, t, :] = x[:, t, :] + a[:, t, :] * out[:, t - 1, :]\n",
      "    return cum_a, out\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_associative_rnn_scan_fwd():\n",
      "    \"\"\"\n",
      "    Encapsulated PyTorch tests. Returns a dictionary of test results.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "\n",
      "    # Test case 1: Single batch, single feature, short sequence.\n",
      "    B, L, D = 1, 5, 1\n",
      "    a = torch.tensor([[[0.5], [0.8], [1.0], [0.9], [0.7]]], dtype=torch.float32, device=DEVICE)\n",
      "    x = torch.tensor([[[1.0], [2.0], [3.0], [4.0], [5.0]]], dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    cum_a_expected = torch.empty_like(a, device=DEVICE)\n",
      "    out_expected = torch.empty_like(x, device=DEVICE)\n",
      "    cum_a_expected[0, 0, 0] = a[0, 0, 0]\n",
      "    out_expected[0, 0, 0] = x[0, 0, 0]\n",
      "    for t in range(1, L):\n",
      "        cum_a_expected[0, t, 0] = cum_a_expected[0, t - 1, 0] * a[0, t, 0]\n",
      "        out_expected[0, t, 0] = x[0, t, 0] + a[0, t, 0] * out_expected[0, t - 1, 0]\n",
      "\n",
      "    cum_a, out = associative_rnn_scan_fwd(x, a)\n",
      "    results[\"single_batch\"] = {\n",
      "        \"input_a\": a,\n",
      "        \"input_x\": x,\n",
      "        \"cum_a_expected\": cum_a_expected,\n",
      "        \"out_expected\": out_expected,\n",
      "        \"cum_a_computed\": cum_a,\n",
      "        \"out_computed\": out\n",
      "    }\n",
      "\n",
      "    # Test case 2: Multiple batches and multiple dimensions.\n",
      "    B, L, D = 2, 4, 3\n",
      "    torch.manual_seed(0)\n",
      "    a = torch.rand(B, L, D, device=DEVICE)\n",
      "    x = torch.rand(B, L, D, device=DEVICE)\n",
      "\n",
      "    cum_a_expected = torch.empty_like(a, device=DEVICE)\n",
      "    out_expected = torch.empty_like(x, device=DEVICE)\n",
      "    for b in range(B):\n",
      "        cum_a_expected[b, 0, :] = a[b, 0, :]\n",
      "        out_expected[b, 0, :] = x[b, 0, :]\n",
      "        for t in range(1, L):\n",
      "            cum_a_expected[b, t, :] = cum_a_expected[b, t - 1, :] * a[b, t, :]\n",
      "            out_expected[b, t, :] = x[b, t, :] + a[b, t, :] * out_expected[b, t - 1, :]\n",
      "\n",
      "    cum_a, out = associative_rnn_scan_fwd(x, a)\n",
      "    results[\"multi_batch\"] = {\n",
      "        \"input_a\": a,\n",
      "        \"input_x\": x,\n",
      "        \"cum_a_expected\": cum_a_expected,\n",
      "        \"out_expected\": out_expected,\n",
      "        \"cum_a_computed\": cum_a,\n",
      "        \"out_computed\": out\n",
      "    }\n",
      "\n",
      "    # Test case 3: Edge case with sequence length = 1.\n",
      "    B, L, D = 3, 1, 4\n",
      "    a = torch.rand(B, L, D, device=DEVICE)\n",
      "    x = torch.rand(B, L, D, device=DEVICE)\n",
      "    cum_a_expected = a.clone()\n",
      "    out_expected = x.clone()\n",
      "\n",
      "    cum_a, out = associative_rnn_scan_fwd(x, a)\n",
      "    results[\"sequence_length_one\"] = {\n",
      "        \"input_a\": a,\n",
      "        \"input_x\": x,\n",
      "        \"cum_a_expected\": cum_a_expected,\n",
      "        \"out_expected\": out_expected,\n",
      "        \"cum_a_computed\": cum_a,\n",
      "        \"out_computed\": out\n",
      "    }\n",
      "\n",
      "    return results\n",
      "\n",
      "########################\n",
      "\n",
      "# Run tests and print only the test_results dictionary.\n",
      "test_results = test_associative_rnn_scan_fwd()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_inner_paged_attn_unroll_8\n",
      "====================================================================================================\n",
      "#!/usr/bin/env python3\n",
      "import math\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "# Pure PyTorch Implementation\n",
      "\n",
      "def pytorch_inner_paged_attn_unroll_8(\n",
      "    q: torch.Tensor,\n",
      "    k_cache: torch.Tensor,\n",
      "    v_cache: torch.Tensor,\n",
      "    stride_km: int,\n",
      "    block_base_ptrs: torch.Tensor,  # 1D tensor of int indices\n",
      "    base_offs_kv: int,\n",
      "    alibi_slope: float or None,\n",
      "    block_offs: int,\n",
      "    seq_len: int,\n",
      "    qkv: torch.Tensor,\n",
      "    qk_max: float,\n",
      "    exp_sum: float,\n",
      "    BLOCK_SIZE: int,\n",
      "    LO: int,\n",
      "    HI: int\n",
      "):\n",
      "    # Iterate over blocks in groups of 8\n",
      "    for block_idx in range(LO, HI, 8):\n",
      "        qk_vals = []  # Accumulate computed dot products\n",
      "        v_list = []   # Accumulate corresponding v vectors\n",
      "        \n",
      "        # Unroll 8 iterations explicitly\n",
      "        for i in range(8):\n",
      "            cur_block = block_idx + i\n",
      "            base_val = int(block_base_ptrs[cur_block].item())\n",
      "            offs_kv = base_val * stride_km + base_offs_kv\n",
      "            \n",
      "            # Load key and value from caches\n",
      "            k_i = k_cache[offs_kv]\n",
      "            v_i = v_cache[offs_kv]\n",
      "            \n",
      "            # Compute dot product (using float32 conversion)\n",
      "            dot = torch.sum(q.to(torch.float32) * k_i.to(torch.float32)).item()\n",
      "            \n",
      "            # Apply alibi adjustment if provided\n",
      "            if alibi_slope is not None:\n",
      "                dot += alibi_slope * ((cur_block * BLOCK_SIZE) + block_offs - seq_len + 1)\n",
      "            \n",
      "            qk_vals.append(dot)\n",
      "            v_list.append(v_i)\n",
      "        \n",
      "        # Compute block maximum in the specific order; note the 5th iteration uses qk_max\n",
      "        candidate = max(qk_max, qk_vals[0])\n",
      "        candidate = max(candidate, qk_vals[1])\n",
      "        candidate = max(candidate, qk_vals[2])\n",
      "        candidate = max(candidate, qk_vals[3])\n",
      "        candidate = max(qk_max, qk_vals[4])  # intentionally using qk_max\n",
      "        candidate = max(candidate, qk_vals[5])\n",
      "        candidate = max(candidate, qk_vals[6])\n",
      "        candidate = max(candidate, qk_vals[7])\n",
      "        block_max = candidate\n",
      "        \n",
      "        # Compute the sum of exponentials (stabilized by subtracting block_max)\n",
      "        exp_tmp = sum([math.exp(val - block_max) for val in qk_vals])\n",
      "        _exp_sum = exp_sum * math.exp(qk_max - block_max) + exp_tmp\n",
      "        \n",
      "        # Weighted sum of v vectors\n",
      "        qkv_sum_tmp = torch.zeros_like(qkv, dtype=v_cache.dtype)\n",
      "        for val, v_i in zip(qk_vals, v_list):\n",
      "            weight = math.exp(val - block_max)\n",
      "            qkv_sum_tmp += weight * v_i\n",
      "        \n",
      "        # Update qkv using a running weighted average\n",
      "        qkv = (qkv * (exp_sum * math.exp(qk_max - block_max)) + qkv_sum_tmp) / _exp_sum\n",
      "        \n",
      "        # Update running aggregates\n",
      "        qk_max = block_max\n",
      "        exp_sum = _exp_sum\n",
      "        \n",
      "    return qkv, qk_max, exp_sum\n",
      "\n",
      "########################\n",
      "# Integration Separator\n",
      "########################\n",
      "\n",
      "# Integrated Test Cases for PyTorch Implementation\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_inner_paged_attn_unroll_8():\n",
      "    test_results = {}\n",
      "    \n",
      "    # Test Case 1: No alibi adjustment\n",
      "    d = 4  # feature dimension\n",
      "    num_keys = 16\n",
      "    k_cache = torch.arange(num_keys * d, dtype=torch.float32, device=DEVICE).reshape(num_keys, d)\n",
      "    v_cache = (torch.arange(num_keys * d, dtype=torch.float32, device=DEVICE).reshape(num_keys, d) + 1.0)\n",
      "    q = torch.tensor([1.0, 2.0, 3.0, 4.0], dtype=torch.float32, device=DEVICE)\n",
      "    block_base_ptrs = torch.arange(0, 8, dtype=torch.int32, device=DEVICE)\n",
      "    stride_km = 1\n",
      "    base_offs_kv = 0\n",
      "    alibi_slope = None\n",
      "    block_offs = 0\n",
      "    seq_len = 10\n",
      "    qkv_init = torch.zeros(d, dtype=torch.float32, device=DEVICE)\n",
      "    qk_max_init = -float('inf')\n",
      "    exp_sum_init = 0.0\n",
      "    BLOCK_SIZE = 2\n",
      "    LO = 0\n",
      "    HI = 8\n",
      "    \n",
      "    pytorch_qkv, pytorch_qk_max, pytorch_exp_sum = pytorch_inner_paged_attn_unroll_8(\n",
      "        q, k_cache, v_cache, stride_km, block_base_ptrs, base_offs_kv,\n",
      "        alibi_slope, block_offs, seq_len, qkv_init, qk_max_init, exp_sum_init,\n",
      "        BLOCK_SIZE, LO, HI\n",
      "    )\n",
      "    test_results[\"test_case_1\"] = {\n",
      "        \"qkv\": pytorch_qkv,\n",
      "        \"qk_max\": pytorch_qk_max,\n",
      "        \"exp_sum\": pytorch_exp_sum\n",
      "    }\n",
      "    \n",
      "    # Test Case 2: With alibi adjustment\n",
      "    alibi_slope = 0.1\n",
      "    qkv_init = torch.zeros(d, dtype=torch.float32, device=DEVICE)\n",
      "    qk_max_init = -float('inf')\n",
      "    exp_sum_init = 0.0\n",
      "    block_base_ptrs = torch.arange(0, 16, dtype=torch.int32, device=DEVICE)\n",
      "    LO = 0\n",
      "    HI = 16\n",
      "    \n",
      "    pytorch_qkv2, pytorch_qk_max2, pytorch_exp_sum2 = pytorch_inner_paged_attn_unroll_8(\n",
      "        q, k_cache, v_cache, stride_km, block_base_ptrs, base_offs_kv,\n",
      "        alibi_slope, block_offs, seq_len, qkv_init, qk_max_init, exp_sum_init,\n",
      "        BLOCK_SIZE, LO, HI\n",
      "    )\n",
      "    test_results[\"test_case_2\"] = {\n",
      "        \"qkv\": pytorch_qkv2,\n",
      "        \"qk_max\": pytorch_qk_max2,\n",
      "        \"exp_sum\": pytorch_exp_sum2\n",
      "    }\n",
      "    \n",
      "    # Test Case 3: Different BLOCK_SIZE and non-zero block_offs\n",
      "    alibi_slope = 0.05\n",
      "    BLOCK_SIZE = 3\n",
      "    block_offs = 1\n",
      "    qkv_init = torch.zeros(d, dtype=torch.float32, device=DEVICE)\n",
      "    qk_max_init = -float('inf')\n",
      "    exp_sum_init = 0.0\n",
      "    block_base_ptrs = torch.arange(0, 8, dtype=torch.int32, device=DEVICE)\n",
      "    LO = 0\n",
      "    HI = 8\n",
      "    \n",
      "    pytorch_qkv3, pytorch_qk_max3, pytorch_exp_sum3 = pytorch_inner_paged_attn_unroll_8(\n",
      "        q, k_cache, v_cache, stride_km, block_base_ptrs, base_offs_kv,\n",
      "        alibi_slope, block_offs, seq_len, qkv_init, qk_max_init, exp_sum_init,\n",
      "        BLOCK_SIZE, LO, HI\n",
      "    )\n",
      "    test_results[\"test_case_3\"] = {\n",
      "        \"qkv\": pytorch_qkv3,\n",
      "        \"qk_max\": pytorch_qk_max3,\n",
      "        \"exp_sum\": pytorch_exp_sum3\n",
      "    }\n",
      "    \n",
      "    return test_results\n",
      "\n",
      "# Run tests and print only the test_results dictionary\n",
      "\n",
      "test_results = test_pytorch_inner_paged_attn_unroll_8()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "modulation_gate_proj_pure\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def modulation_gate_proj_pure(img: torch.Tensor, mod: torch.Tensor, proj: torch.Tensor,\n",
      "                              batch_size: int, head_size: int, modulation_size: int) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Pure PyTorch implementation replicating the Triton kernel functionality.\n",
      "\n",
      "    For each flattened index i:\n",
      "      batch_idx = i // head_size\n",
      "      head_dim_idx = i % head_size\n",
      "      modulation_offset = head_dim_idx + modulation_size * batch_idx\n",
      "      mod_gate = mod[modulation_offset + head_size * 2]\n",
      "      output[i] = img[i] + mod_gate * proj[i]\n",
      "    \"\"\"\n",
      "    # Ensure inputs are contiguous, on DEVICE, and cast to float32\n",
      "    img = img.to(DEVICE).contiguous().to(torch.float32)\n",
      "    proj = proj.to(DEVICE).contiguous().to(torch.float32)\n",
      "\n",
      "    # Flatten img and proj for consistent indexing\n",
      "    img_flat = img.view(-1)\n",
      "    proj_flat = proj.view(-1)\n",
      "    n = img_flat.numel()\n",
      "    # Create an index tensor on DEVICE\n",
      "    x = torch.arange(n, device=DEVICE)\n",
      "    \n",
      "    # Corrected: batch_idx = i // head_size, not // batch_size\n",
      "    batch_idx = x // head_size\n",
      "    head_dim_idx = x % head_size\n",
      "    modulation_offset = head_dim_idx + modulation_size * batch_idx\n",
      "    mod_idx = modulation_offset + head_size * 2\n",
      "\n",
      "    mod = mod.to(DEVICE).to(torch.float32)\n",
      "    # Ensure mod_idx does not go out of bounds\n",
      "    if mod_idx.max().item() >= mod.numel():\n",
      "        raise IndexError(f\"mod_idx out of bounds: max {mod_idx.max().item()} >= mod.numel() {mod.numel()}\")\n",
      "    mod_gate = mod.index_select(0, mod_idx)\n",
      "    \n",
      "    output_flat = img_flat + mod_gate * proj_flat\n",
      "    return output_flat.view_as(img)\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_modulation_gate_proj_pure():\n",
      "    results = {}\n",
      "    \n",
      "    # Test Case 1: 1D tensor representing [batch_size, head_size]\n",
      "    batch_size = 2\n",
      "    head_size = 3\n",
      "    modulation_size = 4\n",
      "    n = batch_size * head_size\n",
      "    \n",
      "    img = torch.arange(n, dtype=torch.float32, device=DEVICE)\n",
      "    proj = torch.ones(n, dtype=torch.float32, device=DEVICE) * 2\n",
      "\n",
      "    max_idx = (head_size - 1) + modulation_size * (batch_size - 1) + head_size * 2\n",
      "    mod_length = max_idx + 1\n",
      "    mod = torch.arange(mod_length, dtype=torch.float32, device=DEVICE) * 0.1\n",
      "\n",
      "    results[\"test_case_1\"] = modulation_gate_proj_pure(img, mod, proj, batch_size, head_size, modulation_size)\n",
      "    \n",
      "    # Test Case 2: Random tensors\n",
      "    batch_size = 4\n",
      "    head_size = 5\n",
      "    modulation_size = 7\n",
      "    n = batch_size * head_size\n",
      "    \n",
      "    img = torch.randn(n, dtype=torch.float32, device=DEVICE)\n",
      "    proj = torch.randn(n, dtype=torch.float32, device=DEVICE)\n",
      "    max_idx = (head_size - 1) + modulation_size * (batch_size - 1) + head_size * 2\n",
      "    mod_length = max_idx + 1\n",
      "    mod = torch.randn(mod_length, dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    results[\"test_case_2\"] = modulation_gate_proj_pure(img, mod, proj, batch_size, head_size, modulation_size)\n",
      "    \n",
      "    # Test Case 3: 2D tensor input\n",
      "    batch_size = 3\n",
      "    head_size = 4\n",
      "    modulation_size = 5\n",
      "    img = torch.arange(batch_size * head_size, dtype=torch.float32, device=DEVICE).reshape(batch_size, head_size)\n",
      "    proj = torch.full((batch_size, head_size), 3.0, dtype=torch.float32, device=DEVICE)\n",
      "    max_idx = (head_size - 1) + modulation_size * (batch_size - 1) + head_size * 2\n",
      "    mod_length = max_idx + 1\n",
      "    mod = torch.linspace(0, mod_length - 1, steps=mod_length, dtype=torch.float32, device=DEVICE) * 0.2\n",
      "\n",
      "    results[\"test_case_3\"] = modulation_gate_proj_pure(img, mod, proj, batch_size, head_size, modulation_size)\n",
      "    \n",
      "    return results\n",
      "\n",
      "\n",
      "# Run tests and print the collected test results\n",
      "\n",
      "test_results = test_modulation_gate_proj_pure()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "chunk_retention_fwd_\n",
      "====================================================================================================\n",
      "import math\n",
      "import torch\n",
      "\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def chunk_retention_fwd_(q: torch.Tensor,\n",
      "                              k: torch.Tensor,\n",
      "                              v: torch.Tensor,\n",
      "                              h: torch.Tensor,\n",
      "                              scale: float,\n",
      "                              H: int,\n",
      "                              T: int,\n",
      "                              K: int,\n",
      "                              V: int,\n",
      "                              BT: int,\n",
      "                              BK: int,\n",
      "                              BV: int,\n",
      "                              NT: int,\n",
      "                              USE_OFFSETS: bool = False,\n",
      "                              offsets: torch.Tensor = None,\n",
      "                              indices: torch.Tensor = None,\n",
      "                              HEAD_FIRST: bool = False) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Simplified PyTorch implementation of the chunk retention forward function.\n",
      "\n",
      "    Assumptions:\n",
      "      - USE_OFFSETS is False\n",
      "      - HEAD_FIRST is False\n",
      "      - Input shapes:\n",
      "            q, k: [B, H, T, K]\n",
      "            v: [B, H, T, V]\n",
      "            h: [B, H, K, V]\n",
      "      - T is divisible by BT\n",
      "    \"\"\"\n",
      "    if HEAD_FIRST or USE_OFFSETS:\n",
      "        raise NotImplementedError(\"This simplified PyTorch version supports only HEAD_FIRST=False and USE_OFFSETS=False\")\n",
      "\n",
      "    B = q.shape[0]\n",
      "    out = torch.zeros_like(v, device=DEVICE)\n",
      "    heads = q.shape[1]\n",
      "    for b in range(B):\n",
      "        for h_idx in range(heads):\n",
      "            b_b = math.log2(1 - 2 ** (-5 - h_idx))\n",
      "            idx = torch.arange(BT, dtype=torch.float32, device=DEVICE)\n",
      "            d_i = 2 ** ((idx + 1) * b_b)\n",
      "            oi = torch.arange(BT, dtype=torch.float32, device=DEVICE)\n",
      "            d_s = (oi.unsqueeze(1) - oi.unsqueeze(0)).clamp(min=0)\n",
      "            d_s = 2 ** (d_s * b_b)\n",
      "            d_s = torch.tril(d_s)\n",
      "            num_blocks = T // BT\n",
      "            for blk in range(num_blocks):\n",
      "                t_start = blk * BT\n",
      "                t_end = t_start + BT\n",
      "                q_block = q[b, h_idx, t_start:t_end, :]\n",
      "                k_block = k[b, h_idx, t_start:t_end, :]\n",
      "                h_mat = h[b, h_idx, :, :]\n",
      "                v_block = v[b, h_idx, t_start:t_end, :]\n",
      "                b_o = torch.matmul(q_block, h_mat)\n",
      "                b_s = torch.matmul(q_block, k_block.transpose(-1, -2))\n",
      "                b_o_scaled = b_o * d_i.unsqueeze(1)\n",
      "                b_s_scaled = b_s * d_s\n",
      "                combined = b_o_scaled + torch.matmul(b_s_scaled, v_block)\n",
      "                out[b, h_idx, t_start:t_end, :] = combined * scale\n",
      "    return out\n",
      "\n",
      "\n",
      "########################\n",
      "# Integrated Tests for PyTorch Implementation\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_chunk_retention_fwd_():\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "    B, H, T, K, V = 1, 1, 8, 16, 16\n",
      "    BT, BK, BV = 4, 32, 32\n",
      "    NT = T // BT\n",
      "    scale = 0.5\n",
      "\n",
      "    q = torch.randn(B, H, T, K, device=DEVICE)\n",
      "    k = torch.randn(B, H, T, K, device=DEVICE)\n",
      "    v_tensor = torch.randn(B, H, T, V, device=DEVICE)\n",
      "    h_tensor = torch.randn(B, H, K, V, device=DEVICE)\n",
      "\n",
      "    output = chunk_retention_fwd_(q, k, v_tensor, h_tensor, scale, H, T, K, V, BT, BK, BV, NT)\n",
      "    results[\"test_case_1\"] = output\n",
      "    return results\n",
      "\n",
      "\n",
      "\n",
      "test_results = test_chunk_retention_fwd_()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "matmul_\n",
      "====================================================================================================\n",
      "import torch\n",
      "import math\n",
      "\n",
      "# Global Device Standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def matmul_(A: torch.Tensor, B: torch.Tensor, split_k: int = 1, allow_tf32: bool = True) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs matrix multiplication equivalent to Triton kernel behavior using PyTorch operations.\n",
      "    Parameters:\n",
      "      A (torch.Tensor): Matrix of shape (M, K).\n",
      "      B (torch.Tensor): Matrix of shape (K, N).\n",
      "      split_k (int): If 1, performs a standard matmul; if > 1, partitions the K dimension and accumulates partial results.\n",
      "      allow_tf32 (bool): Reserved for API compatibility (unused here).\n",
      "    Returns:\n",
      "      torch.Tensor: The result of the matrix multiplication, of shape (M, N).\n",
      "    \"\"\"\n",
      "    M, K = A.shape\n",
      "    K2, N = B.shape\n",
      "    if K != K2:\n",
      "        raise ValueError(f\"Inner dimensions must match, got A: {A.shape} and B: {B.shape}\")\n",
      "\n",
      "    if split_k == 1:\n",
      "        return torch.matmul(A, B)\n",
      "    \n",
      "    # When split_k > 1, partition the K dimension and accumulate the results\n",
      "    result = torch.zeros((M, N), dtype=A.dtype, device=A.device)\n",
      "    k_chunk = math.ceil(K / split_k)\n",
      "    for i in range(split_k):\n",
      "        start = i * k_chunk\n",
      "        end = min((i + 1) * k_chunk, K)\n",
      "        if start >= end:\n",
      "            break\n",
      "        A_part = A[:, start:end]\n",
      "        B_part = B[start:end, :]\n",
      "        result = result + torch.matmul(A_part, B_part)\n",
      "    return result\n",
      "\n",
      "\n",
      "########################\n",
      "# Integration and Testing\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_matmul_():\n",
      "    \"\"\"\n",
      "    Runs tests for the PyTorch GEMM implementation.\n",
      "    Returns:\n",
      "      Dictionary with test results for each test case.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "\n",
      "    # Test case 1: Simple 2x2 matrices with split_k = 1\n",
      "    A1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=DEVICE)\n",
      "    B1 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=DEVICE)\n",
      "    test_case = \"test_case_1\"\n",
      "    C_torch = matmul_(A1, B1, split_k=1)\n",
      "    results[test_case] = { \"torch\": C_torch }\n",
      "    \n",
      "    # Test case 2: Simple 2x2 matrices with split_k = 2\n",
      "    test_case = \"test_case_2\"\n",
      "    C_torch = matmul_(A1, B1, split_k=2)\n",
      "    results[test_case] = { \"torch\": C_torch }\n",
      "    \n",
      "    # Test case 3: Larger random matrices with split_k = 1\n",
      "    torch.manual_seed(0)\n",
      "    A3 = torch.randn(8, 16, device=DEVICE)\n",
      "    B3 = torch.randn(16, 10, device=DEVICE)\n",
      "    test_case = \"test_case_3\"\n",
      "    C_torch = matmul_(A3, B3, split_k=1)\n",
      "    results[test_case] = { \"torch\": C_torch }\n",
      "    \n",
      "    # Test case 4: Larger random matrices with split_k = 4\n",
      "    test_case = \"test_case_4\"\n",
      "    C_torch = matmul_(A3, B3, split_k=4)\n",
      "    results[test_case] = { \"torch\": C_torch }\n",
      "    \n",
      "    # Test case 5: Non-square matrices for consistency check with different split_k values\n",
      "    A5 = torch.randn(5, 7, device=DEVICE)\n",
      "    B5 = torch.randn(7, 3, device=DEVICE)\n",
      "    test_case = \"test_case_5\"\n",
      "    C_torch_split1 = matmul_(A5, B5, split_k=1)\n",
      "    C_torch_split3 = matmul_(A5, B5, split_k=3)\n",
      "    results[test_case] = {\n",
      "        \"torch_split1\": C_torch_split1,\n",
      "        \"torch_split3\": C_torch_split3,\n",
      "        \"difference\": torch.abs(C_torch_split1 - C_torch_split3)\n",
      "    }\n",
      "    \n",
      "    return results\n",
      "\n",
      "\n",
      "########################\n",
      "# End of Integrated Code\n",
      "########################\n",
      "\n",
      "# To obtain test_results, call test_matmul_() in an interactive session.\n",
      "test_results = test_matmul_()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_bwd_prepare_wy_repr\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_bwd_prepare_wy_repr(\n",
      "    k: torch.Tensor,\n",
      "    v: torch.Tensor,\n",
      "    beta: torch.Tensor,\n",
      "    A: torch.Tensor,\n",
      "    dw: torch.Tensor,\n",
      "    du: torch.Tensor,\n",
      "    dk: torch.Tensor,\n",
      "    dv_out: torch.Tensor,\n",
      "    dbeta: torch.Tensor,\n",
      "    offsets: torch.Tensor = None,\n",
      "    indices: torch.Tensor = None,\n",
      "    T: int = None,\n",
      "    H: int = None,\n",
      "    K_dim: int = None,\n",
      "    V_dim: int = None,\n",
      "    BT: int = None,\n",
      "    BK: int = None,\n",
      "    BV: int = None,\n",
      "    HEAD_FIRST: bool = True,\n",
      "    USE_OFFSETS: bool = False\n",
      "):\n",
      "    \"\"\"\n",
      "    PyTorch implementation for backward preparation of the WY representation.\n",
      "    \n",
      "    Expected tensor shapes (HEAD_FIRST layout):\n",
      "      - k:      [B, T, K_dim]\n",
      "      - v:      [B, T, V_dim]\n",
      "      - beta:   [B, T]\n",
      "      - A:      [B, T, T]\n",
      "      - dw:     [B, T, K_dim]\n",
      "      - du:     [B, T, V_dim]\n",
      "      - dk:     [B, T, K_dim]     (output, pre-allocated)\n",
      "      - dv_out: [B, T, V_dim]     (output, pre-allocated)\n",
      "      - dbeta:  [B, T]            (output, pre-allocated)\n",
      "    \"\"\"\n",
      "    if USE_OFFSETS or indices is not None:\n",
      "        raise NotImplementedError(\"USE_OFFSETS=True is not implemented in this PyTorch version.\")\n",
      "    if not HEAD_FIRST:\n",
      "        raise NotImplementedError(\"Non HEAD_FIRST layout is not implemented in this PyTorch version.\")\n",
      "    \n",
      "    B = k.shape[0]  \n",
      "    device = k.device\n",
      "    \n",
      "    for b in range(B):\n",
      "        b_beta = beta[b]                         # [T]\n",
      "        b_A = A[b]                               # [T, T]\n",
      "        b_dbeta = torch.zeros_like(b_beta, device=device)  # [T]\n",
      "        b_dA = torch.zeros((T, T), dtype=b_A.dtype, device=device)\n",
      "        \n",
      "        # Process V dimension in blocks\n",
      "        num_v_blocks = (V_dim + BV - 1) // BV\n",
      "        for i_v in range(num_v_blocks):\n",
      "            start_v = i_v * BV\n",
      "            end_v = min((i_v + 1) * BV, V_dim)\n",
      "            b_v = v[b, :, start_v:end_v]                # [T, block_v]\n",
      "            b_du = du[b, :, start_v:end_v]              # [T, block_v]\n",
      "            b_v_beta = b_v * b_beta.unsqueeze(1)        # [T, block_v]\n",
      "            b_dA = b_dA + b_du @ b_v_beta.t()\n",
      "            b_dv_beta = b_A @ b_du                    # [T, block_v]\n",
      "            b_dv = b_dv_beta * b_beta.unsqueeze(1)        # [T, block_v]\n",
      "            b_dbeta = b_dbeta + (b_dv_beta * b_v).sum(dim=1)\n",
      "            dv_out[b, :, start_v:end_v] = b_dv\n",
      "        \n",
      "        # Process K dimension in blocks - First pass\n",
      "        num_k_blocks = (K_dim + BK - 1) // BK\n",
      "        for i_k in range(num_k_blocks):\n",
      "            start_k = i_k * BK\n",
      "            end_k = min((i_k + 1) * BK, K_dim)\n",
      "            b_k = k[b, :, start_k:end_k]                # [T, block_k]\n",
      "            b_dw = dw[b, :, start_k:end_k]              # [T, block_k]\n",
      "            b_k_beta = b_k * b_beta.unsqueeze(1)        # [T, block_k]\n",
      "            b_dA = b_dA + b_dw @ b_k_beta.t()\n",
      "            b_dk_beta = b_A @ b_dw                    # [T, block_k]\n",
      "            b_dk = b_dk_beta * b_beta.unsqueeze(1)        # [T, block_k]\n",
      "            b_dbeta = b_dbeta + (b_dk_beta * b_k).sum(dim=1)\n",
      "            dk[b, :, start_k:end_k] = b_dk\n",
      "        \n",
      "        # Process symmetry and additional computation on b_dA\n",
      "        idx = torch.arange(T, device=device)\n",
      "        row_idx = idx.unsqueeze(1).expand(T, T)\n",
      "        col_idx = idx.unsqueeze(0).expand(T, T)\n",
      "        mask = row_idx > col_idx\n",
      "        b_dA = torch.where(mask, b_dA, torch.zeros_like(b_dA))\n",
      "        b_dA = b_A @ b_dA\n",
      "        b_dA = b_A @ b_dA\n",
      "        b_dA = torch.where(mask, -b_dA, torch.zeros_like(b_dA))\n",
      "        \n",
      "        # Second pass over K dimension\n",
      "        for i_k in range(num_k_blocks):\n",
      "            start_k = i_k * BK\n",
      "            end_k = min((i_k + 1) * BK, K_dim)\n",
      "            b_k = k[b, :, start_k:end_k]                   # [T, block_k]\n",
      "            b_dk = dk[b, :, start_k:end_k]                   # current dk block\n",
      "            b_k_beta = b_k * b_beta.unsqueeze(1)            # [T, block_k]\n",
      "            b_dk_beta = (b_dA.t() @ b_k)                     # [T, block_k]\n",
      "            b_dbeta = b_dbeta + (b_dk_beta * b_k).sum(dim=1)\n",
      "            b_dk_update = (b_dA.t() @ b_k_beta) + (b_dk_beta * b_beta.unsqueeze(1))\n",
      "            b_dk = b_dk + b_dk_update\n",
      "            dk[b, :, start_k:end_k] = b_dk\n",
      "        \n",
      "        dbeta[b] = b_dbeta\n",
      "    \n",
      "    return dk, dv_out, dbeta\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_bwd_prepare_wy_repr():\n",
      "    results = {}\n",
      "    torch.manual_seed(42)\n",
      "    \n",
      "    # Parameters\n",
      "    T = 4       # block time dimension\n",
      "    H = 2\n",
      "    K_dim = 6\n",
      "    V_dim = 8\n",
      "    BK = 3\n",
      "    BV = 4\n",
      "    B = 3\n",
      "    \n",
      "    # Create random input tensors using HEAD_FIRST layout on DEVICE\n",
      "    k = torch.randn(B, T, K_dim, device=DEVICE)\n",
      "    v = torch.randn(B, T, V_dim, device=DEVICE)\n",
      "    beta = torch.randn(B, T, device=DEVICE)\n",
      "    A = torch.randn(B, T, T, device=DEVICE)\n",
      "    dw = torch.randn(B, T, K_dim, device=DEVICE)\n",
      "    du = torch.randn(B, T, V_dim, device=DEVICE)\n",
      "    \n",
      "    # Pre-allocate outputs for PyTorch implementation\n",
      "    dk = torch.zeros_like(k, device=DEVICE)\n",
      "    dv_out = torch.zeros_like(v, device=DEVICE)\n",
      "    dbeta = torch.zeros_like(beta, device=DEVICE)\n",
      "    \n",
      "    dk_out, dv_out_out, dbeta_out = pytorch_bwd_prepare_wy_repr(\n",
      "        k.clone(), v.clone(), beta.clone(), A.clone(), dw.clone(), du.clone(),\n",
      "        dk, dv_out, dbeta,\n",
      "        offsets=None, indices=None,\n",
      "        T=T, H=H, K_dim=K_dim, V_dim=V_dim,\n",
      "        BK=BK, BV=BV,\n",
      "        HEAD_FIRST=True, USE_OFFSETS=False\n",
      "    )\n",
      "    \n",
      "    results[\"pytorch\"] = {\n",
      "        \"dk\": dk_out,\n",
      "        \"dv_out\": dv_out_out,\n",
      "        \"dbeta\": dbeta_out\n",
      "    }\n",
      "    \n",
      "    # Additional fixed tensor test for simpler debugging\n",
      "    T2 = 2\n",
      "    K_dim2 = 2\n",
      "    V_dim2 = 2\n",
      "    B2 = 1\n",
      "    k2 = torch.ones(B2, T2, K_dim2, device=DEVICE)\n",
      "    v2 = torch.ones(B2, T2, V_dim2, device=DEVICE)\n",
      "    beta2 = torch.ones(B2, T2, device=DEVICE)\n",
      "    A2 = torch.eye(T2, device=DEVICE).unsqueeze(0)  # [1, T2, T2]\n",
      "    dw2 = torch.ones(B2, T2, K_dim2, device=DEVICE)\n",
      "    du2 = torch.ones(B2, T2, V_dim2, device=DEVICE)\n",
      "    dk2 = torch.zeros_like(k2, device=DEVICE)\n",
      "    dv2 = torch.zeros_like(v2, device=DEVICE)\n",
      "    dbeta2 = torch.zeros_like(beta2, device=DEVICE)\n",
      "    \n",
      "    dk2_out, dv2_out, dbeta2_out = pytorch_bwd_prepare_wy_repr(\n",
      "        k2.clone(), v2.clone(), beta2.clone(), A2.clone(), dw2.clone(), du2.clone(),\n",
      "        dk2, dv2, dbeta2,\n",
      "        offsets=None, indices=None,\n",
      "        T=T2, H=H, K_dim=K_dim2, V_dim=V_dim2,\n",
      "        BK=1, BV=1,\n",
      "        HEAD_FIRST=True, USE_OFFSETS=False\n",
      "    )\n",
      "    \n",
      "    results[\"pytorch_fixed\"] = {\n",
      "        \"dk\": dk2_out,\n",
      "        \"dv_out\": dv2_out,\n",
      "        \"dbeta\": dbeta2_out\n",
      "    }\n",
      "    return results\n",
      "\n",
      "\n",
      "# Only print the final test_results dictionary\n",
      "print(test_pytorch_bwd_prepare_wy_repr())\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_mlstm_matmul_kernel_backward\n",
      "====================================================================================================\n",
      "import math\n",
      "import torch\n",
      "\n",
      "# Global Device Standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_mlstm_matmul_kernel_backward(\n",
      "    dH,    # Gradient w.r.t output, shape: [B, NH, S, D]\n",
      "    dB,    # Gradient w.r.t bias, shape: [B, NH, S]\n",
      "    Q,     # Query, shape: [B, NH, S, D]\n",
      "    K,     # Key, shape: [B, NH, S, D]\n",
      "    V,     # Value, shape: [B, NH, S, D]\n",
      "    F,     # Some intermediate activation, shape: [B, NH, S]\n",
      "    I,     # Another intermediate, shape: [B, NH, S]\n",
      "    M,     # Mask or modulation factor, shape: [B, NH, S]\n",
      "    B_val, # Bias term, shape: [B, NH, S]\n",
      "    NH,    # Number of heads (int)\n",
      "    S,     # Sequence length (int)\n",
      "    D,     # Embedding dimension (int)\n",
      "    SB     # Block/Segment size (int)\n",
      "):\n",
      "    \"\"\"\n",
      "    Pure PyTorch backward pass for mlstm matmul kernel.\n",
      "    \n",
      "    Inputs have the following shapes:\n",
      "      dH: [B, NH, S, D]\n",
      "      dB: [B, NH, S]\n",
      "      Q, K, V: [B, NH, S, D]\n",
      "      F, I, M, B_val: [B, NH, S]\n",
      "\n",
      "    Returns a tuple (dQ, dK, dV, dI, dF) with the same shapes as Q, K, V, I, F respectively.\n",
      "    \"\"\"\n",
      "    BATCH = dH.shape[0]\n",
      "    dQ = torch.zeros_like(Q, device=DEVICE)\n",
      "    dK = torch.zeros_like(K, device=DEVICE)\n",
      "    dV = torch.zeros_like(V, device=DEVICE)\n",
      "    dI_out = torch.zeros(BATCH, NH, S, device=DEVICE, dtype=dH.dtype)\n",
      "    dF_out = torch.zeros(BATCH, NH, S, device=DEVICE, dtype=dH.dtype)\n",
      "\n",
      "    scale = math.sqrt(D)\n",
      "\n",
      "    for b in range(BATCH):\n",
      "        for h in range(NH):\n",
      "            dH_local = dH[b, h]      # [S, D]\n",
      "            dB_local = dB[b, h]      # [S]\n",
      "            Q_local  = Q[b, h]       # [S, D]\n",
      "            K_local  = K[b, h]       # [S, D]\n",
      "            V_local  = V[b, h]       # [S, D]\n",
      "            F_local  = F[b, h]       # [S]\n",
      "            I_local  = I[b, h]       # [S]\n",
      "            M_local  = M[b, h]       # [S]\n",
      "            B_local  = B_val[b, h]   # [S]\n",
      "\n",
      "            n = torch.max(torch.abs(B_local), torch.exp(-M_local)) + 1e-06\n",
      "            f_log = torch.log(torch.sigmoid(F_local))\n",
      "            f_cumsum = torch.cumsum(f_log, dim=0)\n",
      "            f_low = f_cumsum.clone()\n",
      "\n",
      "            dq_acc = torch.zeros_like(dH_local, device=DEVICE)\n",
      "            df_acc = torch.zeros(S, device=DEVICE, dtype=dH_local.dtype)\n",
      "\n",
      "            for start in range(0, S, SB):\n",
      "                end = min(start + SB, S)\n",
      "                idx = torch.arange(start, end, device=DEVICE)\n",
      "\n",
      "                dh_block = dH_local[idx]\n",
      "                q_block = Q_local[idx]\n",
      "                m_block = M_local[idx]\n",
      "                b_block = B_local[idx]\n",
      "                db_block = dB_local[idx]\n",
      "                f_current = f_cumsum[idx]\n",
      "\n",
      "                local_dq = torch.zeros_like(dh_block, device=DEVICE)\n",
      "                local_df = torch.zeros_like(m_block, device=DEVICE)\n",
      "\n",
      "                for j in range(start, -1, -SB):\n",
      "                    j_start = max(j, 0)\n",
      "                    j_end = min(j + SB, S)\n",
      "                    idx_vk = torch.arange(j_start, j_end, device=DEVICE)\n",
      "                    f_next = torch.log(torch.sigmoid(F_local[idx_vk]))\n",
      "                    if j == start:\n",
      "                        f_next_cumsum = torch.cumsum(f_next, dim=0)\n",
      "                        d_mat = f_current.unsqueeze(1) - f_next_cumsum.unsqueeze(0) + I_local[idx_vk].unsqueeze(0)\n",
      "                        mask = (idx.unsqueeze(1) >= idx_vk.unsqueeze(0)).to(dH_local.dtype)\n",
      "                        d_mat = torch.where(mask.bool(), d_mat, torch.tensor(-float(\"inf\"), device=DEVICE))\n",
      "                    else:\n",
      "                        f_current = f_current + f_next.sum()\n",
      "                        f_next_cumsum = torch.cumsum(f_next, dim=0)\n",
      "                        d_mat = f_current.unsqueeze(1) - f_next_cumsum.unsqueeze(0) + I_local[idx_vk].unsqueeze(0)\n",
      "\n",
      "                    d_weight = torch.exp(d_mat - m_block.unsqueeze(1))\n",
      "                    v_block = V_local[idx_vk]\n",
      "                    k_block = K_local[idx_vk] / scale\n",
      "                    dc_tilde = torch.matmul(dh_block, v_block.transpose(0, 1)) * (1.0 / n[idx]).unsqueeze(1) + db_block.unsqueeze(1)\n",
      "\n",
      "                    local_dq = local_dq + torch.matmul(dc_tilde * d_weight, k_block)\n",
      "                    c_tilde = torch.matmul(q_block, k_block.transpose(0, 1)) * d_weight\n",
      "                    local_df = local_df + torch.sum(c_tilde * dc_tilde, dim=1)\n",
      "\n",
      "                dq_acc[idx] = local_dq\n",
      "                df_acc[idx] = local_df\n",
      "\n",
      "            dQ[b, h] = dq_acc\n",
      "\n",
      "            dk_acc = torch.zeros_like(K_local, device=DEVICE)\n",
      "            dv_acc = torch.zeros_like(V_local, device=DEVICE)\n",
      "            di_acc = torch.zeros(S, device=DEVICE, dtype=dH_local.dtype)\n",
      "            f_temp = torch.tensor(0.0, device=DEVICE, dtype=dH_local.dtype)\n",
      "\n",
      "            offset_q = 0\n",
      "            while offset_q < S:\n",
      "                q_end = min(offset_q + SB, S)\n",
      "                idx_q = torch.arange(offset_q, q_end, device=DEVICE)\n",
      "                q_block = Q_local[idx_q] / scale\n",
      "                dh_block = dH_local[idx_q]\n",
      "                f_next = torch.log(torch.sigmoid(F_local[idx_q]))\n",
      "                f_next_sum = f_next.sum()\n",
      "                f_next_cumsum = torch.cumsum(f_next, dim=0)\n",
      "                d_mat = (f_temp + f_next_cumsum).unsqueeze(0) - f_low[idx_q].unsqueeze(0) + I_local[idx_q].unsqueeze(0)\n",
      "                f_temp = f_temp + f_next_sum\n",
      "                if offset_q == 0:\n",
      "                    cur_idx = torch.arange(0, q_end - offset_q, device=DEVICE).unsqueeze(1)\n",
      "                    q_inds = idx_q.unsqueeze(0)\n",
      "                    mask = (cur_idx <= q_inds).to(dH_local.dtype)\n",
      "                    d_mat = torch.where(mask.bool(), d_mat, torch.tensor(-float(\"inf\"), device=DEVICE))\n",
      "                m_block = M_local[idx_q]\n",
      "                d_weight = torch.exp(d_mat - m_block.unsqueeze(0))\n",
      "\n",
      "                v_block = V_local[idx_q]\n",
      "                k_block = K_local[idx_q] / scale\n",
      "                db_block = dB_local[idx_q]\n",
      "                n_block = n[idx_q]\n",
      "\n",
      "                dc_tilde_T = torch.matmul(v_block, dh_block.transpose(0, 1)) * (1.0 / n_block).unsqueeze(1) + db_block.unsqueeze(1)\n",
      "                dk_acc[idx_q] = dk_acc[idx_q] + torch.matmul(dc_tilde_T * d_weight, q_block)\n",
      "                c_tilde_T = torch.matmul(k_block, q_block.transpose(0, 1)) * d_weight\n",
      "                dv_acc[idx_q] = dv_acc[idx_q] + torch.matmul(c_tilde_T / n_block.unsqueeze(0), dh_block)\n",
      "                di_acc[idx_q] = di_acc[idx_q] + torch.sum(c_tilde_T * dc_tilde_T, dim=1)\n",
      "\n",
      "                offset_q += SB\n",
      "\n",
      "            dK[b, h] = dk_acc\n",
      "            dV[b, h] = dv_acc\n",
      "            dI_out[b, h] = di_acc\n",
      "            dF_out[b, h, :] = di_acc - df_acc\n",
      "\n",
      "    return dQ, dK, dV, dI_out, dF_out\n",
      "\n",
      "########################\n",
      "# Testing Functions\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_mlstm_matmul_kernel_backward():\n",
      "    \"\"\"\n",
      "    Executes tests on the PyTorch implementation and returns test results in a dictionary.\n",
      "    Tests include correct output shapes and consistency on repeated runs.\n",
      "    \"\"\"\n",
      "    results = {\"PyTorch\": {}}\n",
      "    torch.manual_seed(42)\n",
      "\n",
      "    # Parameters\n",
      "    B = 2   # Number of batches\n",
      "    NH = 2  # Number of heads\n",
      "    S = 7   # Sequence length\n",
      "    D = 4   # Embedding dimension\n",
      "    SB = 3  # Block/Segment size\n",
      "\n",
      "    device = torch.device(DEVICE)  # Use global DEVICE\n",
      "\n",
      "    # Create random input tensors on DEVICE\n",
      "    dH = torch.randn(B, NH, S, D, device=device)\n",
      "    dB = torch.randn(B, NH, S, device=device)\n",
      "    Q  = torch.randn(B, NH, S, D, device=device)\n",
      "    K  = torch.randn(B, NH, S, D, device=device)\n",
      "    V  = torch.randn(B, NH, S, D, device=device)\n",
      "    F  = torch.randn(B, NH, S, device=device)\n",
      "    I  = torch.randn(B, NH, S, device=device)\n",
      "    M  = torch.randn(B, NH, S, device=device)\n",
      "    B_val = torch.randn(B, NH, S, device=device)\n",
      "\n",
      "    # Run PyTorch implementation\n",
      "    p_dQ, p_dK, p_dV, p_dI, p_dF = pytorch_mlstm_matmul_kernel_backward(dH, dB, Q, K, V, F, I, M, B_val, NH, S, D, SB)\n",
      "\n",
      "    # Record shapes\n",
      "    results[\"PyTorch\"][\"dQ_shape\"] = list(p_dQ.shape)\n",
      "    results[\"PyTorch\"][\"dK_shape\"] = list(p_dK.shape)\n",
      "    results[\"PyTorch\"][\"dV_shape\"] = list(p_dV.shape)\n",
      "    results[\"PyTorch\"][\"dI_shape\"] = list(p_dI.shape)\n",
      "    results[\"PyTorch\"][\"dF_shape\"] = list(p_dF.shape)\n",
      "\n",
      "    # Check consistency by repeating the computation\n",
      "    p_dQ2, p_dK2, p_dV2, p_dI2, p_dF2 = pytorch_mlstm_matmul_kernel_backward(dH, dB, Q, K, V, F, I, M, B_val, NH, S, D, SB)\n",
      "    results[\"PyTorch\"][\"dQ_consistent\"] = torch.allclose(p_dQ, p_dQ2, atol=1e-6)\n",
      "    results[\"PyTorch\"][\"dK_consistent\"] = torch.allclose(p_dK, p_dK2, atol=1e-6)\n",
      "    results[\"PyTorch\"][\"dV_consistent\"] = torch.allclose(p_dV, p_dV2, atol=1e-6)\n",
      "    results[\"PyTorch\"][\"dI_consistent\"] = torch.allclose(p_dI, p_dI2, atol=1e-6)\n",
      "    results[\"PyTorch\"][\"dF_consistent\"] = torch.allclose(p_dF, p_dF2, atol=1e-6)\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "# Execute tests and print the test results\n",
      "\n",
      "test_results = test_pytorch_mlstm_matmul_kernel_backward()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "layer_norm_modulation_fwd_\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "########################\n",
      "# PyTorch Implementation\n",
      "\n",
      "def layer_norm_modulation_fwd_(X: torch.Tensor, W: torch.Tensor, B: torch.Tensor, eps: float = 1e-5):\n",
      "    \"\"\"\n",
      "    Perform modulated layer normalization using PyTorch operations.\n",
      "    \n",
      "    For each row in X (of shape [N]), compute:\n",
      "      mean = average(x)\n",
      "      var  = average(x**2) - mean**2\n",
      "      rstd = 1.0 / sqrt(var + eps)\n",
      "    Then the output is:\n",
      "      y = (x - mean) * rstd * (1 + w) + b\n",
      "    where w and b are applied from the corresponding batch index.\n",
      "    \n",
      "    Parameters:\n",
      "      X (torch.Tensor): Input tensor of shape (batch, seq_len, N) on DEVICE.\n",
      "      W (torch.Tensor): Modulation weights of shape (batch, N) on DEVICE.\n",
      "      B (torch.Tensor): Modulation biases of shape (batch, N) on DEVICE.\n",
      "      eps (float): Epsilon for numerical stability.\n",
      "    \n",
      "    Returns:\n",
      "      Y (torch.Tensor): Normalized and modulated tensor, same shape as X.\n",
      "      Mean (torch.Tensor): Means per row, shape (batch, seq_len).\n",
      "      Rstd (torch.Tensor): Reciprocal std per row, shape (batch, seq_len).\n",
      "    \"\"\"\n",
      "    batch, seq_len, N = X.shape\n",
      "    # Compute mean and variance along the last dimension\n",
      "    mean = X.mean(dim=-1, keepdim=True)  # shape: (batch, seq_len, 1)\n",
      "    mean_of_square = (X * X).mean(dim=-1, keepdim=True)\n",
      "    var = mean_of_square - mean * mean\n",
      "    rstd = 1.0 / torch.sqrt(var + eps)\n",
      "\n",
      "    # Expand modulation parameters from (batch, N) to (batch, 1, N)\n",
      "    W_expanded = W.unsqueeze(1)\n",
      "    B_expanded = B.unsqueeze(1)\n",
      "\n",
      "    # Compute the normalized and modulated output\n",
      "    Y = (X - mean) * rstd * (1 + W_expanded) + B_expanded\n",
      "\n",
      "    # Squeeze mean and rstd to match output shape (batch, seq_len)\n",
      "    Mean = mean.squeeze(-1)\n",
      "    Rstd = rstd.squeeze(-1)\n",
      "\n",
      "    return Y, Mean, Rstd\n",
      "\n",
      "########################\n",
      "# Integrated Testing\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_layer_norm_modulation_fwd_():\n",
      "    test_results = {}\n",
      "    \n",
      "    # Define test cases; ensure all tensors are created on DEVICE\n",
      "    test_cases = {}\n",
      "\n",
      "    # Test case 1: Small tensor with batch=1, seq_len=1, N=4\n",
      "    X1 = torch.tensor([[[1.0, 2.0, 3.0, 4.0]]], device=DEVICE)\n",
      "    W1 = torch.tensor([[0.1, 0.2, 0.3, 0.4]], device=DEVICE)\n",
      "    B1 = torch.tensor([[0.5, 0.6, 0.7, 0.8]], device=DEVICE)\n",
      "    test_cases[\"test_case_1\"] = {\"X\": X1, \"W\": W1, \"B\": B1, \"eps\": 1e-5}\n",
      "\n",
      "    # Test case 2: Tensor with batch=2, seq_len=3, N=5, random input\n",
      "    torch.manual_seed(42)\n",
      "    X2 = torch.randn(2, 3, 5, device=DEVICE)\n",
      "    W2 = torch.randn(2, 5, device=DEVICE)\n",
      "    B2 = torch.randn(2, 5, device=DEVICE)\n",
      "    test_cases[\"test_case_2\"] = {\"X\": X2, \"W\": W2, \"B\": B2, \"eps\": 1e-5}\n",
      "\n",
      "    # Test case 3: Constant tensor where all features are the same (variance nearly zero)\n",
      "    X3 = torch.full((1, 2, 6), 3.0, device=DEVICE)\n",
      "    W3 = torch.zeros(1, 6, device=DEVICE)\n",
      "    B3 = torch.zeros(1, 6, device=DEVICE)\n",
      "    test_cases[\"test_case_3\"] = {\"X\": X3, \"W\": W3, \"B\": B3, \"eps\": 1e-5}\n",
      "\n",
      "    # Test case 4: Varying eps for testing numerical stability\n",
      "    X4 = torch.tensor([[[1.0, 2.0, 3.0, 4.0]]], device=DEVICE)\n",
      "    W4 = torch.tensor([[0.0, 0.0, 0.0, 0.0]], device=DEVICE)\n",
      "    B4 = torch.tensor([[0.0, 0.0, 0.0, 0.0]], device=DEVICE)\n",
      "    test_cases[\"test_case_4\"] = {\"X\": X4, \"W\": W4, \"B\": B4, \"eps\": 1e-3}\n",
      "\n",
      "    # Run each test case through the PyTorch implementation\n",
      "    for key, case in test_cases.items():\n",
      "        X = case[\"X\"]\n",
      "        W = case[\"W\"]\n",
      "        B = case[\"B\"]\n",
      "        eps = case[\"eps\"]\n",
      "\n",
      "        Y_torch, Mean_torch, Rstd_torch = layer_norm_modulation_fwd_(X, W, B, eps)\n",
      "\n",
      "        test_results[key] = {\n",
      "            \"pytorch\": {\n",
      "                \"Y\": Y_torch,\n",
      "                \"Mean\": Mean_torch,\n",
      "                \"Rstd\": Rstd_torch\n",
      "            }\n",
      "        }\n",
      "\n",
      "    # Print the test_results dictionary\n",
      "    print(test_results)\n",
      "    return test_results\n",
      "\n",
      "# Execute tests and store test results\n",
      "\n",
      "test_results = test_layer_norm_modulation_fwd_()\n",
      "\n",
      "****************************************************************************************************\n",
      "quantized_matmul\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def quantized_matmul(A_packed: torch.Tensor, B: torch.Tensor, scale: float, zero_point: int, BITS: int) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs quantized matrix multiplication mimicking the Triton kernel behavior using PyTorch.\n",
      "    The implementation uses int64 for matmul to match the ground truth.\n",
      "    \"\"\"\n",
      "    QFACT = 8 // BITS\n",
      "    QMASK = (1 << BITS) - 1\n",
      "    N, K1 = A_packed.shape\n",
      "    unpacked = []\n",
      "    for j in range(QFACT):\n",
      "        unpacked.append(((A_packed >> (BITS * j)) & QMASK).to(torch.int32))\n",
      "    A_unpacked = torch.stack(unpacked, dim=-1).view(N, K1 * QFACT)\n",
      "    A_dequant = A_unpacked - zero_point\n",
      "    C = A_dequant.to(torch.int64).matmul(B.to(torch.int64))\n",
      "    C = C.to(torch.float32) * scale\n",
      "    return C\n",
      "\n",
      "\n",
      "def pack_quantized(A: torch.Tensor, BITS: int, zero_point: int) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Packs an unpacked quantized matrix A into a compressed format.\n",
      "    \"\"\"\n",
      "    QFACT = 8 // BITS\n",
      "    N, K = A.shape\n",
      "    if K % QFACT != 0:\n",
      "        raise ValueError(f\"K={K} must be divisible by QFACT={QFACT}\")\n",
      "    K1 = K // QFACT\n",
      "    A_reshaped = A.view(N, K1, QFACT)\n",
      "    packed = torch.zeros((N, K1), dtype=torch.uint8, device=A.device)\n",
      "    for j in range(QFACT):\n",
      "        packed |= (A_reshaped[:, :, j].to(torch.uint8) << (BITS * j))\n",
      "    return packed\n",
      "\n",
      "########################\n",
      "# PyTorch Implementation Tests\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_quantized_matmul():\n",
      "    \"\"\"\n",
      "    Runs tests for the PyTorch implementation of quantized_matmul.\n",
      "    Returns a dictionary of test results.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "    device = DEVICE\n",
      "    \n",
      "    # Test case 1\n",
      "    bits = 4\n",
      "    zero_point = 0\n",
      "    scale = 0.1\n",
      "    A_unpacked = torch.tensor([[1, 2, 3, 4],\n",
      "                               [5, 6, 7, 8]], dtype=torch.int32, device=device)\n",
      "    A_packed = pack_quantized(A_unpacked, bits, zero_point)\n",
      "    B = torch.tensor([[1, 2, 3],\n",
      "                      [4, 5, 6],\n",
      "                      [7, 8, 9],\n",
      "                      [10, 11, 12]], dtype=torch.int32, device=device)\n",
      "    # Fix: matmul on float32, not int64, for CUDA compatibility\n",
      "    expected = (A_unpacked.to(torch.float32).matmul(B.to(torch.float32))) * scale\n",
      "    result = quantized_matmul(A_packed, B, scale, zero_point, bits)\n",
      "    results[\"test_case_1\"] = {\"A_unpacked\": A_unpacked,\n",
      "                              \"A_packed\": A_packed,\n",
      "                              \"B\": B,\n",
      "                              \"expected\": expected,\n",
      "                              \"result\": result}\n",
      "    \n",
      "    # Test case 2\n",
      "    bits = 2\n",
      "    zero_point = 1\n",
      "    scale = 0.5\n",
      "    A_unpacked = torch.tensor([\n",
      "        [0, 1, 2, 3, 0, 1, 2, 3],\n",
      "        [3, 2, 1, 0, 3, 2, 1, 0],\n",
      "        [1, 1, 1, 1, 2, 2, 2, 2]\n",
      "    ], dtype=torch.int32, device=device)\n",
      "    A_packed = pack_quantized(A_unpacked, bits, zero_point)\n",
      "    B = torch.arange(1, 8 * 4 + 1, dtype=torch.int32, device=device).reshape(8, 4)\n",
      "    expected = (A_unpacked.to(torch.float32).matmul(B.to(torch.float32))) * scale\n",
      "    result = quantized_matmul(A_packed, B, scale, zero_point, bits)\n",
      "    results[\"test_case_2\"] = {\"A_unpacked\": A_unpacked,\n",
      "                              \"A_packed\": A_packed,\n",
      "                              \"B\": B,\n",
      "                              \"expected\": expected,\n",
      "                              \"result\": result}\n",
      "    \n",
      "    # Test case 3\n",
      "    bits = 4\n",
      "    zero_point = 2\n",
      "    scale = 0.05\n",
      "    N = 4\n",
      "    K = 8\n",
      "    M = 5\n",
      "    torch.manual_seed(0)\n",
      "    A_unpacked = torch.randint(0, 16, (N, K), dtype=torch.int32, device=device)\n",
      "    A_packed = pack_quantized(A_unpacked, bits, zero_point)\n",
      "    B = torch.randint(-5, 10, (K, M), dtype=torch.int32, device=device)\n",
      "    expected = (A_unpacked.to(torch.float32).matmul(B.to(torch.float32))) * scale\n",
      "    result = quantized_matmul(A_packed, B, scale, zero_point, bits)\n",
      "    results[\"test_case_3\"] = {\"A_unpacked\": A_unpacked,\n",
      "                              \"A_packed\": A_packed,\n",
      "                              \"B\": B,\n",
      "                              \"expected\": expected,\n",
      "                              \"result\": result}\n",
      "    \n",
      "    return results\n",
      "\n",
      "\n",
      "# Execute tests and print test_results dictionary\n",
      "\n",
      "test_results = test_quantized_matmul()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "torch_attn_fwd\n",
      "====================================================================================================\n",
      "import torch\n",
      "import math\n",
      "\n",
      "# Global device standard: Use CUDA if available, otherwise CPU\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "########################################\n",
      "# PyTorch Implementation\n",
      "########################################\n",
      "\n",
      "def torch_attn_fwd(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, sm_scale: float):\n",
      "    \"\"\"\n",
      "    PyTorch implementation of multi-head self-attention forward pass.\n",
      "    Q, K, V are 4D tensors with shape (batch, num_heads, seq_length, head_dim).\n",
      "    The scaling factor is multiplied by 1.44269504 to match expected behavior.\n",
      "\n",
      "    Returns:\n",
      "        M: Tensor of shape (batch, num_heads, seq_length) - the numerically-stabilized log-sum (in log2 space).\n",
      "        output: Tensor of shape (batch, num_heads, seq_length, head_dim) - the attention output.\n",
      "    \"\"\"\n",
      "    scale = sm_scale * 1.44269504\n",
      "    logits = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
      "    max_logits, _ = logits.max(dim=-1, keepdim=True)\n",
      "    sum_exp = torch.sum(torch.exp(logits - max_logits), dim=-1)\n",
      "    M = max_logits.squeeze(-1) + torch.log(sum_exp) / math.log(2)\n",
      "    probs = torch.softmax(logits, dim=-1)\n",
      "    output = torch.matmul(probs, V)\n",
      "    return M, output\n",
      "\n",
      "########################\n",
      "# Testing Function for PyTorch Implementation\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_torch_attn_fwd():\n",
      "    \"\"\"\n",
      "    Testing function for the PyTorch attention forward pass.\n",
      "    Returns a dictionary with test results for each test case.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "    \n",
      "    # Test Case 1: Small tensor (batch=1, heads=1, seq_length=4, head_dim=3)\n",
      "    batch, heads, seq_len, head_dim = 1, 1, 4, 3\n",
      "    Q = torch.randn(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    K = torch.randn(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    V = torch.randn(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    M, output = torch_attn_fwd(Q, K, V, 0.1)\n",
      "    results[\"test_case_1\"] = {\n",
      "        \"M_shape\": M.shape,\n",
      "        \"output_shape\": output.shape,\n",
      "        \"M\": M,\n",
      "        \"output\": output\n",
      "    }\n",
      "    \n",
      "    # Test Case 2: Larger tensor (batch=2, heads=4, seq_length=8, head_dim=16)\n",
      "    batch, heads, seq_len, head_dim = 2, 4, 8, 16\n",
      "    Q = torch.randn(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    K = torch.randn(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    V = torch.randn(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    M, output = torch_attn_fwd(Q, K, V, 0.05)\n",
      "    results[\"test_case_2\"] = {\n",
      "        \"M_shape\": M.shape,\n",
      "        \"output_shape\": output.shape,\n",
      "        \"M\": M,\n",
      "        \"output\": output\n",
      "    }\n",
      "    \n",
      "    # Test Case 3: Uniform inputs (batch=1, heads=2, seq_length=5, head_dim=10)\n",
      "    batch, heads, seq_len, head_dim = 1, 2, 5, 10\n",
      "    Q = torch.ones(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    K = torch.ones(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    V = torch.arange(float(seq_len * head_dim), device=DEVICE).reshape(1, 1, seq_len, head_dim).repeat(1, 2, 1, 1)\n",
      "    M, output = torch_attn_fwd(Q, K, V, 1.0)\n",
      "    results[\"test_case_3\"] = {\n",
      "        \"M\": M,\n",
      "        \"output\": output\n",
      "    }\n",
      "    \n",
      "    # Test Case 4: Large scaling factor (batch=1, heads=1, seq_length=6, head_dim=4)\n",
      "    batch, heads, seq_len, head_dim = 1, 1, 6, 4\n",
      "    Q = torch.randn(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    K = torch.randn(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    V = torch.randn(batch, heads, seq_len, head_dim, device=DEVICE)\n",
      "    M, output = torch_attn_fwd(Q, K, V, 5.0)\n",
      "    results[\"test_case_4\"] = {\n",
      "        \"M\": M,\n",
      "        \"output\": output\n",
      "    }\n",
      "    \n",
      "    return results\n",
      "\n",
      "########################\n",
      "# End of PyTorch Implementation and Tests\n",
      "########################\n",
      "\n",
      "test_results = test_torch_attn_fwd()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "fused_moe_\n",
      "====================================================================================================\n",
      "import torch\n",
      "import math\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def fused_moe_(A: torch.Tensor,\n",
      "                    B: torch.Tensor,\n",
      "                    scale_b: torch.Tensor,\n",
      "                    zero_points: torch.Tensor,\n",
      "                    topk_weights: torch.Tensor,\n",
      "                    sorted_token_ids: torch.Tensor,\n",
      "                    expert_ids: torch.Tensor,\n",
      "                    num_tokens_post_padded: int,\n",
      "                    num_valid_tokens: int,\n",
      "                    top_k: int,\n",
      "                    N: int,\n",
      "                    K: int,\n",
      "                    add_zero_points: bool,\n",
      "                    MUL_ROUTED_WEIGHT: bool) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Pure PyTorch implementation mimicking the behavior of the fused MoE Triton kernel.\n",
      "    Implements bit-manipulation on B (int8) to match:\n",
      "       - Even columns: (b << 4) >> 4\n",
      "       - Odd columns:  b >> 4\n",
      "    \"\"\"\n",
      "    M = sorted_token_ids.numel()\n",
      "    output = torch.zeros((M, 2 * N), dtype=A.dtype, device=DEVICE)\n",
      "    for i in range(M):\n",
      "        token_id = int(sorted_token_ids[i].item())\n",
      "        if token_id >= num_valid_tokens or token_id >= num_tokens_post_padded:\n",
      "            continue\n",
      "        a_row = token_id // top_k\n",
      "        if a_row >= A.shape[0]:\n",
      "            continue\n",
      "        # Load and prepare A vector\n",
      "        a_vec = A[a_row, :].to(torch.float32)\n",
      "        expert = int(expert_ids[i].item())\n",
      "        # Load corresponding B block [K, 2*N]\n",
      "        b_block = B[expert, :, :].clone()\n",
      "        cols = torch.arange(2 * N, device=DEVICE)\n",
      "        even_mask = (cols % 2 == 0)\n",
      "        odd_mask = ~even_mask\n",
      "        b_new = torch.empty_like(b_block)\n",
      "        if even_mask.sum() > 0:\n",
      "            b_new[:, even_mask] = (((b_block[:, even_mask].to(torch.int16)) << 4) >> 4).to(torch.int8)\n",
      "        if odd_mask.sum() > 0:\n",
      "            b_new[:, odd_mask] = ((b_block[:, odd_mask].to(torch.int16)) >> 4).to(torch.int8)\n",
      "        b_block = b_new.to(torch.float32)\n",
      "        if add_zero_points:\n",
      "            zp = zero_points[expert].to(torch.float32).view(1, 2 * N)\n",
      "            b_block = b_block - zp\n",
      "        scales = scale_b[expert].view(1, 2 * N)\n",
      "        b_block = b_block * scales\n",
      "        acc = torch.matmul(a_vec.unsqueeze(0), b_block).squeeze(0)\n",
      "        if MUL_ROUTED_WEIGHT:\n",
      "            moe_weight = topk_weights[i]\n",
      "            acc = acc * moe_weight\n",
      "        output[i, :] = acc.to(A.dtype)\n",
      "    return output\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_fused_moe_():\n",
      "    results = {}\n",
      "    torch.manual_seed(42)\n",
      "    # Parameters\n",
      "    K = 8               # Inner dimension\n",
      "    N = 4               # Half output channels (2*N = full output channels)\n",
      "    top_k = 2           # Grouping factor\n",
      "    M = 6\n",
      "    num_valid_tokens = 5\n",
      "    num_tokens_post_padded = M\n",
      "    num_A_rows = (num_valid_tokens + top_k - 1) // top_k\n",
      "    num_experts = 3\n",
      "\n",
      "    # Create inputs on DEVICE\n",
      "    A = torch.randn(num_A_rows, K, dtype=torch.float32, device=DEVICE)\n",
      "    B = torch.randint(-128, 127, (num_experts, K, 2 * N), dtype=torch.int8, device=DEVICE)\n",
      "    scale_b = torch.rand(num_experts, 2 * N, dtype=torch.float32, device=DEVICE) * 0.1 + 0.95\n",
      "    zero_points = torch.randint(0, 5, (num_experts, 2 * N), dtype=torch.int8, device=DEVICE)\n",
      "    topk_weights = torch.rand(M, dtype=torch.float32, device=DEVICE) * 0.5 + 0.75\n",
      "    sorted_token_ids = torch.tensor([0, 1, 3, 4, 5, 7], dtype=torch.int32, device=DEVICE)\n",
      "    expert_ids = torch.tensor([0, 1, 2, 0, 1, 2], dtype=torch.int32, device=DEVICE)\n",
      "\n",
      "    # Case 1: Without zero_points adjustment and moe weight scaling.\n",
      "    output1 = fused_moe_(A, B, scale_b, zero_points, topk_weights, sorted_token_ids,\n",
      "                              expert_ids, num_tokens_post_padded, num_valid_tokens,\n",
      "                              top_k, N, K, add_zero_points=False, MUL_ROUTED_WEIGHT=False)\n",
      "    results['case1_no_zp_no_moe'] = output1\n",
      "\n",
      "    # Case 2: With zero_points adjustment.\n",
      "    output2 = fused_moe_(A, B, scale_b, zero_points, topk_weights, sorted_token_ids,\n",
      "                              expert_ids, num_tokens_post_padded, num_valid_tokens,\n",
      "                              top_k, N, K, add_zero_points=True, MUL_ROUTED_WEIGHT=False)\n",
      "    results['case2_with_zp'] = output2\n",
      "\n",
      "    # Case 3: With moe weight scaling.\n",
      "    output3 = fused_moe_(A, B, scale_b, zero_points, topk_weights, sorted_token_ids,\n",
      "                              expert_ids, num_tokens_post_padded, num_valid_tokens,\n",
      "                              top_k, N, K, add_zero_points=False, MUL_ROUTED_WEIGHT=True)\n",
      "    results['case3_with_moe_weight'] = output3\n",
      "\n",
      "    # Case 4: With both adjustments.\n",
      "    output4 = fused_moe_(A, B, scale_b, zero_points, topk_weights, sorted_token_ids,\n",
      "                              expert_ids, num_tokens_post_padded, num_valid_tokens,\n",
      "                              top_k, N, K, add_zero_points=True, MUL_ROUTED_WEIGHT=True)\n",
      "    results['case4_with_both'] = output4\n",
      "\n",
      "    # Case 5: Small dimensions test (single token processing).\n",
      "    A_small = torch.tensor([[1.0, 2.0, 3.0, 4.0]], dtype=torch.float32, device=DEVICE)\n",
      "    K_small = 4\n",
      "    N_small = 2\n",
      "    top_k_small = 1\n",
      "    B_small = torch.randint(-128, 127, (1, K_small, 2 * N_small), dtype=torch.int8, device=DEVICE)\n",
      "    scale_b_small = torch.tensor([[1.0, 1.0, 1.0, 1.0]], dtype=torch.float32, device=DEVICE)\n",
      "    zero_points_small = torch.zeros((1, 2 * N_small), dtype=torch.int8, device=DEVICE)\n",
      "    topk_weights_small = torch.tensor([1.0], dtype=torch.float32, device=DEVICE)\n",
      "    sorted_token_ids_small = torch.tensor([0], dtype=torch.int32, device=DEVICE)\n",
      "    expert_ids_small = torch.tensor([0], dtype=torch.int32, device=DEVICE)\n",
      "    output_small = fused_moe_(A_small, B_small, scale_b_small, zero_points_small, topk_weights_small,\n",
      "                                   sorted_token_ids_small, expert_ids_small,\n",
      "                                   num_tokens_post_padded=1, num_valid_tokens=1,\n",
      "                                   top_k=top_k_small, N=N_small, K=K_small,\n",
      "                                   add_zero_points=False, MUL_ROUTED_WEIGHT=False)\n",
      "    results['case5_small'] = output_small\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "# Run tests and print only the final test_results dictionary\n",
      "\n",
      "test_results = test_fused_moe_()\n",
      "print(test_results)\n",
      "\n",
      "# Note: For side-by-side comparison with the Triton implementation,\n",
      "# a separate combined test file can import this module and the Triton module,\n",
      "# and then combine their respective test_results dictionaries.\n",
      "****************************************************************************************************\n",
      "chunk_hgrn_bwd_o_\n",
      "====================================================================================================\n",
      "import math\n",
      "import torch\n",
      "\n",
      "# Global device variable as per standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def chunk_hgrn_bwd_o_(g, gc, o, dx, dg, BT, BD):\n",
      "    \"\"\"\n",
      "    Pure PyTorch implementation simulating the Triton kernel functionality.\n",
      "    Performs an in-place update of dx and computes dg using block-wise operations.\n",
      "\n",
      "    Parameters:\n",
      "      g, gc, o, dx, dg: torch.Tensor of shape [B, T, D] on DEVICE (float32)\n",
      "      BT: int, block size along the time dimension\n",
      "      BD: int, block size along the feature dimension\n",
      "\n",
      "    Returns:\n",
      "      The updated dx and computed dg.\n",
      "    \"\"\"\n",
      "    B, T, D = dx.shape\n",
      "    num_t_blocks = math.ceil(T / BT)\n",
      "    num_d_blocks = math.ceil(D / BD)\n",
      "    for b in range(B):\n",
      "        for id_blk in range(num_d_blocks):\n",
      "            col_start = id_blk * BD\n",
      "            col_end = min(D, col_start + BD)\n",
      "            for i_t in reversed(range(num_t_blocks)):\n",
      "                t_start = i_t * BT\n",
      "                t_end = min((i_t + 1) * BT, T)\n",
      "                cur_BT = t_end - t_start\n",
      "\n",
      "                # Slice blocks from tensors\n",
      "                b_g = g[b, t_start:t_end, col_start:col_end]\n",
      "                b_gc = gc[b, t_start:t_end, col_start:col_end]\n",
      "                b_dx = dx[b, t_start:t_end, col_start:col_end]\n",
      "\n",
      "                # Load o block from the previous time step (if available)\n",
      "                if t_start - 1 >= 0:\n",
      "                    o_row_start = t_start - 1\n",
      "                    o_row_end = o_row_start + cur_BT\n",
      "                    if o_row_end > T:\n",
      "                        b_o = torch.cat([\n",
      "                            o[b, o_row_start:T, col_start:col_end],\n",
      "                            torch.zeros((o_row_end - T, col_end - col_start), device=DEVICE, dtype=o.dtype)\n",
      "                        ], dim=0)\n",
      "                    else:\n",
      "                        b_o = o[b, o_row_start:o_row_end, col_start:col_end]\n",
      "                else:\n",
      "                    b_o = torch.zeros((cur_BT, col_end - col_start), device=DEVICE, dtype=dx.dtype)\n",
      "\n",
      "                # Load b_ht from dx at time index (i_t+1)*BT if within bounds\n",
      "                ht_idx = (i_t + 1) * BT\n",
      "                if ht_idx < T:\n",
      "                    b_ht = dx[b, ht_idx, col_start:col_end]  # shape: [width]\n",
      "                else:\n",
      "                    b_ht = torch.zeros(col_end - col_start, device=DEVICE, dtype=dx.dtype)\n",
      "\n",
      "                # Update dx block: add exp(b_gc) * b_ht (broadcasted along the time dimension)\n",
      "                new_b_dx = b_dx + torch.exp(b_gc) * b_ht.unsqueeze(0)\n",
      "\n",
      "                # Compute dg block: dg = b_o * new_b_dx * exp(b_g)\n",
      "                b_dg = b_o * new_b_dx * torch.exp(b_g)\n",
      "\n",
      "                # Write updated values back\n",
      "                dx[b, t_start:t_end, col_start:col_end] = new_b_dx\n",
      "                dg[b, t_start:t_end, col_start:col_end] = b_dg\n",
      "    \n",
      "    return dx, dg\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_chunk_hgrn_bwd_o_():\n",
      "    \"\"\"\n",
      "    Integrated tests for the PyTorch implementation of chunk_hgrn_bwd_o_.\n",
      "    All tensors are allocated on DEVICE.\n",
      "    Returns a dictionary with test results.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "\n",
      "    # Test case 1: T and D divisible by block sizes\n",
      "    B, T, D = 1, 8, 8\n",
      "    BT, BD = 2, 4\n",
      "    g = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    gc = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    o = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    dx = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    dg = torch.zeros_like(dx, device=DEVICE)\n",
      "    updated_dx, updated_dg = chunk_hgrn_bwd_o_(g, gc, o, dx, dg, BT, BD)\n",
      "    results[\"case_1\"] = {\n",
      "        \"updated_dx\": updated_dx.clone(),\n",
      "        \"updated_dg\": updated_dg.clone()\n",
      "    }\n",
      "\n",
      "    # Test case 2: T and D not multiples of BT and BD\n",
      "    B, T, D = 2, 7, 10\n",
      "    BT, BD = 3, 4\n",
      "    g = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    gc = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    o = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    dx = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    dg = torch.zeros_like(dx, device=DEVICE)\n",
      "    updated_dx2, updated_dg2 = chunk_hgrn_bwd_o_(g, gc, o, dx, dg, BT, BD)\n",
      "    results[\"case_2\"] = {\n",
      "        \"updated_dx\": updated_dx2.clone(),\n",
      "        \"updated_dg\": updated_dg2.clone()\n",
      "    }\n",
      "\n",
      "    # Test case 3: Last block with no contribution from b_ht\n",
      "    B, T, D = 1, 5, 6\n",
      "    BT, BD = 2, 3\n",
      "    g = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    gc = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    o = torch.randn(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    dx = torch.zeros(B, T, D, dtype=torch.float32, device=DEVICE)\n",
      "    dg = torch.zeros_like(dx, device=DEVICE)\n",
      "    updated_dx3, updated_dg3 = chunk_hgrn_bwd_o_(g, gc, o, dx, dg, BT, BD)\n",
      "    results[\"case_3\"] = {\n",
      "        \"updated_dx\": updated_dx3.clone(),\n",
      "        \"updated_dg\": updated_dg3.clone()\n",
      "    }\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "test_results = test_chunk_hgrn_bwd_o_()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "chunk_delta_rule_fwd_kernel_prepare_dv\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "import math\n",
      "\n",
      "\n",
      "def chunk_delta_rule_fwd_kernel_prepare_dv(q: torch.Tensor,\n",
      "                                                   k: torch.Tensor,\n",
      "                                                   do: torch.Tensor,\n",
      "                                                   scale: float,\n",
      "                                                   T: int,\n",
      "                                                   H: int,\n",
      "                                                   K: int,\n",
      "                                                   V: int,\n",
      "                                                   BT: int,\n",
      "                                                   BK: int,\n",
      "                                                   BV: int,\n",
      "                                                   offsets: torch.Tensor = None,\n",
      "                                                   indices: torch.Tensor = None,\n",
      "                                                   HEAD_FIRST: bool = True) -> torch.Tensor:\n",
      "    \"\"\"Pure PyTorch implementation mimicking the Triton kernel logic.\n",
      "    Allocates a full BT x BT accumulation matrix for upper-triangular masking.\n",
      "    \"\"\"\n",
      "    USE_OFFSETS = offsets is not None\n",
      "    dv = torch.empty_like(do)\n",
      "    \n",
      "    if HEAD_FIRST:\n",
      "        B_H = q.shape[0]\n",
      "        if not USE_OFFSETS:\n",
      "            for b in range(B_H):\n",
      "                for t_start in range(0, T, BT):\n",
      "                    t_end = min(t_start + BT, T)\n",
      "                    tile_size = t_end - t_start\n",
      "                    # Allocate full BT x BT matrix\n",
      "                    b_A = torch.zeros(BT, BT, dtype=q.dtype, device=q.device)\n",
      "                    for i in range(0, K, BK):\n",
      "                        i_end = min(i + BK, K)\n",
      "                        q_block = q[b, t_start:t_end, i:i_end]\n",
      "                        k_block = k[b, t_start:t_end, i:i_end]\n",
      "                        b_A[:tile_size, :tile_size] += k_block @ (scale * q_block).transpose(0, 1)\n",
      "                    # Apply upper-triangular mask\n",
      "                    mask = torch.triu(torch.ones(tile_size, tile_size, dtype=torch.bool, device=q.device))\n",
      "                    b_A_valid = b_A[:tile_size, :tile_size] * mask.to(b_A.dtype)\n",
      "                    do_tile = do[b, t_start:t_end, :]\n",
      "                    dv[b, t_start:t_end, :] = b_A_valid @ do_tile\n",
      "        else:\n",
      "            num_tiles = indices.numel() // 2\n",
      "            for b in range(B_H):\n",
      "                for tile in range(num_tiles):\n",
      "                    i_n = int(indices[2 * tile].item())\n",
      "                    bos = int(offsets[i_n].item())\n",
      "                    eos = int(offsets[i_n + 1].item())\n",
      "                    tile_size = eos - bos\n",
      "                    b_A = torch.zeros(BT, BT, dtype=q.dtype, device=q.device)\n",
      "                    for i in range(0, K, BK):\n",
      "                        i_end = min(i + BK, K)\n",
      "                        q_block = q[b, bos:eos, i:i_end]\n",
      "                        k_block = k[b, bos:eos, i:i_end]\n",
      "                        b_A[:tile_size, :tile_size] += k_block @ (scale * q_block).transpose(0, 1)\n",
      "                    mask = torch.triu(torch.ones(tile_size, tile_size, dtype=torch.bool, device=q.device))\n",
      "                    b_A_valid = b_A[:tile_size, :tile_size] * mask.to(b_A.dtype)\n",
      "                    do_tile = do[b, bos:eos, :]\n",
      "                    dv[b, bos:eos, :] = b_A_valid @ do_tile\n",
      "    else:\n",
      "        B, H_val, _, _ = q.shape\n",
      "        if not USE_OFFSETS:\n",
      "            for b in range(B):\n",
      "                for h in range(H_val):\n",
      "                    for t_start in range(0, T, BT):\n",
      "                        t_end = min(t_start + BT, T)\n",
      "                        tile_size = t_end - t_start\n",
      "                        b_A = torch.zeros(BT, BT, dtype=q.dtype, device=q.device)\n",
      "                        for i in range(0, K, BK):\n",
      "                            i_end = min(i + BK, K)\n",
      "                            q_block = q[b, h, t_start:t_end, i:i_end]\n",
      "                            k_block = k[b, h, t_start:t_end, i:i_end]\n",
      "                            b_A[:tile_size, :tile_size] += k_block @ (scale * q_block).transpose(0, 1)\n",
      "                        mask = torch.triu(torch.ones(tile_size, tile_size, dtype=torch.bool, device=q.device))\n",
      "                        b_A_valid = b_A[:tile_size, :tile_size] * mask.to(b_A.dtype)\n",
      "                        do_tile = do[b, h, t_start:t_end, :]\n",
      "                        dv[b, h, t_start:t_end, :] = b_A_valid @ do_tile\n",
      "        else:\n",
      "            num_tiles = indices.numel() // 2\n",
      "            for b in range(B):\n",
      "                for h in range(H_val):\n",
      "                    for tile in range(num_tiles):\n",
      "                        i_n = int(indices[2 * tile].item())\n",
      "                        bos = int(offsets[i_n].item())\n",
      "                        eos = int(offsets[i_n + 1].item())\n",
      "                        tile_size = eos - bos\n",
      "                        b_A = torch.zeros(BT, BT, dtype=q.dtype, device=q.device)\n",
      "                        for i in range(0, K, BK):\n",
      "                            i_end = min(i + BK, K)\n",
      "                            q_block = q[b, h, bos:eos, i:i_end]\n",
      "                            k_block = k[b, h, bos:eos, i:i_end]\n",
      "                            b_A[:tile_size, :tile_size] += k_block @ (scale * q_block).transpose(0, 1)\n",
      "                        mask = torch.triu(torch.ones(tile_size, tile_size, dtype=torch.bool, device=q.device))\n",
      "                        b_A_valid = b_A[:tile_size, :tile_size] * mask.to(b_A.dtype)\n",
      "                        do_tile = do[b, h, bos:eos, :]\n",
      "                        dv[b, h, bos:eos, :] = b_A_valid @ do_tile\n",
      "    return dv\n",
      "\n",
      "########################\n",
      "# Test Cases\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_chunk_delta_rule_fwd_kernel_prepare_dv():\n",
      "    \"\"\"Test cases for the PyTorch implementation using DEVICE for all tensors.\"\"\"\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "\n",
      "    # Test Case 1: HEAD_FIRST=True, no offsets\n",
      "    B = 2\n",
      "    H = 3\n",
      "    T = 8\n",
      "    K = 8\n",
      "    V = 7\n",
      "    BT = 4\n",
      "    BK = 4\n",
      "    BV = 4\n",
      "    q1 = torch.randn(B * H, T, K, dtype=torch.float32, device=DEVICE)\n",
      "    k1 = torch.randn(B * H, T, K, dtype=torch.float32, device=DEVICE)\n",
      "    do1 = torch.randn(B * H, T, V, dtype=torch.float32, device=DEVICE)\n",
      "    scale = 0.5\n",
      "    dv1 = chunk_delta_rule_fwd_kernel_prepare_dv(q1, k1, do1, scale, T, H, K, V, BT, BK, BV,\n",
      "                                                            offsets=None, indices=None, HEAD_FIRST=True)\n",
      "    results['HEAD_FIRST_true_no_offsets'] = dv1\n",
      "\n",
      "    # Test Case 2: HEAD_FIRST=True, with offsets\n",
      "    offsets = torch.tensor([0, 4, 4, 8], dtype=torch.int32, device=DEVICE)\n",
      "    indices = torch.tensor([0, 0, 1, 0], dtype=torch.int32, device=DEVICE)\n",
      "    dv2 = chunk_delta_rule_fwd_kernel_prepare_dv(q1, k1, do1, scale, T, H, K, V, BT, BK, BV,\n",
      "                                                            offsets=offsets, indices=indices, HEAD_FIRST=True)\n",
      "    results['HEAD_FIRST_true_with_offsets'] = dv2\n",
      "\n",
      "    # Test Case 3: HEAD_FIRST=False, no offsets\n",
      "    B = 2\n",
      "    H = 3\n",
      "    q3 = torch.randn(B, H, T, K, dtype=torch.float32, device=DEVICE)\n",
      "    k3 = torch.randn(B, H, T, K, dtype=torch.float32, device=DEVICE)\n",
      "    do3 = torch.randn(B, H, T, V, dtype=torch.float32, device=DEVICE)\n",
      "    dv3 = chunk_delta_rule_fwd_kernel_prepare_dv(q3, k3, do3, scale, T, H, K, V, BT, BK, BV,\n",
      "                                                            offsets=None, indices=None, HEAD_FIRST=False)\n",
      "    results['HEAD_FIRST_false_no_offsets'] = dv3\n",
      "\n",
      "    # Test Case 4: HEAD_FIRST=False, with offsets\n",
      "    dv4 = chunk_delta_rule_fwd_kernel_prepare_dv(q3, k3, do3, scale, T, H, K, V, BT, BK, BV,\n",
      "                                                            offsets=offsets, indices=indices, HEAD_FIRST=False)\n",
      "    results['HEAD_FIRST_false_with_offsets'] = dv4\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "test_results = test_chunk_delta_rule_fwd_kernel_prepare_dv()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "chunk_abc_fwd\n",
      "====================================================================================================\n",
      "import torch\n",
      "import math\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def chunk_abc_fwd(q, k, z, h, scale, T, K, V, BT, BK, BV):\n",
      "    \"\"\"\n",
      "    Simulates the computation of the Triton kernel using pure PyTorch operations.\n",
      "\n",
      "    Inputs:\n",
      "      q: tensor of shape [B, T, K]\n",
      "      k: tensor of shape [B, K, T]\n",
      "      z: tensor of shape [B, T, V]\n",
      "      h: tensor of shape [B, T*K, V]\n",
      "      scale: scaling factor (float)\n",
      "      T, K, V, BT, BK, BV: integer dimensions\n",
      "\n",
      "    Outputs:\n",
      "      o: tensor of shape [B, T, V]\n",
      "      A_out: tensor of shape [B, T, BT]\n",
      "    \"\"\"\n",
      "    B = q.shape[0]\n",
      "    o = torch.empty_like(z, device=DEVICE)\n",
      "    A_out = torch.empty((B, T, BT), device=DEVICE, dtype=torch.float32)\n",
      "\n",
      "    # Create lower-triangular mask\n",
      "    m_s = (torch.arange(BT, device=DEVICE).unsqueeze(1) >= torch.arange(BT, device=DEVICE).unsqueeze(0)).float()\n",
      "\n",
      "    for i_bh in range(B):\n",
      "        num_t_blocks = T // BT\n",
      "        num_v_blocks = z.shape[2] // BV\n",
      "        for i_t in range(num_t_blocks):\n",
      "            t_start = i_t * BT\n",
      "            t_end = t_start + BT\n",
      "            for i_v in range(num_v_blocks):\n",
      "                v_start = i_v * BV\n",
      "                v_end = v_start + BV\n",
      "\n",
      "                b_o = torch.zeros((BT, BV), device=DEVICE, dtype=torch.float32)\n",
      "                b_A = torch.zeros((BT, BT), device=DEVICE, dtype=torch.float32)\n",
      "                num_k_blocks = (K + BK - 1) // BK  # equivalent to ceil(K/BK)\n",
      "                for i_k in range(num_k_blocks):\n",
      "                    k_start = i_k * BK\n",
      "                    k_end = min(k_start + BK, K)\n",
      "                    # Extract block from q and scale\n",
      "                    b_q = q[i_bh, t_start:t_end, k_start:k_end] * scale\n",
      "                    # Extract block from k\n",
      "                    b_k = k[i_bh, k_start:k_end, t_start:t_end]\n",
      "                    # For h, rows for the current time block start at i_t*K\n",
      "                    h_offset = i_t * K\n",
      "                    b_h = h[i_bh, h_offset + k_start:h_offset + k_end, v_start:v_end]\n",
      "\n",
      "                    b_o = b_o + torch.matmul(b_q, b_h)\n",
      "                    b_A = b_A + torch.matmul(b_q, b_k)\n",
      "\n",
      "                # Exponential scaling with b_z and b_zp\n",
      "                b_z = z[i_bh, t_start:t_end, v_start:v_end]\n",
      "                i_p = max(t_start - 1, 0)  # ensuring non-negative index\n",
      "                b_zp = z[i_bh, i_p, v_start:v_end]\n",
      "                b_o = b_o * torch.exp(b_zp.unsqueeze(0) - b_z)\n",
      "                o[i_bh, t_start:t_end, v_start:v_end] = b_o\n",
      "\n",
      "                if i_v == 0:\n",
      "                    b_A = b_A * m_s  # Apply lower-triangular mask\n",
      "                    A_out[i_bh, t_start:t_end, :BT] = b_A\n",
      "    return o, A_out\n",
      "\n",
      "########################\n",
      "# Integration & Testing\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_chunk_abc_fwd():\n",
      "    \"\"\"Test function for the PyTorch implementation. Uses two test cases and returns the results in a dictionary.\"\"\"\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "\n",
      "    # Test case 1\n",
      "    B = 1\n",
      "    T = 4\n",
      "    K = 4\n",
      "    V = 4\n",
      "    BT = 2\n",
      "    BK = 2\n",
      "    BV = 2\n",
      "    scale = 0.5\n",
      "\n",
      "    q = torch.randn(B, T, K, dtype=torch.float32, device=DEVICE)\n",
      "    k = torch.randn(B, K, T, dtype=torch.float32, device=DEVICE)\n",
      "    z = torch.randn(B, T, V, dtype=torch.float32, device=DEVICE)\n",
      "    h = torch.randn(B, T * K, V, dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    o, A_out = chunk_abc_fwd(q, k, z, h, scale, T, K, V, BT, BK, BV)\n",
      "    results[\"test_case_1_o_shape\"] = o.shape\n",
      "    results[\"test_case_1_A_out_shape\"] = A_out.shape\n",
      "    results[\"test_case_1_o\"] = o\n",
      "    results[\"test_case_1_A_out\"] = A_out\n",
      "\n",
      "    # Test case 2\n",
      "    B2 = 2\n",
      "    T2 = 6\n",
      "    K2 = 8\n",
      "    V2 = 4\n",
      "    BT2 = 3\n",
      "    BK2 = 4\n",
      "    BV2 = 2\n",
      "    scale2 = 1.0\n",
      "\n",
      "    q2 = torch.randn(B2, T2, K2, dtype=torch.float32, device=DEVICE)\n",
      "    k2 = torch.randn(B2, K2, T2, dtype=torch.float32, device=DEVICE)\n",
      "    z2 = torch.randn(B2, T2, V2, dtype=torch.float32, device=DEVICE)\n",
      "    h2 = torch.randn(B2, T2 * K2, V2, dtype=torch.float32, device=DEVICE)\n",
      "\n",
      "    o2, A_out2 = chunk_abc_fwd(q2, k2, z2, h2, scale2, T2, K2, V2, BT2, BK2, BV2)\n",
      "    results[\"test_case_2_o_shape\"] = o2.shape\n",
      "    results[\"test_case_2_A_out_shape\"] = A_out2.shape\n",
      "    results[\"test_case_2_o\"] = o2\n",
      "    results[\"test_case_2_A_out\"] = A_out2\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "# Execute tests and print the test_results dictionary\n",
      "\n",
      "test_results = test_chunk_abc_fwd()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_softmax\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_softmax(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    A pure PyTorch implementation of the softmax forward pass.\n",
      "    Mimics the behavior of the Triton kernel for numerical stability.\n",
      "\n",
      "    Args:\n",
      "        x (torch.Tensor): Input logits tensor.\n",
      "        dim (int): Dimension along which softmax is applied.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Tensor of softmax probabilities.\n",
      "    \"\"\"\n",
      "    # Compute maximum value along specified dimension for numerical stability\n",
      "    max_val, _ = torch.max(x, dim=dim, keepdim=True)\n",
      "    # Exponentiate shifted values\n",
      "    exp_x = torch.exp(x - max_val)\n",
      "    # Normalize by the sum of exponentials\n",
      "    sum_exp = torch.sum(exp_x, dim=dim, keepdim=True)\n",
      "    return exp_x / sum_exp\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_softmax():\n",
      "    \"\"\"Integrated tests for the PyTorch softmax implementation.\"\"\"\n",
      "    test_results = {}\n",
      "\n",
      "    # Test Case 1: 1D tensor\n",
      "    input_tensor1 = torch.tensor([1.0, 2.0, 3.0], device=DEVICE)\n",
      "    output1 = pytorch_softmax(input_tensor1, dim=0)\n",
      "    test_results['1D_tensor'] = output1\n",
      "    test_results['1D_sum'] = output1.sum().item()\n",
      "\n",
      "    # Test Case 2: 2D tensor (each row softmaxed along the last dimension)\n",
      "    input_tensor2 = torch.tensor([[1.0, 2.0, 3.0], [3.0, 2.0, 1.0]], device=DEVICE)\n",
      "    output2 = pytorch_softmax(input_tensor2, dim=1)\n",
      "    test_results['2D_tensor'] = output2\n",
      "    test_results['2D_row_sums'] = output2.sum(dim=1).tolist()\n",
      "\n",
      "    # Test Case 3: 2D tensor with negative values\n",
      "    input_tensor3 = torch.tensor([[-1.0, -2.0, -3.0], [-3.0, -2.0, -1.0]], device=DEVICE)\n",
      "    output3 = pytorch_softmax(input_tensor3, dim=1)\n",
      "    test_results['2D_negative'] = output3\n",
      "    test_results['2D_negative_row_sums'] = output3.sum(dim=1).tolist()\n",
      "\n",
      "    # Test Case 4: Higher dimensional tensor (3D example)\n",
      "    input_tensor4 = torch.randn(4, 5, 6, device=DEVICE)\n",
      "    output4 = pytorch_softmax(input_tensor4, dim=-1)\n",
      "    test_results['3D_tensor_shape'] = output4.shape\n",
      "    test_results['3D_last_dim_sums'] = torch.sum(output4, dim=-1)\n",
      "\n",
      "    # Test Case 5: Explicit CUDA test\n",
      "    input_tensor5 = torch.randn(10, 20, device=DEVICE)\n",
      "    output5 = pytorch_softmax(input_tensor5, dim=1)\n",
      "    test_results['CUDA_tensor_shape'] = output5.shape\n",
      "    test_results['CUDA_row_sums'] = torch.sum(output5, dim=1).cpu()\n",
      "    \n",
      "    return test_results\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "def run_pytorch_tests():\n",
      "    test_results = test_pytorch_softmax()\n",
      "    # Only printing the final test results dictionary for comparison purposes\n",
      "    print(test_results)\n",
      "    return test_results\n",
      "\n",
      "\n",
      "pytorch_test_results = run_pytorch_tests()\n",
      "\n",
      "****************************************************************************************************\n",
      "triton_red_fused_mv_0\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global Device Standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def triton_red_fused_mv_0(in0: torch.Tensor, in1: torch.Tensor, in2: torch.Tensor,\n",
      "                                  xnumel: int, rnumel: int) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs a fused matrix-vector operation with reduction matching the Triton kernel.\n",
      "\n",
      "    Parameters:\n",
      "      in0: 1D tensor used to compute a conditional index modifier.\n",
      "      in1: 1D tensor containing the data for the multiplication.\n",
      "      in2: 1D tensor acting as the vector multiplier.\n",
      "      xnumel: Total number of output elements.\n",
      "      rnumel: Reduction dimension size.\n",
      "\n",
      "    Returns:\n",
      "      Tensor of shape (xnumel, 1) with the computed reduced result.\n",
      "    \"\"\"\n",
      "    device = in0.device\n",
      "    # Create index vector for x\n",
      "    x_indices = torch.arange(xnumel, device=device, dtype=torch.int64)\n",
      "    a_idx = x_indices // rnumel\n",
      "    a = in0[a_idx].to(torch.int64)\n",
      "    # Adjust by 8 if value is negative\n",
      "    a_cond = torch.where(a < 0, a + 8, a)\n",
      "    b = x_indices % rnumel\n",
      "    \n",
      "    # Compute indices to gather elements from in1\n",
      "    r = torch.arange(rnumel, device=device, dtype=torch.int64)\n",
      "    indices = r.unsqueeze(0) + rnumel * (b.unsqueeze(1) + rnumel * a_cond.unsqueeze(1))\n",
      "    gathered = in1[indices].to(torch.float32)\n",
      "    vec = in2.to(torch.float32)\n",
      "    prod = gathered * vec.unsqueeze(0)\n",
      "    summed = prod.sum(dim=1, keepdim=True)\n",
      "    return summed\n",
      "\n",
      "\n",
      "########################\n",
      "# Integrated Testing Code\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_triton_red_fused_mv_0():\n",
      "    \"\"\"\n",
      "    Execute tests for the PyTorch implementation.\n",
      "\n",
      "    Returns:\n",
      "      A dictionary with test cases as keys containing the PyTorch result.\n",
      "    \"\"\"\n",
      "    test_results = {}\n",
      "    device = DEVICE\n",
      "\n",
      "    # ------------- Test Case 1 -------------\n",
      "    xnumel = 8\n",
      "    rnumel = 4\n",
      "    in0 = torch.tensor([-1, 2], device=device, dtype=torch.int64)\n",
      "    in2 = torch.ones(rnumel, device=device, dtype=torch.float32)\n",
      "\n",
      "    # Compute required size for in1\n",
      "    x_indices = torch.arange(xnumel, device=device, dtype=torch.int64)\n",
      "    a_idx = x_indices // rnumel\n",
      "    a = in0[a_idx]\n",
      "    a_cond = torch.where(a < 0, a + 8, a)\n",
      "    b = x_indices % rnumel\n",
      "    r = torch.arange(rnumel, device=device, dtype=torch.int64)\n",
      "    all_indices = r.unsqueeze(0) + rnumel * (b.unsqueeze(1) + rnumel * a_cond.unsqueeze(1))\n",
      "    size_in1 = int(all_indices.max().item()) + 1\n",
      "    in1 = torch.arange(size_in1, device=device, dtype=torch.int64)\n",
      "\n",
      "    out_pytorch = triton_red_fused_mv_0(in0, in1, in2, xnumel, rnumel)\n",
      "    test_results[\"test_case_1\"] = out_pytorch.cpu()\n",
      "\n",
      "    # ------------- Test Case 2 -------------\n",
      "    xnumel = 16\n",
      "    rnumel = 8\n",
      "    in0 = torch.tensor([3, -2], device=device, dtype=torch.int64)\n",
      "    torch.manual_seed(0)\n",
      "    in2 = torch.randn(rnumel, device=device, dtype=torch.float32)\n",
      "    x_indices = torch.arange(xnumel, device=device, dtype=torch.int64)\n",
      "    a_idx = x_indices // rnumel\n",
      "    a = in0[a_idx]\n",
      "    a_cond = torch.where(a < 0, a + 8, a)\n",
      "    b = x_indices % rnumel\n",
      "    r = torch.arange(rnumel, device=device, dtype=torch.int64)\n",
      "    all_indices = r.unsqueeze(0) + rnumel * (b.unsqueeze(1) + rnumel * a_cond.unsqueeze(1))\n",
      "    size_in1 = int(all_indices.max().item()) + 1\n",
      "    in1 = torch.randn(size_in1, device=device, dtype=torch.float32).to(torch.int64)\n",
      "\n",
      "    out_pytorch = triton_red_fused_mv_0(in0, in1, in2, xnumel, rnumel)\n",
      "    test_results[\"test_case_2\"] = out_pytorch.cpu()\n",
      "\n",
      "    # ------------- Test Case 3 -------------\n",
      "    xnumel = 4\n",
      "    rnumel = 4\n",
      "    in0 = torch.tensor([0], device=device, dtype=torch.int64)\n",
      "    in2 = torch.tensor([2.0, -1.0, 0.5, 3.0], device=device, dtype=torch.float32)\n",
      "    x_indices = torch.arange(xnumel, device=device, dtype=torch.int64)\n",
      "    a_idx = x_indices // rnumel\n",
      "    a = in0[a_idx]\n",
      "    a_cond = torch.where(a < 0, a + 8, a)\n",
      "    b = x_indices % rnumel\n",
      "    r = torch.arange(rnumel, device=device, dtype=torch.int64)\n",
      "    all_indices = r.unsqueeze(0) + rnumel * (b.unsqueeze(1) + rnumel * a_cond.unsqueeze(1))\n",
      "    size_in1 = int(all_indices.max().item()) + 1\n",
      "    in1 = torch.ones(size_in1, device=device, dtype=torch.int64)\n",
      "\n",
      "    out_pytorch = triton_red_fused_mv_0(in0, in1, in2, xnumel, rnumel)\n",
      "    test_results[\"test_case_3\"] = out_pytorch.cpu()\n",
      "\n",
      "    # ------------- Test Case 4 -------------\n",
      "    xnumel = 32\n",
      "    rnumel = 8\n",
      "    in0 = torch.randint(-5, 6, ((xnumel + rnumel - 1) // rnumel,), device=device, dtype=torch.int64)\n",
      "    in2 = torch.randn(rnumel, device=device, dtype=torch.float32)\n",
      "    x_indices = torch.arange(xnumel, device=device, dtype=torch.int64)\n",
      "    a_idx = x_indices // rnumel\n",
      "    a = in0[a_idx]\n",
      "    a_cond = torch.where(a < 0, a + 8, a)\n",
      "    b = x_indices % rnumel\n",
      "    r = torch.arange(rnumel, device=device, dtype=torch.int64)\n",
      "    all_indices = r.unsqueeze(0) + rnumel * (b.unsqueeze(1) + rnumel * a_cond.unsqueeze(1))\n",
      "    size_in1 = int(all_indices.max().item()) + 1\n",
      "    in1 = torch.randint(0, 20, (size_in1,), device=device, dtype=torch.int64)\n",
      "\n",
      "    out_pytorch = triton_red_fused_mv_0(in0, in1, in2, xnumel, rnumel)\n",
      "    test_results[\"test_case_4\"] = out_pytorch.cpu()\n",
      "    \n",
      "    return test_results\n",
      "\n",
      "\n",
      "# Execute tests and print the test_results dictionary\n",
      "\n",
      "test_results = test_triton_red_fused_mv_0()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "softmax_2d\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard: All tensors are explicitly allocated on DEVICE.\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def softmax_tiled(input: torch.Tensor, dim: int = 0) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Computes the softmax function along the specified dimension.\n",
      "    For numerical stability, subtract the maximum value along the given dimension.\n",
      "    \"\"\"\n",
      "    max_val, _ = torch.max(input, dim=dim, keepdim=True)\n",
      "    e = torch.exp(input - max_val)\n",
      "    sum_e = torch.sum(e, dim=dim, keepdim=True)\n",
      "    return e / sum_e\n",
      "\n",
      "\n",
      "def softmax_2d(input: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Computes softmax for a 2D tensor along the first dimension (rows).\n",
      "    \"\"\"\n",
      "    return softmax_tiled(input, dim=0)\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_softmax_2d():\n",
      "    results = {}\n",
      "    \n",
      "    # Test Case 1: Small 2D tensor\n",
      "    input_tensor_1 = torch.tensor([\n",
      "        [1.0, 2.0, 3.0],\n",
      "        [2.0, 4.0, 0.5]\n",
      "    ], device=DEVICE)\n",
      "    expected_1 = torch.softmax(input_tensor_1, dim=0)\n",
      "    output_1 = softmax_2d(input_tensor_1)\n",
      "    results[\"test_case_1\"] = {\n",
      "        \"input\": input_tensor_1,\n",
      "        \"output\": output_1,\n",
      "        \"expected\": expected_1,\n",
      "        \"close\": torch.allclose(output_1, expected_1, atol=1e-6)\n",
      "    }\n",
      "    \n",
      "    # Test Case 2: 2D tensor with negative values\n",
      "    input_tensor_2 = torch.tensor([\n",
      "        [-1.0, -2.0, 0.0],\n",
      "        [-0.5, -1.5, 2.0]\n",
      "    ], device=DEVICE)\n",
      "    expected_2 = torch.softmax(input_tensor_2, dim=0)\n",
      "    output_2 = softmax_2d(input_tensor_2)\n",
      "    results[\"test_case_2\"] = {\n",
      "        \"input\": input_tensor_2,\n",
      "        \"output\": output_2,\n",
      "        \"expected\": expected_2,\n",
      "        \"close\": torch.allclose(output_2, expected_2, atol=1e-6)\n",
      "    }\n",
      "    \n",
      "    # Test Case 3: 3D tensor\n",
      "    input_tensor_3 = torch.tensor([\n",
      "        [[1.0, 2.0], [2.0, 0.5], [0.0, 1.0]],\n",
      "        [[-1.0, 3.0], [0.5, 0.0], [1.0, -2.0]]\n",
      "    ], device=DEVICE)  # shape (2, 3, 2)\n",
      "    expected_3 = torch.softmax(input_tensor_3, dim=1)  # softmax along dim=1 for each 2D slice\n",
      "    output_3_list = []\n",
      "    for m in range(input_tensor_3.size(0)):\n",
      "        # To mimic softmax along dim=1 using our softmax_tiled (which applies along dim=0),\n",
      "        # we transpose, apply softmax, then transpose back.\n",
      "        mat = input_tensor_3[m].transpose(0, 1)\n",
      "        softmaxed = softmax_tiled(mat, dim=1)\n",
      "        output_3_list.append(softmaxed.transpose(0, 1))\n",
      "    output_3 = torch.stack(output_3_list, dim=0)\n",
      "    results[\"test_case_3\"] = {\n",
      "        \"input\": input_tensor_3,\n",
      "        \"output\": output_3,\n",
      "        \"expected\": expected_3,\n",
      "        \"close\": torch.allclose(output_3, expected_3, atol=1e-6)\n",
      "    }\n",
      "    \n",
      "    # Test Case 4: Larger 2D tensor\n",
      "    input_tensor_4 = torch.randn(64, 128, device=DEVICE)\n",
      "    expected_4 = torch.softmax(input_tensor_4, dim=0)\n",
      "    output_4 = softmax_2d(input_tensor_4)\n",
      "    results[\"test_case_4\"] = {\n",
      "        \"input_shape\": input_tensor_4.shape,\n",
      "        \"close\": torch.allclose(output_4, expected_4, atol=1e-6)\n",
      "    }\n",
      "    \n",
      "    return results\n",
      "\n",
      "########################\n",
      "# Run tests and print only the final test_results dictionary.\n",
      "test_results = test_softmax_2d()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "pytorch_chunk_global_reversed_cumsum_vector\n",
      "====================================================================================================\n",
      "import math\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def pytorch_chunk_global_reversed_cumsum_vector(s: torch.Tensor, offsets: torch.Tensor = None, HEAD_FIRST: bool = True) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Computes the global reversed cumulative sum using PyTorch.\n",
      "    \n",
      "    For HEAD_FIRST mode, offsets (if provided) are ignored to match Triton behavior.\n",
      "    \n",
      "    Parameters:\n",
      "      s (torch.Tensor): input tensor.\n",
      "      offsets (torch.Tensor, optional): segmentation offsets (ignored if HEAD_FIRST is True).\n",
      "      HEAD_FIRST (bool): layout flag.\n",
      "      \n",
      "    Returns:\n",
      "      torch.Tensor: tensor with the reversed cumulative sum.\n",
      "    \"\"\"\n",
      "    if HEAD_FIRST:\n",
      "        return torch.flip(torch.cumsum(torch.flip(s, dims=[1]), dim=1), dims=[1])\n",
      "    else:\n",
      "        if offsets is None:\n",
      "            return torch.flip(torch.cumsum(torch.flip(s, dims=[2]), dim=2), dims=[2])\n",
      "        else:\n",
      "            B, H, T_max, S = s.shape\n",
      "            z = torch.zeros_like(s)\n",
      "            offsets = offsets.to(dtype=torch.int64)\n",
      "            for b in range(B):\n",
      "                bos = int(offsets[b].item())\n",
      "                eos = int(offsets[b+1].item())\n",
      "                if bos < 0 or eos > T_max or bos >= eos:\n",
      "                    continue\n",
      "                valid = s[b, :, bos:eos, :]\n",
      "                rcumsum = torch.flip(torch.cumsum(torch.flip(valid, dims=[1]), dim=1), dims=[1])\n",
      "                z[b, :, bos:eos, :] = rcumsum\n",
      "            return z\n",
      "\n",
      "\n",
      "########################\n",
      "# Integrated Test Cases for PyTorch Implementation\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_pytorch_chunk_global_reversed_cumsum_vector():\n",
      "    test_results = {}\n",
      "    device = DEVICE\n",
      "\n",
      "    # Test 1: HEAD_FIRST True, no offsets\n",
      "    B, H, T, S = 2, 3, 5, 4\n",
      "    input_tensor_hf = torch.randn(B * H, T, S, device=device)\n",
      "    expected_hf = torch.flip(torch.cumsum(torch.flip(input_tensor_hf, dims=[1]), dim=1), dims=[1])\n",
      "    output_hf = pytorch_chunk_global_reversed_cumsum_vector(input_tensor_hf, offsets=None, HEAD_FIRST=True)\n",
      "    test_results[\"HEAD_FIRST_true_no_offsets\"] = torch.allclose(output_hf, expected_hf)\n",
      "\n",
      "    # Test 2: HEAD_FIRST False without offsets\n",
      "    B, H, T, S = 2, 3, 5, 4\n",
      "    input_tensor_nonhf = torch.randn(B, H, T, S, device=device)\n",
      "    expected_nonhf = torch.flip(torch.cumsum(torch.flip(input_tensor_nonhf, dims=[2]), dim=2), dims=[2])\n",
      "    output_nonhf = pytorch_chunk_global_reversed_cumsum_vector(input_tensor_nonhf, offsets=None, HEAD_FIRST=False)\n",
      "    test_results[\"HEAD_FIRST_false_no_offsets\"] = torch.allclose(output_nonhf, expected_nonhf)\n",
      "\n",
      "    # Test 3: HEAD_FIRST False with offsets\n",
      "    B, H, T_max, S = 3, 2, 8, 3\n",
      "    input_tensor_offsets = torch.randn(B, H, T_max, S, device=device)\n",
      "    offsets = torch.tensor([2, 7, 8, 6], device=device, dtype=torch.int32)\n",
      "    expected_offsets = torch.zeros_like(input_tensor_offsets)\n",
      "    for b in range(B):\n",
      "        bos = int(offsets[b].item())\n",
      "        eos = int(offsets[b+1].item())\n",
      "        if bos < 0 or eos > T_max or bos >= eos:\n",
      "            continue\n",
      "        valid = input_tensor_offsets[b, :, bos:eos, :]\n",
      "        rcumsum = torch.flip(torch.cumsum(torch.flip(valid, dims=[1]), dim=1), dims=[1])\n",
      "        expected_offsets[b, :, bos:eos, :] = rcumsum\n",
      "    output_offsets = pytorch_chunk_global_reversed_cumsum_vector(input_tensor_offsets, offsets=offsets, HEAD_FIRST=False)\n",
      "    test_results[\"HEAD_FIRST_false_with_offsets\"] = torch.allclose(output_offsets, expected_offsets)\n",
      "\n",
      "    # Test 4: HEAD_FIRST True with offsets provided (offsets are ignored)\n",
      "    try:\n",
      "        _ = pytorch_chunk_global_reversed_cumsum_vector(input_tensor_hf, offsets=torch.tensor([0, T], device=device), HEAD_FIRST=True)\n",
      "        test_results[\"HEAD_FIRST_true_offsets_error\"] = True\n",
      "    except ValueError:\n",
      "        test_results[\"HEAD_FIRST_true_offsets_error\"] = False\n",
      "\n",
      "    # Test 5: Known small tensor test for HEAD_FIRST True\n",
      "    x = torch.tensor([[1., 2., 3.]], device=device)\n",
      "    expected_known = torch.tensor([[6., 5., 3.]], device=device)\n",
      "    output_known = pytorch_chunk_global_reversed_cumsum_vector(x.unsqueeze(-1), HEAD_FIRST=True)\n",
      "    test_results[\"known_small_tensor\"] = torch.allclose(output_known.squeeze(-1), expected_known)\n",
      "    \n",
      "    return test_results\n",
      "\n",
      "\n",
      "# Run test cases and print the test_results dictionary\n",
      "\n",
      "test_results = test_pytorch_chunk_global_reversed_cumsum_vector()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "test_kernel\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_test_kernel(A: torch.Tensor, B: torch.Tensor, k: int, C: torch.Tensor = None) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Performs a tiled matrix multiplication and accumulates results into C.\n",
      "    A: shape (32, 32*k)\n",
      "    B: shape (32*k, 32)\n",
      "    C: optional; if None, a zero tensor is created on A.device.\n",
      "    For each tile i in range(k), the function computes:\n",
      "      A_tile = A[:, i*32:(i+1)*32]\n",
      "      B_tile = B[i*32:(i+1)*32, :]\n",
      "    and adds A_tile @ B_tile into the corresponding block in C.\n",
      "    \"\"\"\n",
      "    M, N = 32, 32\n",
      "    if A.shape[0] != M:\n",
      "        raise ValueError(f\"A should have {M} rows, got {A.shape[0]}\")\n",
      "    if B.shape[1] != N:\n",
      "        raise ValueError(f\"B should have {N} columns, got {B.shape[1]}\")\n",
      "    if A.shape[1] < k * N:\n",
      "        raise ValueError(f\"A should have at least {k * N} columns, got {A.shape[1]}\")\n",
      "    if B.shape[0] < k * N:\n",
      "        raise ValueError(f\"B should have at least {k * N} rows, got {B.shape[0]}\")\n",
      "\n",
      "    if C is None:\n",
      "        C = torch.zeros((M, k * N), dtype=A.dtype, device=A.device)\n",
      "    else:\n",
      "        if C.shape[0] != M or C.shape[1] < k * N:\n",
      "            raise ValueError(f\"C should have shape (32, at least {k * N}), got {C.shape}\")\n",
      "\n",
      "    # Loop over tiles: perform multiplication and accumulation\n",
      "    for i in range(k):\n",
      "        A_tile = A[:, i * N:(i + 1) * N]\n",
      "        B_tile = B[i * N:(i + 1) * N, :]\n",
      "        X = A_tile @ B_tile\n",
      "        C[:, i * N:(i + 1) * N] += X\n",
      "\n",
      "    return C\n",
      "\n",
      "########################\n",
      "# Integration & Testing\n",
      "########################\n",
      "\n",
      "def test_test_kernel_comparison():\n",
      "    \"\"\"\n",
      "    Executes tests for the PyTorch implementation.\n",
      "    The results are stored in a dictionary with identical keys for PyTorch and Triton\n",
      "    to enable easy side-by-side comparisons.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "\n",
      "    # Test Case 1: Single tile (k=1)\n",
      "    k = 1\n",
      "    A1 = torch.randn(32, 32 * k, device=DEVICE)\n",
      "    B1 = torch.randn(32 * k, 32, device=DEVICE)\n",
      "    C1 = torch.zeros(32, 32 * k, device=DEVICE)\n",
      "    out = test_test_kernel(A1, B1, k, C1.clone())\n",
      "    results['test_case_1'] = {\n",
      "        'pytorch': {'output': out, 'match_ground_truth': True},\n",
      "        'triton': {'output': out, 'match_ground_truth': True}\n",
      "    }\n",
      "\n",
      "    # Test Case 2: Multiple tiles (k=2)\n",
      "    k = 2\n",
      "    A2 = torch.randn(32, 32 * k, device=DEVICE)\n",
      "    B2 = torch.randn(32 * k, 32, device=DEVICE)\n",
      "    C2 = torch.zeros(32, 32 * k, device=DEVICE)\n",
      "    out = test_test_kernel(A2, B2, k, C2.clone())\n",
      "    results['test_case_2'] = {\n",
      "        'pytorch': {'output': out, 'match_ground_truth': True},\n",
      "        'triton': {'output': out, 'match_ground_truth': True}\n",
      "    }\n",
      "\n",
      "    # Test Case 3: Non-zero initial C (k=2)\n",
      "    k = 2\n",
      "    A3 = torch.randn(32, 32 * k, device=DEVICE)\n",
      "    B3 = torch.randn(32 * k, 32, device=DEVICE)\n",
      "    C3_init = torch.randn(32, 32 * k, device=DEVICE)  # non-zero initial C\n",
      "    out = test_test_kernel(A3, B3, k, C3_init.clone())\n",
      "    results['test_case_3'] = {\n",
      "        'pytorch': {'output': out, 'match_ground_truth': True},\n",
      "        'triton': {'output': out, 'match_ground_truth': True}\n",
      "    }\n",
      "\n",
      "    # Test Case 4: Larger k value (k=4)\n",
      "    k = 4\n",
      "    A4 = torch.randn(32, 32 * k, device=DEVICE)\n",
      "    B4 = torch.randn(32 * k, 32, device=DEVICE)\n",
      "    C4 = torch.zeros(32, 32 * k, device=DEVICE)\n",
      "    out = test_test_kernel(A4, B4, k, C4.clone())\n",
      "    results['test_case_4'] = {\n",
      "        'pytorch': {'output': out, 'match_ground_truth': True},\n",
      "        'triton': {'output': out, 'match_ground_truth': True}\n",
      "    }\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "test_results = test_test_kernel_comparison()\n",
      "\n",
      "# Only print the final test_results dictionary\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "jagged_sum\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def jagged_sum(input_values: torch.Tensor, offsets: torch.Tensor, M: int) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Computes a jagged sum over sub-arrays stored in a flattened 1D tensor using PyTorch operations.\n",
      "    Parameters:\n",
      "      input_values (torch.Tensor): 1D tensor with concatenated values.\n",
      "      offsets (torch.Tensor): 1D tensor with batch boundary offsets.\n",
      "      M (int): Number of columns per row; each batch should have a number of elements divisible by M.\n",
      "    Returns:\n",
      "      torch.Tensor: 2D tensor where each row is the column-wise sum for the batch.\n",
      "    \"\"\"\n",
      "    if offsets.dim() != 1:\n",
      "        raise ValueError('offsets must be a 1D tensor')\n",
      "    if input_values.dim() != 1:\n",
      "        raise ValueError('input_values must be a 1D tensor')\n",
      "\n",
      "    num_batches = offsets.numel() - 1\n",
      "    outputs = []\n",
      "    \n",
      "    for i in range(num_batches):\n",
      "        start = int(offsets[i].item())\n",
      "        end = int(offsets[i+1].item())\n",
      "        # Handle empty batch: return zeros when no elements are found\n",
      "        if end <= start:\n",
      "            outputs.append(torch.zeros(M, device=DEVICE, dtype=torch.float32))\n",
      "        else:\n",
      "            num_elements = end - start\n",
      "            if num_elements % M != 0:\n",
      "                raise ValueError(f'For batch {i}, the number of elements ({num_elements}) is not a multiple of M={M}')\n",
      "            sub_array = input_values[start:end].reshape(-1, M)\n",
      "            outputs.append(sub_array.sum(dim=0))\n",
      "    \n",
      "    return torch.stack(outputs, dim=0)\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_jagged_sum():\n",
      "    results = {}\n",
      "\n",
      "    # Test Case 1: Two batches with M=3\n",
      "    input_values = torch.tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.], dtype=torch.float32, device=DEVICE)\n",
      "    offsets = torch.tensor([0, 6, 9], dtype=torch.int32, device=DEVICE)\n",
      "    M = 3\n",
      "    output = jagged_sum(input_values, offsets, M)\n",
      "    results['test_case_1'] = output.cpu()\n",
      "\n",
      "    # Test Case 2: Three batches with M=4\n",
      "    batch0 = torch.tensor([1, 2, 3, 4], dtype=torch.float32, device=DEVICE)\n",
      "    batch1 = torch.tensor([5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], dtype=torch.float32, device=DEVICE)\n",
      "    batch2 = torch.tensor([17, 18, 19, 20, 21, 22, 23, 24], dtype=torch.float32, device=DEVICE)\n",
      "    input_values = torch.cat([batch0, batch1, batch2])\n",
      "    offsets = torch.tensor([\n",
      "        0,\n",
      "        batch0.numel(),\n",
      "        batch0.numel() + batch1.numel(),\n",
      "        batch0.numel() + batch1.numel() + batch2.numel()\n",
      "    ], dtype=torch.int32, device=DEVICE)\n",
      "    M = 4\n",
      "    output = jagged_sum(input_values, offsets, M)\n",
      "    results['test_case_2'] = output.cpu()\n",
      "\n",
      "    # Test Case 3: Empty batch edge case with M=4\n",
      "    batch0 = torch.tensor([10, 20, 30, 40], dtype=torch.float32, device=DEVICE)\n",
      "    batch1 = torch.tensor([], dtype=torch.float32, device=DEVICE)\n",
      "    batch2 = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8], dtype=torch.float32, device=DEVICE)\n",
      "    input_values = torch.cat([batch0, batch1, batch2])\n",
      "    offsets = torch.tensor([\n",
      "        0,\n",
      "        batch0.numel(),\n",
      "        batch0.numel() + batch1.numel(),\n",
      "        batch0.numel() + batch1.numel() + batch2.numel()\n",
      "    ], dtype=torch.int32, device=DEVICE)\n",
      "    M = 4\n",
      "    output = jagged_sum(input_values, offsets, M)\n",
      "    results['test_case_3'] = output.cpu()\n",
      "\n",
      "    # Test Case 4: Error case when batch length is not a multiple of M\n",
      "    try:\n",
      "        input_values = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32, device=DEVICE)\n",
      "        offsets = torch.tensor([0, 5], dtype=torch.int32, device=DEVICE)\n",
      "        M = 3\n",
      "        output = jagged_sum(input_values, offsets, M)\n",
      "        results['test_case_4'] = output.cpu()\n",
      "    except Exception as e:\n",
      "        results['test_case_4'] = str(e)\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "test_results = test_jagged_sum()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "rms_norm_backward_torch_impl\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def rms_norm_backward_torch_impl(output_grad: torch.Tensor,\n",
      "                                   inp: torch.Tensor,\n",
      "                                   inv_rms: torch.Tensor,\n",
      "                                   weight: torch.Tensor = None,\n",
      "                                   scale_by_weight: bool = False):\n",
      "    \"\"\"\n",
      "    PyTorch implementation of the RMSNorm backward pass.\n",
      "\n",
      "    Args:\n",
      "        output_grad: (batch, feat) tensor.\n",
      "        inp: (batch, feat) input tensor.\n",
      "        inv_rms: (batch,) tensor containing inverse RMS values.\n",
      "        weight: (feat,) weight vector if scaling is applied.\n",
      "        scale_by_weight: Flag to indicate scaling by weight.\n",
      "\n",
      "    Returns:\n",
      "        Tuple: (input_grad, weight_grad [if applicable]).\n",
      "    \"\"\"\n",
      "    # Ensure precision by converting to float32\n",
      "    output_grad = output_grad.to(torch.float32)\n",
      "    inp = inp.to(torch.float32)\n",
      "    inv_rms = inv_rms.to(torch.float32)\n",
      "\n",
      "    batch_size, feat_dim = inp.shape\n",
      "    pre_lin = inp * inv_rms.unsqueeze(1)\n",
      "\n",
      "    if scale_by_weight:\n",
      "        if weight is None:\n",
      "            raise ValueError(\"Weight tensor must be provided when scale_by_weight is True.\")\n",
      "        weight_output_grad_prod = output_grad * weight.unsqueeze(0)\n",
      "    else:\n",
      "        weight_output_grad_prod = output_grad\n",
      "\n",
      "    sum_prod = torch.sum(inp * weight_output_grad_prod, dim=1, keepdim=True)\n",
      "    term1 = inp * sum_prod\n",
      "    term2 = (inv_rms.unsqueeze(1)) ** 2\n",
      "    input_grad = inv_rms.unsqueeze(1) * (weight_output_grad_prod - (term1 * term2) / feat_dim)\n",
      "\n",
      "    if scale_by_weight:\n",
      "        weight_grad = torch.sum(output_grad * pre_lin, dim=0)\n",
      "        return input_grad, weight_grad\n",
      "    else:\n",
      "        return input_grad\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_rms_norm_backward_torch_impl():\n",
      "    \"\"\"\n",
      "    Integrated tests for the PyTorch RMSNorm backward implementation.\n",
      "    All tensors are created on the global DEVICE.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "    torch.manual_seed(0)\n",
      "\n",
      "    # Test Case 1: Small tensor without scaling (batch=4, feat=8)\n",
      "    batch_size, feat_dim = 4, 8\n",
      "    inp = torch.randn(batch_size, feat_dim, device=DEVICE)\n",
      "    output_grad = torch.randn(batch_size, feat_dim, device=DEVICE)\n",
      "    rms = torch.sqrt(torch.mean(inp ** 2, dim=1) + 1e-6)\n",
      "    inv_rms = 1.0 / rms\n",
      "    input_grad = rms_norm_backward_torch_impl(output_grad, inp, inv_rms, scale_by_weight=False)\n",
      "    results[\"test_case_1\"] = {\"input_grad\": input_grad}\n",
      "\n",
      "    # Test Case 2: Small tensor with scaling by weight\n",
      "    weight = torch.randn(feat_dim, device=DEVICE)\n",
      "    input_grad, weight_grad = rms_norm_backward_torch_impl(output_grad, inp, inv_rms,\n",
      "                                                             weight=weight, scale_by_weight=True)\n",
      "    results[\"test_case_2\"] = {\"input_grad\": input_grad, \"weight_grad\": weight_grad}\n",
      "\n",
      "    # Test Case 3: Larger tensor without scaling (batch=16, feat=32)\n",
      "    batch_size, feat_dim = 16, 32\n",
      "    inp = torch.randn(batch_size, feat_dim, device=DEVICE)\n",
      "    output_grad = torch.randn(batch_size, feat_dim, device=DEVICE)\n",
      "    rms = torch.sqrt(torch.mean(inp ** 2, dim=1) + 1e-6)\n",
      "    inv_rms = 1.0 / rms\n",
      "    input_grad = rms_norm_backward_torch_impl(output_grad, inp, inv_rms, scale_by_weight=False)\n",
      "    results[\"test_case_3\"] = {\"input_grad\": input_grad}\n",
      "\n",
      "    # Test Case 4: Larger tensor with scaling by weight (batch=16, feat=32)\n",
      "    weight = torch.randn(feat_dim, device=DEVICE)\n",
      "    input_grad, weight_grad = rms_norm_backward_torch_impl(output_grad, inp, inv_rms,\n",
      "                                                             weight=weight, scale_by_weight=True)\n",
      "    results[\"test_case_4\"] = {\"input_grad\": input_grad, \"weight_grad\": weight_grad}\n",
      "\n",
      "    # Test Case 5: Edge case with a single batch element (batch=1, feat=10)\n",
      "    batch_size, feat_dim = 1, 10\n",
      "    inp = torch.randn(batch_size, feat_dim, device=DEVICE)\n",
      "    output_grad = torch.randn(batch_size, feat_dim, device=DEVICE)\n",
      "    rms = torch.sqrt(torch.mean(inp ** 2, dim=1) + 1e-6)\n",
      "    inv_rms = 1.0 / rms\n",
      "    input_grad = rms_norm_backward_torch_impl(output_grad, inp, inv_rms, scale_by_weight=False)\n",
      "    results[\"test_case_5\"] = {\"input_grad\": input_grad}\n",
      "\n",
      "    return results\n",
      "\n",
      "########################\n",
      "\n",
      "test_results = test_rms_norm_backward_torch_impl()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "torch_random_matrix\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def torch_random_matrix(seed: int, M: int, K: int, N: int,\n",
      "                        stride_dm: int = None, stride_dk: int = None, stride_dn: int = None) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Generate a random binary matrix using PyTorch operations.\n",
      "    The output is a contiguous tensor of shape (M, K, N) with int8 values.\n",
      "\n",
      "    Note: Strides are provided for API consistency but do not affect the output.\n",
      "    \"\"\"\n",
      "    if stride_dm is None:\n",
      "        stride_dm = K * N\n",
      "    if stride_dk is None:\n",
      "        stride_dk = N\n",
      "    if stride_dn is None:\n",
      "        stride_dn = 1\n",
      "    \n",
      "    # Create a generator on DEVICE to ensure reproducibility on the CUDA device.\n",
      "    gen = torch.Generator(device=DEVICE)\n",
      "    gen.manual_seed(seed)\n",
      "    rand_tensor = torch.rand(M, K, N, generator=gen, device=DEVICE)\n",
      "    binary_matrix = (rand_tensor > 0.5).to(torch.int8)\n",
      "    return binary_matrix\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_torch_random_matrix():\n",
      "    test_results = {}\n",
      "    # Test case 1: Small matrix test\n",
      "    seed = 42\n",
      "    M, K, N = 4, 5, 3\n",
      "    result1 = torch_random_matrix(seed, M, K, N)\n",
      "    test_results['small_matrix'] = result1.cpu().numpy()\n",
      "\n",
      "    # Test case 2: Reproducibility test\n",
      "    result2_a = torch_random_matrix(seed, M, K, N)\n",
      "    result2_b = torch_random_matrix(seed, M, K, N)\n",
      "    test_results['reproducibility'] = torch.equal(result2_a, result2_b)\n",
      "\n",
      "    # Test case 3: Different seeds produce different matrices\n",
      "    result3_a = torch_random_matrix(100, M, K, N)\n",
      "    result3_b = torch_random_matrix(200, M, K, N)\n",
      "    test_results['different_seeds'] = not torch.equal(result3_a, result3_b)\n",
      "\n",
      "    # Test case 4: Larger matrix test\n",
      "    M_large, K_large, N_large = 32, 64, 16\n",
      "    result4 = torch_random_matrix(123, M_large, K_large, N_large)\n",
      "    test_results['large_matrix_shape'] = result4.shape\n",
      "\n",
      "    # Test case 5: Strides provided do not affect the output\n",
      "    result5 = torch_random_matrix(seed, M, K, N, stride_dm=K * N, stride_dk=N, stride_dn=1)\n",
      "    test_results['strides_ignored'] = torch.equal(result1, result5)\n",
      "\n",
      "    return test_results\n",
      "\n",
      "########################\n",
      "\n",
      "# Execute tests and print the test_results dictionary.\n",
      "torch_test_results = test_torch_random_matrix()\n",
      "print(torch_test_results)\n",
      "\n",
      "****************************************************************************************************\n",
      "relu_linear_add_backward\n",
      "====================================================================================================\n",
      "import torch\n",
      "\n",
      "# Global device setting\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "\n",
      "def relu_linear_add_backward(input_tensor: torch.Tensor,\n",
      "                               grad_tensor: torch.Tensor,\n",
      "                               weight: torch.Tensor):\n",
      "    \"\"\"\n",
      "    Computes the backward pass for a fused linear+ReLU layer.\n",
      "    Given:\n",
      "      input_tensor: (N, in_channels) pre-activation input,\n",
      "      grad_tensor:  (N, out_channels) gradient from the next layer,\n",
      "      weight:       (in_channels, out_channels) weight matrix.\n",
      "\n",
      "    Computes:\n",
      "      input_grad = (grad_tensor @ weight^T) * (input_tensor > 0)\n",
      "      weight_grad = (ReLU(input_tensor)^T @ grad_tensor)\n",
      "\n",
      "    Returns:\n",
      "      input_grad as float16 and weight_grad as float32.\n",
      "    \"\"\"\n",
      "    # Compute gradient with respect to the input using the ReLU derivative\n",
      "    relu_mask = (input_tensor > 0).to(grad_tensor.dtype)\n",
      "    input_grad = (grad_tensor @ weight.t()) * relu_mask\n",
      "    input_grad = input_grad.to(torch.float16)\n",
      "    \n",
      "    # Compute gradient with respect to the weight\n",
      "    input_relu = torch.relu(input_tensor).to(torch.float32)\n",
      "    weight_grad = input_relu.t() @ grad_tensor.to(torch.float32)\n",
      "    \n",
      "    return input_grad, weight_grad\n",
      "\n",
      "\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_relu_linear_add_backward():\n",
      "    test_results = {}\n",
      "\n",
      "    # Test Case 1: Small tensor with mixed values\n",
      "    input1 = torch.tensor([[-1.0, 2.0], [3.0, -4.0]], device=DEVICE, dtype=torch.float32)\n",
      "    grad1 = torch.tensor([[0.5, 1.0], [1.5, -0.5]], device=DEVICE, dtype=torch.float32)\n",
      "    weight1 = torch.tensor([[2.0, -1.0], [0.5, 3.0]], device=DEVICE, dtype=torch.float32)\n",
      "    ig1, wg1 = relu_linear_add_backward(input1, grad1, weight1)\n",
      "    test_results[\"test_case_1\"] = {\"input_grad\": ig1, \"weight_grad\": wg1}\n",
      "\n",
      "    # Test Case 2: Random large dimensions\n",
      "    torch.manual_seed(0)\n",
      "    N, in_channels, out_channels = 8, 4, 5\n",
      "    input2 = torch.randn(N, in_channels, device=DEVICE, dtype=torch.float32)\n",
      "    grad2 = torch.randn(N, out_channels, device=DEVICE, dtype=torch.float32)\n",
      "    weight2 = torch.randn(in_channels, out_channels, device=DEVICE, dtype=torch.float32)\n",
      "    ig2, wg2 = relu_linear_add_backward(input2, grad2, weight2)\n",
      "    test_results[\"test_case_2\"] = {\"input_grad_shape\": ig2.shape,\n",
      "                                   \"weight_grad_shape\": wg2.shape}\n",
      "\n",
      "    # Test Case 3: All negative inputs: input_grad should be zero\n",
      "    input3 = -torch.abs(torch.randn(3, 3, device=DEVICE, dtype=torch.float32))\n",
      "    grad3 = torch.randn(3, 2, device=DEVICE, dtype=torch.float32)\n",
      "    weight3 = torch.randn(3, 2, device=DEVICE, dtype=torch.float32)\n",
      "    ig3, wg3 = relu_linear_add_backward(input3, grad3, weight3)\n",
      "    test_results[\"test_case_3\"] = {\"input_grad\": ig3, \"weight_grad\": wg3}\n",
      "\n",
      "    # Test Case 4: Numerical verification using autograd\n",
      "    input4 = torch.tensor([[1.0, -0.5], [2.0, 3.0]], device=DEVICE, \n",
      "                           dtype=torch.float32, requires_grad=True)\n",
      "    weight4 = torch.tensor([[0.5, -1.0], [2.0, 1.0]], device=DEVICE, \n",
      "                            dtype=torch.float32, requires_grad=True)\n",
      "    output4 = torch.relu(input4 @ weight4)\n",
      "    loss = output4.sum()\n",
      "    loss.backward()\n",
      "    # Manual computation (using detached tensors)\n",
      "    grad_manual = torch.ones_like(output4, device=DEVICE, dtype=torch.float32)\n",
      "    ig4, wg4 = relu_linear_add_backward(input4.detach(), grad_manual, weight4.detach())\n",
      "    test_results[\"test_case_4\"] = {\"autograd_input_grad\": input4.grad,\n",
      "                                   \"manual_input_grad\": ig4,\n",
      "                                   \"autograd_weight_grad\": weight4.grad,\n",
      "                                   \"manual_weight_grad\": wg4}\n",
      "\n",
      "    return test_results\n",
      "\n",
      "\n",
      "# Run tests and print the results\n",
      "\n",
      "test_results = test_relu_linear_add_backward()\n",
      "print(test_results)\n",
      "****************************************************************************************************\n",
      "attn_fwd_\n",
      "====================================================================================================\n",
      "import math\n",
      "import torch\n",
      "\n",
      "# Global device standard\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "def attn_fwd_(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, sm_scale: float) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Pure PyTorch implementation of the scaled dot-product attention.\n",
      "    Computes:\n",
      "      scores = (Q @ K^T) * (sm_scale * 1.44269504),\n",
      "      attn = softmax(scores), and output = attn @ V.\n",
      "    Args:\n",
      "      Q (torch.Tensor): Query tensor of shape (B, H, N, D).\n",
      "      K (torch.Tensor): Key tensor of shape (B, H, N, D).\n",
      "      V (torch.Tensor): Value tensor of shape (B, H, N, D).\n",
      "      sm_scale (float): Scale factor applied before softmax.\n",
      "    Returns:\n",
      "      torch.Tensor: The attention output tensor.\n",
      "    \"\"\"\n",
      "    qk_scale = sm_scale * 1.44269504\n",
      "    scores = torch.matmul(Q, K.transpose(-2, -1)) * qk_scale\n",
      "    attn = torch.softmax(scores, dim=-1)\n",
      "    output = torch.matmul(attn, V)\n",
      "    return output\n",
      "\n",
      "########################\n",
      "# Integrated Testing Function\n",
      "########################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "def test_attn_fwd_():\n",
      "    \"\"\"\n",
      "    Runs tests for the PyTorch attention forward implementation.\n",
      "    Returns:\n",
      "      dict: A dictionary containing the test outputs for each test case.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "    device = DEVICE\n",
      "\n",
      "    # Test case 1: Small tensor test (batch=1, head=1, sequence length=4, d_model=8)\n",
      "    batch, heads, seq, d = 1, 1, 4, 8\n",
      "    Q = torch.randn(batch, heads, seq, d, device=device)\n",
      "    K = torch.randn(batch, heads, seq, d, device=device)\n",
      "    V = torch.randn(batch, heads, seq, d, device=device)\n",
      "    sm_scale = 0.125\n",
      "    results[\"test_case_1\"] = attn_fwd_(Q, K, V, sm_scale)\n",
      "\n",
      "    # Test case 2: Different batch and head sizes (batch=2, head=4, sequence length=6, d_model=16)\n",
      "    batch, heads, seq, d = 2, 4, 6, 16\n",
      "    Q = torch.randn(batch, heads, seq, d, device=device)\n",
      "    K = torch.randn(batch, heads, seq, d, device=device)\n",
      "    V = torch.randn(batch, heads, seq, d, device=device)\n",
      "    sm_scale = 0.2\n",
      "    results[\"test_case_2\"] = attn_fwd_(Q, K, V, sm_scale)\n",
      "\n",
      "    # Test case 3: Larger sequence with moderate dimensions (batch=3, head=2, sequence length=64, d_model=32)\n",
      "    batch, heads, seq, d = 3, 2, 64, 32\n",
      "    Q = torch.randn(batch, heads, seq, d, device=device)\n",
      "    K = torch.randn(batch, heads, seq, d, device=device)\n",
      "    V = torch.randn(batch, heads, seq, d, device=device)\n",
      "    sm_scale = 0.05\n",
      "    results[\"test_case_3\"] = attn_fwd_(Q, K, V, sm_scale)\n",
      "\n",
      "    # Test case 4: Numerical stability test (identical Q and K for high correlation)\n",
      "    batch, heads, seq, d = 1, 1, 10, 4\n",
      "    Q = torch.randn(batch, heads, seq, d, device=device)\n",
      "    K = Q.clone()  # High correlation scenario\n",
      "    V = torch.ones(batch, heads, seq, d, device=device)\n",
      "    sm_scale = 0.1\n",
      "    results[\"test_case_4\"] = attn_fwd_(Q, K, V, sm_scale)\n",
      "\n",
      "    return {\"PyTorch\": results}\n",
      "\n",
      "########################\n",
      "# Run Tests (only printing the test_results dictionary)\n",
      "########################\n",
      "\n",
      "test_results = test_attn_fwd_()\n",
      "print(test_results)\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for row in fixed_ds:\n",
    "    print(row[\"entrypoint\"])\n",
    "    print(\"=\"*100)\n",
    "    print(row[\"pt_code_without_tests\"])\n",
    "    print(\"=\"*100)\n",
    "    print(row[\"tests\"])\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 847/847 [00:00<00:00, 9050.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing `test_unpack64` with `test_pytorch_unpack64`\n",
      "Replacing `test_custom_attention` with `test_custom_attention_forward`\n",
      "Replacing `test_fourth_order_fwd` with `test_torch_fourth_order_fwd`\n",
      "Replacing `test_layer_norm_fwd_fused` with `test_layer_norm_fwd_fused_`\n",
      "Replacing `test__single_query_cached_kv_attention_v2_torch` with `test_torch_single_query_cached_kv_attention_v2`\n",
      "Replacing `test_bwd_intra` with `test_pytorch_bwd_intra`\n",
      "Replacing `test_elementwise_mul` with `test_elementwise_mul_`\n",
      "Replacing `test_rms_norm_fwd` with `test_pytorch_rms_norm_fwd`\n",
      "Replacing `test_rotary_embedding` with `test_apply_rotary_embedding`\n",
      "Replacing `test_fused_chunk_delta_rule_bwd` with `test_fused_chunk_delta_rule_bwd_`\n",
      "Replacing `test_bwd_block` with `test_torch_bwd_block`\n",
      "Replacing `test_bwd_decay_global_cumsum` with `test_bwd_decay_global_cumsum_`\n",
      "Replacing `test_attn_fwd_inner_torch` with `test_torch_attn_fwd_inner`\n",
      "Replacing `test_atomic_kernel` with `test_atomic_kernel_`\n",
      "Replacing `test_cross_entropy_forward` with `test_cross_entropy_forward_`\n",
      "Replacing `test_softmax_kernel_online_v2_pytorch` with `test_softmax_online`\n",
      "Replacing `test_attn_fwd_inner` with `test_attn_fwd_inner_`\n",
      "Replacing `test_output_sparse_matmul_torch` with `test_torch_output_sparse_matmul`\n",
      "Replacing `test_dequantize` with `test_dequantize_`\n",
      "Replacing `test_rms_norm_bwd` with `test_rms_norm_bwd_`\n",
      "Replacing `test_update_ema` with `test_pytorch_update_ema`\n",
      "Replacing `test_tanh` with `test_pytorch_tanh`\n",
      "Replacing `test_dense_to_sparse` with `test_pytorch_dense_to_sparse`\n",
      "Replacing `test_dynamic_quantize` with `test_dynamic_quantize_`\n",
      "Replacing `test_paged_attn_wo_mma_v2_reduce` with `test_paged_attn_wo_mma_v2_reduce_`\n",
      "Replacing `test_bgmv_expand_slice` with `test_pytorch_bgmv_expand_slice`\n",
      "Replacing `test_pytorch_sum_kernel_1d_result_sum_then_buffer` with `test_sum_kernel_1d_result_sum_then_buffer`\n",
      "Replacing `test_sum_dim0_in_fp32` with `test_pytorch_sum_dim0_in_fp32`\n",
      "Replacing `test_forward` with `test_linear_forward`\n",
      "Replacing `test_matmul_AT_B` with `test_pytorch_matmul_AT_B`\n",
      "Replacing `test_chunk_transform_qk_fwd` with `test_chunk_transform_qk_fwd_`\n",
      "Replacing `test_associative_rnn_scan` with `test_associative_rnn_scan_fwd`\n",
      "Replacing `test_inner_paged_attn_unroll_8` with `test_pytorch_inner_paged_attn_unroll_8`\n",
      "Replacing `test_modulation_gate_proj` with `test_modulation_gate_proj_pure`\n",
      "Replacing `test_chunk_retention_fwd` with `test_chunk_retention_fwd_`\n",
      "Replacing `test_matmul` with `test_matmul_`\n",
      "Replacing `test_bwd_prepare_wy_repr` with `test_pytorch_bwd_prepare_wy_repr`\n",
      "Replacing `test_mlstm_matmul_kernel_backward` with `test_pytorch_mlstm_matmul_kernel_backward`\n",
      "Replacing `test_layer_norm_modulation_fwd` with `test_layer_norm_modulation_fwd_`\n",
      "Replacing `test_matmul_quant_kernel_pytorch` with `test_quantized_matmul`\n",
      "Replacing `test_attn_fwd_torch` with `test_torch_attn_fwd`\n",
      "Replacing `test_fused_moe` with `test_fused_moe_`\n",
      "Replacing `test_chunk_hgrn_bwd_o` with `test_chunk_hgrn_bwd_o_`\n",
      "Replacing `test_pytorch_chunk_delta_rule_fwd_kernel_prepare_dv` with `test_chunk_delta_rule_fwd_kernel_prepare_dv`\n",
      "Replacing `test_pytorch_chunk_abc_fwd_kernel_K` with `test_chunk_abc_fwd`\n",
      "Replacing `test_softmax_fwd` with `test_pytorch_softmax`\n",
      "Replacing `test_red_fused_mv_0_pytorch` with `test_triton_red_fused_mv_0`\n",
      "Replacing `test_softmax_torch` with `test_softmax_2d`\n",
      "Replacing `test_chunk_global_reversed_cumsum_vector` with `test_pytorch_chunk_global_reversed_cumsum_vector`\n",
      "Replacing `test_kernel` with `test_test_kernel`\n",
      "Replacing `test_pytorch_jagged_sum` with `test_jagged_sum`\n",
      "Replacing `test_rms_norm_backward_torch` with `test_rms_norm_backward_torch_impl`\n",
      "Replacing `test_random_matrix` with `test_torch_random_matrix`\n",
      "Replacing `test__ReLULinearAddBackward` with `test_relu_linear_add_backward`\n",
      "Replacing `test_attn_fwd` with `test_attn_fwd_`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"tcapelle/train_ds_triton\", split=\"train\")\n",
    "ds = ds.map(fix_test_entrypoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 20.14ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tcapelle/train_ds_triton/commit/2a56b9158791b972a3e25d9727cd135ae21c2bed', commit_message='Fix tests/entrypoint', commit_description='', oid='2a56b9158791b972a3e25d9727cd135ae21c2bed', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tcapelle/train_ds_triton', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tcapelle/train_ds_triton'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(\"tcapelle/train_ds_triton\", commit_message=\"Fix tests/entrypoint\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
