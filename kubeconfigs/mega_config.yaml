apiVersion: apps/v1
kind: Deployment
metadata:
  name: axolotl-vllm-server
  labels:
    app: axolotl-vllm
    task: serving # Task label for anti-affinity
spec:
  replicas: 1
  selector:
    matchLabels:
      app: axolotl-vllm
  template:
    metadata:
      labels:
        app: axolotl-vllm # Pods get this label
        task: serving     # And this task label
    spec:
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 500Gi # Adjust size as needed
      - name: config-volume
        configMap:
          name: axolotl-config
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: task # Avoid pods with these task labels
                operator: In
                values:
                - training
                - rewards
            topologyKey: "kubernetes.io/hostname" # Spread across different nodes
      # Uncomment and configure if your image is in a private registry:
      # imagePullSecrets:
      # - name: my-registry-secret
      containers:
      - name: axolotl-vllm-container
        image: ghcr.io/tcapelle/triton_eval:hack # <-- UPDATE THIS
        workingDir: /app/axolotl_dev
        command: ["axolotl"]
        args:
          - "vllm-serve"
          - "/mnt/axolotl-config/config_14b.yaml"
          - "--tensor-parallel-size"
          - "8" # Matches 8 GPUs
        resources:
          limits:
            nvidia.com/gpu: "8"
            cpu: "32"
            memory: "1000Gi"
          requests:
            cpu: "32"
            memory: "1000Gi"
            nvidia.com/gpu: "8"
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - name: config-volume
          mountPath: /mnt/axolotl-config
        If vllm-serve listens on a port (e.g., 8000) and you need to expose it:
        ports:
        - containerPort: 8000
          name: http-vllm
        - containerPort: 51216
          name: weight-updates
      # Add tolerations if your GPU nodes have taints
      # tolerations:
      # - key: "nvidia.com/gpu"
      #   operator: "Exists"
      #   effect: "NoSchedule"
      # Add nodeSelector if you need to target specific types of GPU nodes
      # nodeSelector:
      #   cloud.google.com/gke-accelerator: nvidia-tesla-t4 # Example for GKE 
---
apiVersion: v1
kind: Service
metadata:
  name: axolotl-vllm-service
spec:
  selector:
    app: axolotl-vllm # Matches the labels of the pods in your Deployment
  ports:
    - protocol: TCP
      port: 8000 # Port on which the service is exposed
      targetPort: 8000 # Port on the pod to forward traffic to (containerPort)
      name: http-vllm
    - protocol: TCP
      port: 51216
      targetPort: 51216
      name: weight-updates
  type: ClusterIP 

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: axolotl-rewards-server
  labels:
    app: axolotl-rewards
    task: rewards # Task label for anti-affinity
spec:
  replicas: 1
  selector:
    matchLabels:
      app: axolotl-rewards
  template:
    metadata:
      labels:
        app: axolotl-rewards # Pods get this label
        task: rewards        # And this task label
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: task # Avoid pods with these task labels
                operator: In
                values:
                - serving
                - training
            topologyKey: "kubernetes.io/hostname"
      # imagePullSecrets: ...
      containers:
      - name: axolotl-rewards-container
        image: ghcr.io/tcapelle/triton_eval:hack # <-- UPDATE THIS
        workingDir: /app/axolotl_dev
        command: ["python"]
        args:
          - "server.py"
        env:
          - name: TASK_TIMEOUT_SECONDS
            value: "240"
          - name: WORKER_JOIN_TIMEOUT
            value: "20"
        resources:
          limits:
            nvidia.com/gpu: "8"
            cpu: "32"
            memory: "1000Gi"
          requests:
            cpu: "32"
            memory: "1000Gi"
            nvidia.com/gpu: "8"
        ports:
        # README mentioned ssh -L 9347:localhost:9347 for rewards server
        - containerPort: 9347
          name: http-rewards
      # tolerations: ...
      # nodeSelector: ... 
---
apiVersion: v1
kind: Service
metadata:
  name: axolotl-rewards-service
spec:
  selector:
    app: axolotl-rewards # Matches the labels of the pods in your Deployment
  ports:
    - protocol: TCP
      port: 9347 # Port on which the service is exposed
      targetPort: 9347 # Port on the pod to forward traffic to (containerPort)
  type: ClusterIP 
---
apiVersion: batch/v1
kind: Job
metadata:
  name: axolotl-training-job # Consider generateName for multiple runs: axolotl-training-job-
  labels:
    app: axolotl-train
    task: training # Task label for anti-affinity
spec:
  template:
    metadata:
      labels:
        app: axolotl-train # Pods get this label
        task: training     # And this task label
    spec:
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 1600Gi # Adjust size as needed
      - name: config-volume
        configMap:
          name: axolotl-config
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: task # Avoid pods with these task labels
                operator: In
                values:
                - serving
                - rewards
            topologyKey: "kubernetes.io/hostname"
      # imagePullSecrets: ...
      containers:
      - name: axolotl-train-container
        image: ghcr.io/tcapelle/triton_eval:hack # <-- UPDATE THIS
        workingDir: /app/axolotl_dev
        command: ["axolotl"]
        args:
          - "train"
          - "/mnt/axolotl-config/config_14b.yaml"
          - "--deepspeed"
          - "deepspeed_configs/zero2.json"
        env:
        - name: WANDB_API_KEY
          valueFrom:
            secretKeyRef:
              name: wandb-api-key-secret # Name of the secret you created
              key: WANDB_API_KEY        # Key within the secret
        - name: TRITON_SERVER_URL
          value: "http://axolotl-rewards-service:9347"
        - name: WEAVE_PRINT_CALL_LINK
          value: "0"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token-secret # Name of the secret for HF_TOKEN
              key: HF_TOKEN        # Key within the hf-token-secret
        # You can add other environment variables here if needed
        # - name: ANOTHER_ENV_VAR
        #   value: "some_value"
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - name: config-volume
          mountPath: /mnt/axolotl-config
        resources:
          limits:
            nvidia.com/gpu: "8"
            cpu: "32"
            memory: "1600Gi"
          requests:
            cpu: "32"
            memory: "1600Gi"
            nvidia.com/gpu: "8"
      restartPolicy: OnFailure
      # tolerations: ...
      # nodeSelector: ...
  backoffLimit: 0 # No retries for the job on failure 