"Reverse the jit triton kernel to pytorch code"

import openai
from rich.console import Console
from rich.pretty import pprint
from datasets import load_dataset, load_from_disk
from pydantic import BaseModel, Field

from prompts import generate_pytorch_prompt, KernelInfo, description_prompt, PytorchCodeWithTestCases

console = Console()





# console.rule("[bold blue]Generating Descriptions[/bold blue]")

client = openai.OpenAI()

## Add some description to the dataset
######################################
# dataset_name = "GPUMODE/categorized_triton_data_permissive"
# dataset = load_dataset(dataset_name, split="train")

# def generate_description(row):
#     code = row["input"]
#     response = client.beta.chat.completions.parse(
#         model="gpt-4o",
#         messages=[{"role": "user", "content": description_prompt.format(code=code)}],
#         response_format=KernelInfo,
#     )
#     return {"description": response.choices[0].message.parsed.description}



# dataset = dataset.map(generate_description, num_proc=40)
# dataset.save_to_disk("annotated_dataset")

## Generate Pytorch Code
######################################

# dataset = load_from_disk("annotated_dataset")

# def generate_pytorch_code(row):
#     code = row["input"]
#     description = row["description"]
#     response = client.beta.chat.completions.parse(
#         model="o3-mini",
#         messages=[{"role": "user", "content": generate_pytorch_prompt.format(code=code, description=description)}],
#         response_format=PytorchCodeWithTestCases,
#     )
#     return {"pytorch_code_with_test_cases": response.choices[0].message.parsed.pytorch_code_with_test_cases}


# dataset = dataset.map(generate_pytorch_code, num_proc=40)
# dataset.save_to_disk("annotated_dataset_pt")
# dataset.push_to_hub("tcapelle/annotated_dataset_o3")

## Run code
######################################

# dataset = load_from_disk("annotated_dataset_pt")

# from tools import run_python_code

# def run_code(row):
#     pytorch_code_with_test_cases = row["pytorch_code_with_test_cases"]
#     out = run_python_code(pytorch_code_with_test_cases)
#     print(f"=============== Running code ==========================")
#     print(f"status: {out['status_code']}")
#     if out['status_code'] != 0:
#         print(f"error: {out['output']}")
#     return {"test_cpu_passing": out["status_code"] == 0, "test_cpu_output": out["output"]}

# # Only run first 10 examples
# # dataset = dataset.select(range(10))
# dataset = dataset.map(run_code, num_proc=12)
# dataset.save_to_disk("annotated_dataset_pt_tested")
# dataset.push_to_hub("tcapelle/annotated_dataset_o3")


## Fix with Agent
######################################

import weave
from pydantic import BaseModel
from my_smol_agent.agent import Agent
from my_smol_agent.tools import clear_temp_files

clear_temp_files()

system_prompt = """You are an expert PyTorch developer. Your goal is to fix the test cases for a given PyTorch function. The function code is correct, but the tests were generated by an LLM and may contain errors. Do not modify the function code—only adjust the tests.

# Instructions:
- Try running the code first, to see what the error is.
- Don't change the code, only the tests.
- Fix the tests, make sure that they are fixed and the logic of the tests is preserved.
- If a test cannot be fixed, remove it or create a simpler one.
- At the last step, return only the code with the fixed tests. It should be basically the same code as the original code, but with the tests fixed.

# Regarding the tests:
- Write a test code in Python for the above code. Ensure that all branch tests are in a single function starting with
“test_”, with no parameters.
- Particular attention should be paid to the fact that tensor parameters are of GPU type.
- Try to limit the number of branches to no more than 4.
- In branch tests, avoid modifying parameters that are later in the argument list with default values (especially if
they have out parameters, do not assign them).
- Store the results of all branch calculations in a dictionary, where the dictionary key is "test_case_n", with n
representing the test case number.
- Ensure that the import paths match exactly as described in the operator documentation to maintain accuracy.
- The code should run directly, without if __name__ == "__main__".
- Remember to run the code one last time to make sure the tests are fixed before returning the code.
- The tests are meant to be run on the GPU, so use device='cuda' when creating the tensors and when appropriate.
- Remove any unnecesary comments or commented out code.

IMPORTANT: Never change the code, only the tests.

# A perfect example would look like this:

```python
import torch

def add(input, other, alpha=1, out=None):
    \"\"\"
    Adds the tensor or number 'other', scaled by 'alpha', to the 'input' tensor.
    
    Args:
        input (Tensor): The input tensor.
        other (Tensor or Number): The tensor or number to add to input.
        alpha (Number, optional): The multiplier for 'other'. Default is 1.
        out (Tensor, optional): The output tensor. If provided, the result will be stored in this tensor.
        
    Returns:
        Tensor: The result of adding 'other' scaled by 'alpha' to 'input'.
    \"\"\"
    return torch.add(input, other, alpha=alpha, out=out)

##################################################################################################################################################


import torch

def test_add():
    results = {{}}

    # Test case 1: Adding two tensors with default alpha
    input1 = torch.tensor([1.0, 2.0, 3.0], device='cuda')
    other1 = torch.tensor([4.0, 5.0, 6.0], device='cuda')
    results["test_case_1"] = add(input1, other1)

    # Test case 2: Adding a tensor and a scalar with default alpha
    input2 = torch.tensor([1.0, 2.0, 3.0], device='cuda')
    other2 = 2.0
    results["test_case_2"] = add(input2, other2)

    # Test case 3: Adding two tensors with a specified alpha
    input3 = torch.tensor([1.0, 2.0, 3.0], device='cuda')
    other3 = torch.tensor([4.0, 5.0, 6.0], device='cuda')
    results["test_case_3"] = add(input3, other3, alpha=0.5)

    # Test case 4: Adding a tensor and a scalar with a specified alpha
    input4 = torch.tensor([1.0, 2.0, 3.0], device='cuda')
    other4 = 2.0
    results["test_case_4"] = add(input4, other4, alpha=0.5)

    return results

test_results = test_add()
```
"""

system_prompt = """You are an expert PyTorch developer. Don't change the code or the tests, just remove the commented code like the triton kernel.
- Return the original code cleaned up.
"""


system_prompt = """You are an expert developer with deep experience in Triton kernels and PyTorch testing. Your task is to integrate existing PyTorch tests into a Triton kernel implementation.

Instructions:
	•	Input:
	•	You will be provided with:
	1.	Correct and validated Triton kernel code.
	2.	PyTorch code generated from the Triton kernel (verified correct).
	3.	PyTorch test cases that have already been thoroughly validated.
	•	Output:
	•	Add the PyTorch tests verbatim (do not modify their logic or structure) to the bottom of the Triton kernel code.
	•	Make minimal, essential adjustments only if absolutely necessary for compatibility (e.g., imports, device handling).
	•	Ensure that the integrated tests can run directly after the Triton kernel without additional setup, clearly separated by a comment header.

Important Guidelines:
	•	Do NOT modify the Triton kernel logic.
	•	The tests should run on GPU (device='cuda') as originally intended.
	•	Retain the PyTorch testing style:
	•	All tests should be within a single function named test_<kernel_function_name>.
	•	Store test results in a dictionary with keys like "test_case_n".
	•	Ensure imports are precise and match PyTorch and Triton best practices.
	•	Do not add if __name__ == "__main__": sections.
	•	Provide the final, integrated code with the Triton kernel followed by PyTorch tests clearly indicated by a comment separator.
    
    
# Example of Integrated Structure:

```python
# Triton kernel code (do not modify)
import triton
import triton.language as tl

@triton.jit
def kernel_add(x_ptr, y_ptr, output_ptr, N):
    pid = tl.program_id(0)
    idx = pid * tl.num_programs(0) + tl.arange(0, 1024)
    mask = idx < N
    x = tl.load(x_ptr + idx, mask=mask)
    y = tl.load(y_ptr + idx, mask=mask)
    tl.store(output_ptr + idx, x + y, mask=mask)

#########################################
# Integrated PyTorch tests from validated code

import torch

def test_kernel_add():
    results = {{}}

    # Test case 1: Adding two tensors
    input1 = torch.tensor([1.0, 2.0, 3.0], device='cuda')
    other1 = torch.tensor([4.0, 5.0, 6.0], device='cuda')
    output1 = torch.empty_like(input1, device='cuda')
    kernel_add[(1,)](input1, other1, output1, input1.size(0))
    results["test_case_1"] = output1

    # Add additional test cases similarly...
    
    return results

test_results = test_kernel_add()
```
Your task is to follow exactly this pattern to combine provided Triton kernel code with existing validated PyTorch tests, making minimal adjustments only when required for compatibility and correctness.

Run the code to make sure the tests well integrated before returning the code.
"""


system_prompt = """You are an expert developer with deep expertise in Triton kernels and PyTorch implementations. Your primary objective is to:
1.	Set a global device standard:
	-	Add a global variable at the top of your scripts:
    ```py
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else 'cpu')
    ```
    - Ensure every PyTorch tensor created explicitly uses device=DEVICE.
2.	Code Cleanup & Consistency:
	-	Maintain consistency in commenting style and clarity.
	-	Remove unnecessary commented code, redundant imports, print statements or unused variables.
3.	Integration & Testing:
	-   Separate the Code and the tests by a line of `########################`
    -   Store all test results in a dictionary (e.g., results["test_case_n"]).
    -   Do not include an if __name__ == "__main__": section.

- Return both the triton and pytorch code with the tests integrated. Also output the `test_results` dictionary for both implementations side-by-side for easy comparison.
- Make sure not to mix pytorch and triton code in the same file.

# Input

You will receive a pytorch and a triton code with the tests. They may have duplicated code, you should remove the duplicated code. 
- The pytorch code shouldn't contain triton and the triton code shouldn't the pytorch code.

# Example output:

- Pytorch code:

A single file with the pytorch code and the tests.

```python 
import torch
DEVICE = torch.device("cuda" if torch.cuda.is_available() else 'cpu')

def fancy_function(input: torch.Tensor):
    return input + 1

#####################

def test_fancy_function():
    results = {{}}
    input_1 = torch.tensor([1.0, 2.0, 3.0], device=DEVICE)
    results["test_case_1"] = fancy_function(input_1)
    return results

test_results = test_fancy_function()

# we only print the test_results dictionary
print(test_results)
```

- Triton code:

A single file with the triton code and the tests.

```python
import torch
import triton
DEVICE = torch.device("cuda" if torch.cuda.is_available() else 'cpu')

@triton.jit
def fancy_function_triton(input):
    "triton implementation of fancy_function"
    return input + 1

#####################

def test_fancy_function():
    test_results = {{}}
    input_1 = torch.tensor([1.0, 2.0, 3.0], device=DEVICE)
    test_results["test_case_1"] = fancy_function_triton(input_1)
    return test_results

test_results = test_fancy_function()

# we only print the test_results dictionary
print(test_results)
```
Don't forget to only print the test_results dictionary, not the whole code, not the tests individually. 
Make sure the tests are not printing intermediate results. As you can see there is a single print statement at the end of the tests.
This is really important if we want to compare the 2 implementations in the future.

You have to make sure that the pytorch and triton code are equivalent and can be used interchangeably. This is the purpose of triton kernels, to replace the pytorch code.
"""


system_prompt = """You are an expert developer with deep expertise in PyTorch and Triton kernels. Your task is to:
1.	Extract the PyTorch tests from the provided code.
2.	Return only the PyTorch tests, no other code.
3.	Ensure the tests are formatted correctly and can be run directly.
4.	Make sure the tests are not printing intermediate results. As you can see there is a single print statement at the end of the tests.
This is really important if we want to compare the 2 implementations in the future.
"""




# weave.init("mini-agent2")

def fix_code(row):

    class PytorchTests(BaseModel):
        # pytorch_code: str = Field(description="The pytorch file with the tests. No ```python or ``` needed, just the code.")
        # triton_code: str = Field(description="The Triton file with the tests. No ```python or ``` needed, just the code.")
        # pytorch_output: str = Field(description="The output of the pytorch code, it should be the output of the tests.")
        # triton_output: str = Field(description="The output of the triton code, it should be the output of the tests.")
        pytorch_tests: str = Field(description="The pytorch tests, no ```python or ``` needed, just the code.")
    # triton_code = row["final_triton_code"]
    pytorch_code = row["final_pytorch_code"]
    try:
        agent = Agent(model_name="o3-mini", system_message=system_prompt, silent=True, response_format=PytorchTests)
        agent_response = agent.run(
            user_prompt=f"Let's format these sample codes. Here is the pytorch code with the tests:\n{pytorch_code}", max_steps=20)
        res = agent_response.final_response
        res = {"pytorch_tests": res.pytorch_tests}
        print(f"=============== Fixed code ==========================")
        return res
    except Exception as e:
        print(f"Error: {e}")
        return {"pytorch_tests": None}


dataset = load_dataset("tcapelle/annotated_dataset_o3_train_pytorch_triton", split="train")

# apply the agent to every row of the dataset (8 at a time)
dataset = dataset.map(fix_code, num_proc=8)

# dataset.save_to_disk("annotated_dataset_o3_fixed")
dataset.push_to_hub("tcapelle/annotated_dataset_o3_train_pytorch_triton", commit_message="Extract tests from pytorch code")



# # new_dataset.save_to_disk("annotated_dataset_o3_fixed")
# # new_dataset.push_to_hub("tcapelle/annotated_dataset_o3")


# iterative fix
# from rich.progress import track

# @weave.op
# def fix_test_cases(dataset):
#     fixed_rows = []
#     for row in track(dataset, description="Fixing test cases"):
#         row.update(fix_code(row))
#         fixed_rows.append(row)

#     from datasets import Dataset
#     new_dataset = Dataset.from_list(fixed_rows)
#     return new_dataset

# dataset = fix_test_cases(dataset)

