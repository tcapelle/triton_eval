"Reverse the jit triton kernel to pytorch code"

import openai
from rich.console import Console
from rich.pretty import pprint
from datasets import load_dataset, load_from_disk
from pydantic import BaseModel, Field

from prompts import generate_pytorch_prompt, KernelInfo, description_prompt, PytorchCodeWithTestCases

console = Console()





# console.rule("[bold blue]Generating Descriptions[/bold blue]")

client = openai.OpenAI()

## Add some description to the dataset
######################################
# dataset_name = "GPUMODE/categorized_triton_data_permissive"
# dataset = load_dataset(dataset_name, split="train")

# def generate_description(row):
#     code = row["input"]
#     response = client.beta.chat.completions.parse(
#         model="gpt-4o",
#         messages=[{"role": "user", "content": description_prompt.format(code=code)}],
#         response_format=KernelInfo,
#     )
#     return {"description": response.choices[0].message.parsed.description}



# dataset = dataset.map(generate_description, num_proc=40)
# dataset.save_to_disk("annotated_dataset")

## Generate Pytorch Code
######################################

# dataset = load_from_disk("annotated_dataset")

# def generate_pytorch_code(row):
#     code = row["input"]
#     description = row["description"]
#     response = client.beta.chat.completions.parse(
#         model="o3-mini",
#         messages=[{"role": "user", "content": generate_pytorch_prompt.format(code=code, description=description)}],
#         response_format=PytorchCodeWithTestCases,
#     )
#     return {"pytorch_code_with_test_cases": response.choices[0].message.parsed.pytorch_code_with_test_cases}


# dataset = dataset.map(generate_pytorch_code, num_proc=40)
# dataset.save_to_disk("annotated_dataset_pt")
# dataset.push_to_hub("tcapelle/annotated_dataset_o3")

## Run code
######################################

# dataset = load_from_disk("annotated_dataset_pt")

# from tools import run_python_code

# def run_code(row):
#     pytorch_code_with_test_cases = row["pytorch_code_with_test_cases"]
#     out = run_python_code(pytorch_code_with_test_cases)
#     print(f"=============== Running code ==========================")
#     print(f"status: {out['status_code']}")
#     if out['status_code'] != 0:
#         print(f"error: {out['output']}")
#     return {"test_cpu_passing": out["status_code"] == 0, "test_cpu_output": out["output"]}

# # Only run first 10 examples
# # dataset = dataset.select(range(10))
# dataset = dataset.map(run_code, num_proc=12)
# dataset.save_to_disk("annotated_dataset_pt_tested")
# dataset.push_to_hub("tcapelle/annotated_dataset_o3")


## Fix with Agent
######################################

import weave
from pydantic import BaseModel
from my_smol_agent.agent import Agent
from my_smol_agent.tools import clear_temp_files

clear_temp_files()

system_prompt = """You are an expert PyTorch developer. Your goal is to fix the test cases for a given PyTorch function. The function code is correct, but the tests were generated by an LLM and may contain errors. Do not modify the function code—only adjust the tests.

# Instructions:
- Try running the code first, to see what the error is.
- Don't change the code, only the tests.
- Fix the tests, make sure that they are fixed and the logic of the tests is preserved.
- If a test cannot be fixed, remove it or create a simpler one.
- At the last step, return only the code with the fixed tests. It should be basically the same code as the original code, but with the tests fixed.

# Regarding the tests:
- Write a test code in Python for the above code. Ensure that all branch tests are in a single function starting with
“test_”, with no parameters.
- Particular attention should be paid to the fact that tensor parameters are of GPU type.
- Try to limit the number of branches to no more than 4.
- In branch tests, avoid modifying parameters that are later in the argument list with default values (especially if
they have out parameters, do not assign them).
- Store the results of all branch calculations in a dictionary, where the dictionary key is "test_case_n", with n
representing the test case number.
- Ensure that the import paths match exactly as described in the operator documentation to maintain accuracy.
- The code should run directly, without if __name__ == "__main__".
- Remember to run the code one last time to make sure the tests are fixed before returning the code.
- The tests are meant to be run on the GPU, so use device='cuda' when creating the tensors and when appropriate.
- Remove any unnecesary comments or commented out code.

IMPORTANT: Never change the code, only the tests.

# A perfect example would look like this:

```python
import torch

def add(input, other, alpha=1, out=None):
    \"\"\"
    Adds the tensor or number 'other', scaled by 'alpha', to the 'input' tensor.
    
    Args:
        input (Tensor): The input tensor.
        other (Tensor or Number): The tensor or number to add to input.
        alpha (Number, optional): The multiplier for 'other'. Default is 1.
        out (Tensor, optional): The output tensor. If provided, the result will be stored in this tensor.
        
    Returns:
        Tensor: The result of adding 'other' scaled by 'alpha' to 'input'.
    \"\"\"
    return torch.add(input, other, alpha=alpha, out=out)

##################################################################################################################################################


import torch

def test_add():
    results = {{}}

    # Test case 1: Adding two tensors with default alpha
    input1 = torch.tensor([1.0, 2.0, 3.0], device='cuda')
    other1 = torch.tensor([4.0, 5.0, 6.0], device='cuda')
    results["test_case_1"] = add(input1, other1)

    # Test case 2: Adding a tensor and a scalar with default alpha
    input2 = torch.tensor([1.0, 2.0, 3.0], device='cuda')
    other2 = 2.0
    results["test_case_2"] = add(input2, other2)

    # Test case 3: Adding two tensors with a specified alpha
    input3 = torch.tensor([1.0, 2.0, 3.0], device='cuda')
    other3 = torch.tensor([4.0, 5.0, 6.0], device='cuda')
    results["test_case_3"] = add(input3, other3, alpha=0.5)

    # Test case 4: Adding a tensor and a scalar with a specified alpha
    input4 = torch.tensor([1.0, 2.0, 3.0], device='cuda')
    other4 = 2.0
    results["test_case_4"] = add(input4, other4, alpha=0.5)

    return results

test_results = test_add()
```
"""

system_prompt = """You are an expert PyTorch developer. Don't change the code or the tests, just remove the commented code like the triton kernel.
- Return the original code cleaned up.
"""


system_prompt = """You are an expert developer with deep experience in Triton kernels and PyTorch testing. Your task is to integrate existing PyTorch tests into a Triton kernel implementation.

Instructions:
	•	Input:
	•	You will be provided with:
	1.	Correct and validated Triton kernel code.
	2.	PyTorch code generated from the Triton kernel (verified correct).
	3.	PyTorch test cases that have already been thoroughly validated.
	•	Output:
	•	Add the PyTorch tests verbatim (do not modify their logic or structure) to the bottom of the Triton kernel code.
	•	Make minimal, essential adjustments only if absolutely necessary for compatibility (e.g., imports, device handling).
	•	Ensure that the integrated tests can run directly after the Triton kernel without additional setup, clearly separated by a comment header.

Important Guidelines:
	•	Do NOT modify the Triton kernel logic.
	•	The tests should run on GPU (device='cuda') as originally intended.
	•	Retain the PyTorch testing style:
	•	All tests should be within a single function named test_<kernel_function_name>.
	•	Store test results in a dictionary with keys like "test_case_n".
	•	Ensure imports are precise and match PyTorch and Triton best practices.
	•	Do not add if __name__ == "__main__": sections.
	•	Provide the final, integrated code with the Triton kernel followed by PyTorch tests clearly indicated by a comment separator.
    
    
# Example of Integrated Structure:

```python
# Triton kernel code (do not modify)
import triton
import triton.language as tl

@triton.jit
def kernel_add(x_ptr, y_ptr, output_ptr, N):
    pid = tl.program_id(0)
    idx = pid * tl.num_programs(0) + tl.arange(0, 1024)
    mask = idx < N
    x = tl.load(x_ptr + idx, mask=mask)
    y = tl.load(y_ptr + idx, mask=mask)
    tl.store(output_ptr + idx, x + y, mask=mask)

#########################################
# Integrated PyTorch tests from validated code

import torch

def test_kernel_add():
    results = {{}}

    # Test case 1: Adding two tensors
    input1 = torch.tensor([1.0, 2.0, 3.0], device='cuda')
    other1 = torch.tensor([4.0, 5.0, 6.0], device='cuda')
    output1 = torch.empty_like(input1, device='cuda')
    kernel_add[(1,)](input1, other1, output1, input1.size(0))
    results["test_case_1"] = output1

    # Add additional test cases similarly...
    
    return results

test_results = test_kernel_add()
```
Your task is to follow exactly this pattern to combine provided Triton kernel code with existing validated PyTorch tests, making minimal adjustments only when required for compatibility and correctness.

Run the code to make sure the tests well integrated before returning the code.
"""


system_prompt = """Role: You are an expert developer with deep expertise in Triton kernels and PyTorch implementations. Your primary objective is to:
1.	Set a global device standard:
	-	Add a global variable at the top of your script:
    ```py
    DEVICE = 'cuda:0'
    ```
    - Ensure every PyTorch tensor created explicitly uses device=DEVICE.
2.	Execute Side-by-Side Validation:
	-	Run both provided implementations (the Triton kernel and its corresponding PyTorch implementation) on identical input test cases, all on CUDA (DEVICE).
	-	Make sure to print the `test_results` dictionary for both implementations side-by-side for easy comparison.
3.	Ground Truth:
	-	Treat outputs from the Triton kernel as the ground truth.
	-	If discrepancies occur, only modify the PyTorch implementation minimally until outputs exactly match Triton kernel results.
	-	Explicitly comment on all changes made, stating precisely why they were required.
4.	Code Cleanup & Consistency:
	-	Format Triton kernel and PyTorch implementation identically.
	-	Maintain consistency in commenting style and clarity.
	-	Remove unnecessary commented code, redundant imports, print statements or unused variables.
5.	Integration & Testing:
	-   Separate the Code and the tests by a line of `########################`
    -   Retain PyTorch testing style, encapsulating all test cases within a single function named: `test_<kernel_function_name>()`
    -   Store all test results in a dictionary (e.g., results["test_case_n"]).
    -   Do not include an if __name__ == "__main__": section.

Return both the triton and pytorch code with the tests integrated. Also output the `test_results` dictionary for both implementations side-by-side for easy comparison.
"""
# weave.init("mini-agent2")



# # One File example
# broken_file = "./scripts/data/airy_ai.py"
# agent = Agent(system_message=system_prompt, response_format=PytorchCode)
# result = agent.run(user_prompt=f"Here is the file that needs fixing:\n#File:\n{broken_file}")
# print(result)


def fix_code(row):

    class PytorchTritonCode(BaseModel):
        outputs_match: bool = Field(description="Whether the outputs of the pytorch code and the triton code match.")
        pytorch_output: str = Field(description="The output of the pytorch code.")
        triton_output: str = Field(description="The output of the triton code.")
        pytorch_code: str = Field(description="The pytorch code with the tests. No ```python or ``` needed, just the code.")
        triton_code: str = Field(description="The cleaned up Triton code with tests. No ```python or ``` needed, just the code.")
    
    triton_code = row["triton_code_with_tests"]
    pytorch_code = row["pytorch_code_with_test_cases_fixed"]
    # if row["test_cuda_passing"]:
    #     return {"pytorch_code_with_test_cases_fixed": pytorch_code, "test_cuda_passing": True, "fixed": False}
    # else:
    try:
        agent = Agent(model_name="o3-mini", system_message=system_prompt, silent=True, response_format=PytorchTritonCode)
        agent_response = agent.run(
            user_prompt=f"Here is the triton code:\n{triton_code}\n\nHere is the pytorch code with the tests:\n{pytorch_code}", max_steps=20)
        res = agent_response.final_response
        res = {"final_triton_code": res.triton_code, 
                "final_pytorch_code": res.pytorch_code,
                "outputs_match": res.outputs_match,
                "pytorch_output": res.pytorch_output,
                "triton_output": res.triton_output}
        print(f"=============== Fixed code ==========================")
        return res
    except Exception as e:
        print(f"Error: {e}")
        return {"final_triton_code": None, "final_pytorch_code": None, "outputs_match": None, "pytorch_output": None, "triton_output": None}


# # iterative fix
# from rich.progress import track

# @weave.op
# def fix_test_cases(dataset):
#     fixed_rows = []
#     for row in track(dataset, description="Fixing test cases"):
#         row.update(fix_code(row))
#         fixed_rows.append(row)
#     return fixed_rows
        

# dataset = load_dataset("tcapelle/annotated_dataset_o3", split="train", revision="4f4f465ccfb15752d701d29b2c402d80be1e9eea")
dataset = load_dataset("tcapelle/annotated_dataset_o3", split="train")

# rows_list = fix_test_cases(dataset)

dataset = dataset.map(fix_code, num_proc=6)
dataset.save_to_disk("annotated_dataset_o3_fixed")
dataset.push_to_hub("tcapelle/annotated_dataset_o3", commit_message="Triton vs PyTorch")


# # from datasets import Dataset
# # new_dataset = Dataset.from_list(rows_list)
# # new_dataset.save_to_disk("annotated_dataset_o3_fixed")
# # new_dataset.push_to_hub("tcapelle/annotated_dataset_o3")