[
    {
      "entrypoint": "add",
      "description": "Performs element-wise addition of two tensors. This is interesting for Triton because addition is a simple, widely used element-wise operation that can often be fused with other element-wise ops (e.g., activation functions, bias adds) to reduce memory traffic and kernel launch overhead.",
      "pt_code": "import torch\n\ndef add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    return x + y"
    },
    {
      "entrypoint": "sub",
      "description": "Performs element-wise subtraction of two tensors. Like addition, subtraction is a fundamental element-wise operation that can benefit from fusion with subsequent element-wise kernels (e.g., activation or scaling) in Triton to minimize global memory reads/writes and reduce the number of small kernels launched.",
      "pt_code": "import torch\n\ndef sub(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    return x - y"
    },
    {
      "entrypoint": "mul",
      "description": "Performs element-wise multiplication of two tensors. Multiplication is a core building block for many neuralâ€network layers (e.g., gated activations, element-wise gating). In Triton, fusing multiplication with adjacent pointwise operations (such as adding a bias or applying an activation) can save memory bandwidth and reduce kernel-launch overhead.",
      "pt_code": "import torch\n\ndef mul(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    return x * y"
    },
    {
      "entrypoint": "div",
      "description": "Performs element-wise division of two tensors. Division is common in normalization layers or custom scaling operations. Triton can fuse division with other element-wise transforms (e.g., multiply, add, or activation) to avoid writing intermediate results back to global memory, improving throughput.",
      "pt_code": "import torch\n\ndef div(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    return x / y"
    },
    {
      "entrypoint": "relu",
      "description": "Applies the ReLU activation function (max(x, 0)) to an input tensor. ReLU is ubiquitous in modern networks. In Triton, fusing ReLU with preceding operations (e.g., convolution, linear, or tensor-wise additions) can eliminate an extra pass over data, minimizing memory traffic and kernel-launch overhead.",
      "pt_code": "import torch\n\ndef relu(x: torch.Tensor) -> torch.Tensor:\n    return torch.relu(x)"
    },
    {
      "entrypoint": "exp",
      "description": "Computes the element-wise exponential of a tensor. Exponentials are used in softmax, attention, and many custom activation or gating functions. By implementing exp in Triton and fusing it with surrounding operations (e.g., subtraction for numerical stability, division for normalization), you can reduce intermediate memory writes and improve numerical throughput.",
      "pt_code": "import torch\n\ndef exp(x: torch.Tensor) -> torch.Tensor:\n    return torch.exp(x)"
    },
    {
      "entrypoint": "sigmoid",
      "description": "Computes the element-wise sigmoid activation (1 / (1 + exp(-x))). Sigmoid is used in gates and output layers. In Triton, fusing sigmoid with neighboring operations (e.g., multiplication for gating or adding biases) can dramatically cut down on memory bandwidth by keeping intermediate values in registers until the fused result is written.",
      "pt_code": "import torch\n\ndef sigmoid(x: torch.Tensor) -> torch.Tensor:\n    return torch.sigmoid(x)"
    },
    {
      "entrypoint": "tanh",
      "description": "Computes the element-wise hyperbolic tangent activation. Tanh is common in RNNs and certain feedforward layers. Triton can implement a fused tanh kernel or fuse tanh with preceding and subsequent element-wise operations (e.g., scaling or add) to reduce global memory I/O and kernel launch overhead.",
      "pt_code": "import torch\n\ndef tanh(x: torch.Tensor) -> torch.Tensor:\n    return torch.tanh(x)"
    },
    {
      "entrypoint": "sqrt",
      "description": "Computes the element-wise square root of a tensor. Square root appears in normalization layers or custom distance computations. A Triton kernel that fuses sqrt with adjacent multiplies or adds can avoid multiple global memory passes, improving throughput and reducing latency.",
      "pt_code": "import torch\n\ndef sqrt(x: torch.Tensor) -> torch.Tensor:\n    return torch.sqrt(x)"
    },
    {
      "entrypoint": "clamp",
      "description": "Clamps each element in the input tensor to a specified [min_val, max_val] range. Clamping is often used to enforce numeric stability or implement hard thresholds. Triton can fuse clamp with preceding arithmetic (e.g., add, multiply) to ensure that intermediate values never hit global memory, thus saving memory bandwidth and launch costs.",
      "pt_code": "import torch\n\ndef clamp(x: torch.Tensor, min_val: float, max_val: float) -> torch.Tensor:\n    return torch.clamp(x, min=min_val, max=max_val)"
    },
    {
      "entrypoint": "mul_add",
      "description": "Performs a fused multiply-add: computes (x * y) + z in a single pass. This pattern appears in many linear-algebra kernels and attention computations. Triton can implement a single kernel that carries out the multiply and add together, reducing two memory accesses into one and lowering kernel-launch overhead compared to separate multiply and add kernels.",
      "pt_code": "import torch\n\ndef mul_add(x: torch.Tensor, y: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n    return x * y + z"
    },
    {
      "entrypoint": "add_mul",
      "description": "Computes (x + y) * z in a fused manner. This operation can appear in custom gated activations or residual updates. By fusing add and multiply into a single Triton kernel, you avoid writing the intermediate x+y result back to global memory, saving bandwidth and reducing launch overhead.",
      "pt_code": "import torch\n\ndef add_mul(x: torch.Tensor, y: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n    return (x + y) * z"
    },
    {
      "entrypoint": "relu_sum",
      "description": "First applies ReLU to tensor x and then adds tensor y element-wise: relu(x) + y. This composite pattern is common when combining activation outputs with biases or skip connections. In Triton, implementing a fused kernel that does ReLU and addition together avoids an extra memory pass and reduces kernel-launch overhead.",
      "pt_code": "import torch\n\ndef relu_sum(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    return torch.relu(x) + y"
    },
    {
      "entrypoint": "abs_add",
      "description": "Computes the element-wise absolute value of x and then adds y: abs(x) + y. This pattern can arise in custom distance metrics or regularization terms. Triton can fuse abs and add into a single kernel, ensuring the absolute-value computation and addition happen in registers before writing back, saving memory bandwidth.",
      "pt_code": "import torch\n\ndef abs_add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    return torch.abs(x) + y"
    },
    {
      "entrypoint": "softmax",
      "description": "Computes the softmax of a tensor. Interesting for Triton due to its common use in neural networks and potential for fused operations (e.g., with log or scaling) to improve memory bandwidth and reduce kernel launches."
    },
    {
      "entrypoint": "layer_norm",
      "description": "Applies Layer Normalization over a mini-batch of inputs. This is a good candidate for Triton as it involves multiple element-wise operations (mean, variance, normalization) and reductions that can be fused into a single kernel, significantly improving performance by reducing memory I/O and kernel launch overhead."
    },
    {
      "entrypoint": "fused_attention",
      "description": "Implements a fused attention mechanism, such as scaled dot-product attention. This is highly interesting for Triton as it combines multiple computationally intensive operations (matrix multiplications, softmax, dropout, masking) that benefit greatly from kernel fusion, optimized memory access patterns, and tiling strategies to maximize GPU utilization."
    },
    {
      "entrypoint": "silu_and_mul",
      "description": "Computes SiLU (Sigmoid Linear Unit) activation (x * sigmoid(x)) and then multiplies the result with another tensor (gate). This pattern, often found in models like LLaMA (SwiGLU), is interesting for Triton because fusing these element-wise operations (sigmoid, two multiplications) can reduce memory bandwidth usage and kernel launch overhead."
    },
    {
      "entrypoint": "rope_embedding",
      "description": "Applies Rotary Position Embedding (RoPE) to input tensors, a common technique in modern transformers. This is interesting for Triton as it involves complex-number-like manipulations, trigonometric functions, and specific slicing/reshaping operations that can be efficiently implemented in a custom kernel to optimize data movement and computation on the GPU."
    },
    {
      "entrypoint": "conv1d_relu",
      "description": "Performs a 1D convolution followed by a ReLU activation. Fusing these two operations in Triton is beneficial as it avoids writing the intermediate convolution output to global memory and then reading it back for the ReLU, thus saving memory bandwidth and reducing latency."
    },
    {
      "entrypoint": "group_norm",
      "description": "Applies Group Normalization over a mini-batch of inputs. Similar to LayerNorm, but with grouping, it involves reductions and element-wise ops. Triton can optimize this by fusing these steps and handling the group-wise calculations efficiently, which can be complex for generic library implementations to optimize perfectly."
    }
  ]